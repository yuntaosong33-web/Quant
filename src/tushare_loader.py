"""
Tushare æ•°æ®åŠ è½½å™¨æ¨¡å—

è¯¥æ¨¡å—æä¾›åŸºäº Tushare Pro çš„æ•°æ®è·å–åŠŸèƒ½ï¼Œæ›¿ä»£ä¸ç¨³å®šçš„ AkShareã€‚
æ”¯æŒè·å–æ—¥çº¿æ•°æ®ã€è´¢åŠ¡æŒ‡æ ‡ã€æŒ‡æ•°æˆåˆ†è‚¡ç­‰ã€‚

Features
--------
- æ—¥çº¿è¡Œæƒ…æ•°æ® (daily, daily_basic)
- è´¢åŠ¡æŒ‡æ ‡æ•°æ® (fina_indicator)
- æŒ‡æ•°æˆåˆ†è‚¡æƒé‡ (index_weight)
- æœ¬åœ°ç¼“å­˜æœºåˆ¶
- è‡ªåŠ¨é‡è¯•å’Œé™æµ

Notes
-----
ä½¿ç”¨å‰éœ€è¦é…ç½® Tushare API Tokenï¼š
1. åœ¨ config/strategy_config.yaml ä¸­è®¾ç½® tushare.api_token
2. æˆ–é€šè¿‡ç¯å¢ƒå˜é‡ TUSHARE_TOKEN è®¾ç½®
"""

from typing import Optional, List, Dict, Any, Union
from datetime import datetime, timedelta
from pathlib import Path
import logging
import time
import os

import pandas as pd
import numpy as np

logger = logging.getLogger(__name__)

# å…¨å±€å˜é‡ï¼šè¿½è¸ªæ–°é—» API æœ€åè°ƒç”¨æ—¶é—´ï¼ˆè·¨å®ä¾‹å…±äº«ï¼‰
_GLOBAL_NEWS_API_LAST_CALL = 0.0
_GLOBAL_NEWS_RATE_LIMIT_COUNT = 0


class TushareDataLoader:
    """
    Tushare Pro æ•°æ®åŠ è½½å™¨
    
    æä¾›ç¨³å®šå¯é çš„ A è‚¡æ•°æ®è·å–æœåŠ¡ï¼ŒåŒ…æ‹¬ï¼š
    - æ—¥çº¿è¡Œæƒ…æ•°æ® (OHLCV + åŸºç¡€æŒ‡æ ‡)
    - è´¢åŠ¡æŒ‡æ ‡æ•°æ® (PE, PB, ROE ç­‰)
    - æŒ‡æ•°æˆåˆ†è‚¡æƒé‡
    - è‚¡ç¥¨åŸºç¡€ä¿¡æ¯
    
    Parameters
    ----------
    api_token : Optional[str]
        Tushare API Tokenï¼Œå¦‚æœä¸æä¾›åˆ™ä»ç¯å¢ƒå˜é‡ TUSHARE_TOKEN è¯»å–
    cache_dir : str
        æœ¬åœ°ç¼“å­˜ç›®å½•ï¼Œé»˜è®¤ "data/tushare_cache"
    
    Attributes
    ----------
    pro : tushare.pro_api
        Tushare Pro API å®ä¾‹
    cache_dir : Path
        ç¼“å­˜ç›®å½•è·¯å¾„
    
    Examples
    --------
    >>> loader = TushareDataLoader(api_token="your_token")
    >>> df = loader.fetch_daily_data("000001.SZ", "20240101", "20241231")
    >>> financial = loader.fetch_financial_indicator("000001.SZ")
    """
    
    # API è¯·æ±‚é™æµå‚æ•°
    # æ™®é€šç”¨æˆ·é™åˆ¶: 200 æ¬¡/åˆ†é’Ÿ = 3.33 æ¬¡/ç§’
    # ä»˜è´¹ç”¨æˆ·é™åˆ¶æ›´é«˜ï¼Œå¯é€‚å½“é™ä½é—´éš”
    REQUEST_INTERVAL = 0.12  # æ¯æ¬¡è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰- æ¿€è¿›æ¨¡å¼
    MAX_RETRIES = 3
    RETRY_DELAY = 2.0  # é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰
    RATE_LIMIT_DELAY = 30.0  # è§¦å‘é¢‘ç‡é™åˆ¶åç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
    HTTP_TIMEOUT = 60  # HTTP è¯·æ±‚è¶…æ—¶ï¼ˆç§’ï¼‰
    
    # æ–°é—»æ¥å£ç‰¹æ®Šé™åˆ¶ï¼šæ¯åˆ†é’Ÿæœ€å¤š 1 æ¬¡
    NEWS_API_INTERVAL = 61.0  # æ–°é—»æ¥å£è°ƒç”¨é—´éš”ï¼ˆç§’ï¼‰
    
    # è‚¡ç¥¨æ± ä»£ç æ˜ å°„
    INDEX_CODE_MAPPING = {
        "hs300": "000300.SH",
        "zz500": "000905.SH",
        "zz1000": "000852.SH",
        "sz50": "000016.SH",
        "cyb": "399006.SZ",  # åˆ›ä¸šæ¿æŒ‡
    }
    
    def __init__(
        self,
        api_token: Optional[str] = None,
        cache_dir: str = "data/tushare_cache"
    ) -> None:
        """
        åˆå§‹åŒ– Tushare æ•°æ®åŠ è½½å™¨
        
        Parameters
        ----------
        api_token : Optional[str]
            Tushare API Token
        cache_dir : str
            ç¼“å­˜ç›®å½•
        """
        # è·å– API Token (ä¼˜å…ˆçº§: å‚æ•° > ç¯å¢ƒå˜é‡ > é…ç½®æ–‡ä»¶)
        self.api_token = api_token or os.environ.get("TUSHARE_TOKEN", "")
        
        # å°è¯•ä»é…ç½®æ–‡ä»¶è¯»å–
        self._skip_news = False  # é»˜è®¤ä¸è·³è¿‡æ–°é—»
        try:
            import yaml
            config_path = Path("config/strategy_config.yaml")
            if config_path.exists():
                with open(config_path, "r", encoding="utf-8") as f:
                    config = yaml.safe_load(f)
                tushare_config = config.get("tushare", {})
                
                # è¯»å– Tokenï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
                if not self.api_token:
                    self.api_token = tushare_config.get("api_token", "")
                    if self.api_token:
                        logger.info("ä»é…ç½®æ–‡ä»¶åŠ è½½ Tushare Token")
                
                # è¯»å– skip_news é…ç½®
                self._skip_news = tushare_config.get("skip_news", False)
                if self._skip_news:
                    logger.info("ğŸ“° æ–°é—»è·å–å·²ç¦ç”¨ (tushare.skip_news=true)")
        except Exception as e:
            logger.debug(f"ä»é…ç½®æ–‡ä»¶è¯»å–é…ç½®å¤±è´¥: {e}")
        
        if not self.api_token:
            raise ValueError(
                "Tushare API Token æœªé…ç½®ï¼\n"
                "è¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼ä¹‹ä¸€é…ç½®ï¼š\n"
                "1. æ„é€ å‡½æ•°å‚æ•° api_token\n"
                "2. ç¯å¢ƒå˜é‡ TUSHARE_TOKEN\n"
                "3. config/strategy_config.yaml ä¸­çš„ tushare.api_token\n"
                "è·å– Token: https://tushare.pro/register"
            )
        
        # åˆå§‹åŒ– Tushare Pro API
        try:
            import tushare as ts
            
            # è®¾ç½® Token å¹¶åˆå§‹åŒ– API
            ts.set_token(self.api_token)
            self.pro = ts.pro_api()
            
            # é…ç½®æ›´é•¿çš„ HTTP è¶…æ—¶ï¼ˆé€šè¿‡ä¿®æ”¹åº•å±‚ DataApiï¼‰
            try:
                if hasattr(self.pro, '_DataApi__http'):
                    # æ–°ç‰ˆ Tushare ä½¿ç”¨ __http å±æ€§
                    self.pro._DataApi__http.timeout = self.HTTP_TIMEOUT
                elif hasattr(self.pro, 'timeout'):
                    self.pro.timeout = self.HTTP_TIMEOUT
                logger.info(f"Tushare Pro API åˆå§‹åŒ–æˆåŠŸ (timeout={self.HTTP_TIMEOUT}s)")
            except Exception:
                logger.info("Tushare Pro API åˆå§‹åŒ–æˆåŠŸ")
                
        except ImportError:
            raise ImportError("è¯·å®‰è£… tushare: pip install tushare")
        except Exception as e:
            raise RuntimeError(f"Tushare API åˆå§‹åŒ–å¤±è´¥: {e}")
        
        # è®¾ç½®ç¼“å­˜ç›®å½•
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # è¯·æ±‚è®¡æ•°å™¨ï¼ˆç”¨äºé™æµï¼‰
        self._last_request_time = 0.0
    
    def _rate_limit(self) -> None:
        """API è¯·æ±‚é™æµ"""
        elapsed = time.time() - self._last_request_time
        if elapsed < self.REQUEST_INTERVAL:
            time.sleep(self.REQUEST_INTERVAL - elapsed)
        self._last_request_time = time.time()
    
    def _fetch_with_retry(
        self,
        func,
        *args,
        **kwargs
    ) -> Optional[pd.DataFrame]:
        """
        å¸¦é‡è¯•çš„ API è¯·æ±‚
        
        Parameters
        ----------
        func : callable
            Tushare API å‡½æ•°
        *args, **kwargs
            å‡½æ•°å‚æ•°
        
        Returns
        -------
        Optional[pd.DataFrame]
            è¿”å›æ•°æ®ï¼Œå¤±è´¥è¿”å› None
        """
        for attempt in range(self.MAX_RETRIES):
            try:
                self._rate_limit()
                result = func(*args, **kwargs)
                if result is not None and not result.empty:
                    return result
                # ç©ºç»“æœä¹Ÿç®—æˆåŠŸï¼Œä¸éœ€è¦é‡è¯•
                if result is not None:
                    return result
            except Exception as e:
                error_msg = str(e)
                error_msg_lower = error_msg.lower()
                # æ£€æŸ¥æ˜¯å¦è§¦å‘é¢‘ç‡é™åˆ¶ï¼ˆå¤šç§é”™è¯¯æ ¼å¼ï¼‰
                rate_limit_keywords = ["æ¯åˆ†é’Ÿæœ€å¤šè®¿é—®", "æŠ±æ­‰", "é¢‘ç‡", "rate limit", "too many", "é™åˆ¶"]
                if any(kw in error_msg or kw in error_msg_lower for kw in rate_limit_keywords):
                    logger.warning(f"è§¦å‘ API é¢‘ç‡é™åˆ¶ï¼Œç­‰å¾… {self.RATE_LIMIT_DELAY} ç§’åé‡è¯•... é”™è¯¯: {error_msg[:100]}")
                    time.sleep(self.RATE_LIMIT_DELAY)
                # ç½‘ç»œè¶…æ—¶ï¼šä½¿ç”¨æŒ‡æ•°é€€é¿
                elif "timeout" in error_msg_lower or "timed out" in error_msg_lower:
                    wait_time = self.RETRY_DELAY * (2 ** attempt)  # æŒ‡æ•°é€€é¿: 2, 4, 8 ç§’
                    logger.warning(
                        f"ç½‘ç»œè¶…æ—¶ (å°è¯• {attempt + 1}/{self.MAX_RETRIES}), "
                        f"ç­‰å¾… {wait_time:.1f}s åé‡è¯•..."
                    )
                    if attempt < self.MAX_RETRIES - 1:
                        time.sleep(wait_time)
                # è¿æ¥é”™è¯¯ï¼šå¯èƒ½æ˜¯ç½‘ç»œé—®é¢˜
                elif "connection" in error_msg_lower or "connect" in error_msg_lower:
                    wait_time = self.RETRY_DELAY * (2 ** attempt)
                    logger.warning(
                        f"è¿æ¥å¤±è´¥ (å°è¯• {attempt + 1}/{self.MAX_RETRIES}): {e}, "
                        f"ç­‰å¾… {wait_time:.1f}s åé‡è¯•..."
                    )
                    if attempt < self.MAX_RETRIES - 1:
                        time.sleep(wait_time)
                else:
                    logger.warning(f"API è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{self.MAX_RETRIES}): {e}")
                    if attempt < self.MAX_RETRIES - 1:
                        time.sleep(self.RETRY_DELAY * (attempt + 1))
        return None
    
    # ==================== æŒ‡æ•°æˆåˆ†è‚¡ ====================
    
    def fetch_index_constituents(
        self,
        index_code: str = "hs300",
        trade_date: Optional[str] = None
    ) -> List[str]:
        """
        è·å–æŒ‡æ•°æˆåˆ†è‚¡åˆ—è¡¨
        
        Parameters
        ----------
        index_code : str
            æŒ‡æ•°ä»£ç ï¼Œæ”¯æŒ: hs300, zz500, zz1000, sz50, cyb
            æˆ–ç›´æ¥ä½¿ç”¨ Tushare ä»£ç å¦‚ "000300.SH"
        trade_date : Optional[str]
            äº¤æ˜“æ—¥æœŸï¼Œæ ¼å¼ YYYYMMDDï¼Œé»˜è®¤æœ€è¿‘äº¤æ˜“æ—¥
        
        Returns
        -------
        List[str]
            æˆåˆ†è‚¡ä»£ç åˆ—è¡¨ï¼ˆ6ä½ä»£ç ï¼Œå¦‚ "000001"ï¼‰
        
        Examples
        --------
        >>> loader = TushareDataLoader()
        >>> stocks = loader.fetch_index_constituents("hs300")
        >>> print(len(stocks))  # çº¦ 300 åª
        """
        # è½¬æ¢æŒ‡æ•°ä»£ç 
        ts_index_code = self.INDEX_CODE_MAPPING.get(index_code.lower(), index_code)
        
        # é»˜è®¤ä½¿ç”¨æœ€è¿‘äº¤æ˜“æ—¥
        if trade_date is None:
            trade_date = (datetime.now() - timedelta(days=7)).strftime("%Y%m%d")
        
        logger.info(f"è·å–æŒ‡æ•°æˆåˆ†è‚¡: {ts_index_code}, æ—¥æœŸ: {trade_date}")
        
        # å°è¯•ç¼“å­˜
        cache_file = self.cache_dir / f"index_{index_code}_{trade_date[:6]}.parquet"
        if cache_file.exists():
            try:
                df = pd.read_parquet(cache_file)
                if not df.empty:
                    logger.info(f"ä»ç¼“å­˜åŠ è½½æŒ‡æ•°æˆåˆ†è‚¡: {len(df)} åª")
                    # è¿”å› 6 ä½ä»£ç 
                    return df["con_code"].str[:6].tolist()
            except Exception as e:
                logger.warning(f"ç¼“å­˜è¯»å–å¤±è´¥: {e}")
        
        # API è·å–
        df = self._fetch_with_retry(
            self.pro.index_weight,
            index_code=ts_index_code,
            start_date=trade_date,
            end_date=trade_date
        )
        
        if df is None or df.empty:
            # å°è¯•æœ€è¿‘ä¸€ä¸ªæœˆçš„æ•°æ®
            end_date = trade_date
            start_date = (datetime.strptime(trade_date, "%Y%m%d") - timedelta(days=30)).strftime("%Y%m%d")
            df = self._fetch_with_retry(
                self.pro.index_weight,
                index_code=ts_index_code,
                start_date=start_date,
                end_date=end_date
            )
        
        if df is None or df.empty:
            logger.warning(f"æ— æ³•è·å–æŒ‡æ•°æˆåˆ†è‚¡: {ts_index_code}")
            return []
        
        # å–æœ€æ–°æ—¥æœŸçš„æˆåˆ†è‚¡
        df = df.sort_values("trade_date", ascending=False)
        latest_date = df["trade_date"].iloc[0]
        df = df[df["trade_date"] == latest_date]
        
        # ä¿å­˜ç¼“å­˜
        try:
            df.to_parquet(cache_file, index=False)
            logger.info(f"æŒ‡æ•°æˆåˆ†è‚¡å·²ç¼“å­˜: {cache_file}")
        except Exception as e:
            logger.warning(f"ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
        
        # è¿”å› 6 ä½ä»£ç 
        stock_list = df["con_code"].str[:6].tolist()
        logger.info(f"è·å–åˆ° {len(stock_list)} åªæˆåˆ†è‚¡")
        return stock_list
    
    def fetch_all_stocks(
        self,
        exchange: Optional[str] = None,
        list_status: str = "L"
    ) -> List[str]:
        """
        è·å–å…¨å¸‚åœºè‚¡ç¥¨åˆ—è¡¨
        
        ä½¿ç”¨ Tushare stock_basic æ¥å£è·å–æ‰€æœ‰ä¸Šå¸‚è‚¡ç¥¨ã€‚
        
        Parameters
        ----------
        exchange : Optional[str]
            äº¤æ˜“æ‰€ç­›é€‰ï¼š
            - None: å…¨éƒ¨ï¼ˆé»˜è®¤ï¼‰
            - "SSE": ä¸Šäº¤æ‰€
            - "SZSE": æ·±äº¤æ‰€
        list_status : str
            ä¸Šå¸‚çŠ¶æ€ï¼š
            - "L": ä¸Šå¸‚ä¸­ï¼ˆé»˜è®¤ï¼‰
            - "D": é€€å¸‚
            - "P": æš‚åœä¸Šå¸‚
        
        Returns
        -------
        List[str]
            è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼ˆ6ä½ä»£ç ï¼‰
        
        Notes
        -----
        - é»˜è®¤åªè·å–ä¸Šå¸‚ä¸­çš„è‚¡ç¥¨
        - ä¼šè‡ªåŠ¨è¿‡æ»¤ STã€é€€å¸‚é£é™©è­¦ç¤ºè‚¡ç¥¨
        - ç»“æœä¼šç¼“å­˜åˆ°æœ¬åœ°ï¼ˆå½“æ—¥æœ‰æ•ˆï¼‰
        
        Examples
        --------
        >>> loader = TushareDataLoader()
        >>> all_stocks = loader.fetch_all_stocks()
        >>> print(f"å…¨å¸‚åœºå…± {len(all_stocks)} åªè‚¡ç¥¨")
        """
        logger.info(f"ğŸ” è·å–å…¨å¸‚åœºè‚¡ç¥¨åˆ—è¡¨: exchange={exchange}, list_status={list_status}")
        
        # å°è¯•ä»Šæ—¥ç¼“å­˜
        today = datetime.now().strftime("%Y%m%d")
        cache_file = self.cache_dir / f"stock_basic_{today}.parquet"
        
        if cache_file.exists():
            try:
                df = pd.read_parquet(cache_file)
                # åº”ç”¨ç­›é€‰æ¡ä»¶
                if exchange:
                    df = df[df["exchange"] == exchange]
                stock_list = df["ts_code"].str[:6].tolist()
                logger.info(f"ä»ç¼“å­˜åŠ è½½å…¨å¸‚åœºè‚¡ç¥¨åˆ—è¡¨: {len(stock_list)} åª")
                return stock_list
            except Exception as e:
                logger.warning(f"è¯»å–ç¼“å­˜å¤±è´¥: {e}")
        
        # è°ƒç”¨ API è·å–è‚¡ç¥¨åŸºç¡€ä¿¡æ¯
        df = self._fetch_with_retry(
            self.pro.stock_basic,
            exchange=exchange or "",
            list_status=list_status,
            fields="ts_code,symbol,name,area,industry,market,list_date,exchange"
        )
        
        if df is None or df.empty:
            # ç½‘ç»œå¤±è´¥æ—¶ï¼Œå°è¯•ä½¿ç”¨æœ€è¿‘çš„ç¼“å­˜æ–‡ä»¶
            logger.warning("API è¯·æ±‚å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨å†å²ç¼“å­˜...")
            cache_files = sorted(
                self.cache_dir.glob("stock_basic_*.parquet"),
                reverse=True
            )
            for old_cache in cache_files[:5]:  # æœ€å¤šæ£€æŸ¥æœ€è¿‘5ä¸ªç¼“å­˜
                try:
                    df = pd.read_parquet(old_cache)
                    if not df.empty:
                        if exchange:
                            df = df[df["exchange"] == exchange]
                        stock_list = df["ts_code"].str[:6].tolist()
                        logger.info(
                            f"ä½¿ç”¨å†å²ç¼“å­˜ {old_cache.name}: {len(stock_list)} åªè‚¡ç¥¨"
                        )
                        return stock_list
                except Exception:
                    continue
            logger.warning("æ— å¯ç”¨ç¼“å­˜ï¼Œæ— æ³•è·å–å…¨å¸‚åœºè‚¡ç¥¨åˆ—è¡¨")
            return []
        
        # è¿‡æ»¤ ST å’Œé€€å¸‚é£é™©è‚¡ç¥¨
        if "name" in df.columns:
            st_mask = df["name"].str.contains(r"ST|\*ST|é€€|S\s|PT", na=False, regex=True)
            before_count = len(df)
            df = df[~st_mask]
            filtered_count = before_count - len(df)
            if filtered_count > 0:
                logger.info(f"è¿‡æ»¤ ST/é€€å¸‚é£é™©è‚¡ç¥¨: {filtered_count} åª")
        
        # ä¿å­˜ç¼“å­˜
        try:
            df.to_parquet(cache_file, index=False)
            logger.info(f"å…¨å¸‚åœºè‚¡ç¥¨åˆ—è¡¨å·²ç¼“å­˜: {cache_file}")
        except Exception as e:
            logger.warning(f"ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
        
        # è¿”å› 6 ä½ä»£ç 
        stock_list = df["ts_code"].str[:6].tolist()
        logger.info(f"è·å–å…¨å¸‚åœºè‚¡ç¥¨åˆ—è¡¨å®Œæˆ: {len(stock_list)} åª")
        return stock_list
    
    # ==================== æ—¥çº¿æ•°æ® ====================
    
    def fetch_daily_data(
        self,
        stock_code: str,
        start_date: str,
        end_date: str,
        adj: str = "qfq"
    ) -> Optional[pd.DataFrame]:
        """
        è·å–å•åªè‚¡ç¥¨æ—¥çº¿æ•°æ®
        
        Parameters
        ----------
        stock_code : str
            è‚¡ç¥¨ä»£ç ï¼ˆ6ä½ï¼Œå¦‚ "000001"ï¼‰
        start_date : str
            å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼ YYYYMMDD æˆ– YYYY-MM-DD
        end_date : str
            ç»“æŸæ—¥æœŸï¼Œæ ¼å¼ YYYYMMDD æˆ– YYYY-MM-DD
        adj : str
            å¤æƒæ–¹å¼: qfq(å‰å¤æƒ), hfq(åå¤æƒ), None(ä¸å¤æƒ)
        
        Returns
        -------
        Optional[pd.DataFrame]
            æ—¥çº¿æ•°æ®ï¼ŒåŒ…å« date, open, high, low, close, volume, amount ç­‰
        """
        # æ ‡å‡†åŒ–è‚¡ç¥¨ä»£ç 
        ts_code = self._to_ts_code(stock_code)
        
        # æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼
        start_date = start_date.replace("-", "")
        end_date = end_date.replace("-", "")
        
        # å°è¯•ç¼“å­˜
        cache_file = self.cache_dir / f"daily_{stock_code}_{start_date}_{end_date}.parquet"
        if cache_file.exists():
            try:
                df = pd.read_parquet(cache_file)
                if not df.empty:
                    logger.debug(f"ä»ç¼“å­˜åŠ è½½æ—¥çº¿æ•°æ®: {stock_code}")
                    return self._standardize_daily_columns(df)
            except Exception:
                pass
        
        # API è·å–
        df = self._fetch_with_retry(
            self.pro.daily,
            ts_code=ts_code,
            start_date=start_date,
            end_date=end_date
        )
        
        if df is None or df.empty:
            logger.debug(f"è·å–æ—¥çº¿æ•°æ®å¤±è´¥: {stock_code}")
            return None
        
        # å‰å¤æƒå¤„ç†
        if adj == "qfq":
            adj_factor = self._fetch_with_retry(
                self.pro.adj_factor,
                ts_code=ts_code,
                start_date=start_date,
                end_date=end_date
            )
            if adj_factor is not None and not adj_factor.empty:
                df = df.merge(adj_factor[["trade_date", "adj_factor"]], on="trade_date", how="left")
                df["adj_factor"] = df["adj_factor"].fillna(1.0)
                latest_factor = df["adj_factor"].iloc[0]
                factor = df["adj_factor"] / latest_factor
                for col in ["open", "high", "low", "close"]:
                    if col in df.columns:
                        df[col] = df[col] * factor
        
        # ä¿å­˜ç¼“å­˜
        try:
            df.to_parquet(cache_file, index=False)
        except Exception:
            pass
        
        return self._standardize_daily_columns(df)
    
    def fetch_daily_data_batch(
        self,
        stock_list: List[str],
        start_date: str,
        end_date: str,
        adj: str = "qfq",
        show_progress: bool = True,
        batch_size: int = 200,
        batch_sleep: float = 5.0
    ) -> pd.DataFrame:
        """
        æ‰¹é‡è·å–æ—¥çº¿æ•°æ®ï¼ˆå¸¦é™æµä¿æŠ¤ï¼‰
        
        Parameters
        ----------
        stock_list : List[str]
            è‚¡ç¥¨ä»£ç åˆ—è¡¨
        start_date : str
            å¼€å§‹æ—¥æœŸ
        end_date : str
            ç»“æŸæ—¥æœŸ
        adj : str
            å¤æƒæ–¹å¼
        show_progress : bool
            æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
        batch_size : int
            æ¯æ‰¹æ¬¡å¤„ç†çš„è‚¡ç¥¨æ•°é‡ï¼ˆé»˜è®¤ 150ï¼‰
        batch_sleep : float
            æ¯æ‰¹æ¬¡ä¹‹é—´çš„ä¼‘æ¯æ—¶é—´ï¼ˆç§’ï¼‰
        
        Returns
        -------
        pd.DataFrame
            åˆå¹¶åçš„æ—¥çº¿æ•°æ®
        """
        all_data = []
        total = len(stock_list)
        success_count = 0
        
        # ä½¿ç”¨ tqdm è¿›åº¦æ¡
        if show_progress:
            try:
                from tqdm import tqdm
                iterator = tqdm(
                    enumerate(stock_list), 
                    total=total, 
                    desc="ğŸ“Š è·å–æ—¥çº¿æ•°æ®",
                    unit="åª",
                    ncols=80
                )
            except ImportError:
                iterator = enumerate(stock_list)
                logger.info(f"å¼€å§‹è·å–æ—¥çº¿æ•°æ®: {total} åªè‚¡ç¥¨...")
        else:
            iterator = enumerate(stock_list)
        
        for i, stock in iterator:
            df = self.fetch_daily_data(stock, start_date, end_date, adj)
            if df is not None and not df.empty:
                df["stock_code"] = stock
                all_data.append(df)
                success_count += 1
            
            # æ›´æ–°è¿›åº¦æ¡åç¼€
            if show_progress and hasattr(iterator, 'set_postfix'):
                iterator.set_postfix({"æˆåŠŸ": success_count, "å½“å‰": stock})
            
            # æ‰¹æ¬¡ä¼‘æ¯ï¼ˆé¿å…è§¦å‘é¢‘ç‡é™åˆ¶ï¼‰
            if (i + 1) % batch_size == 0 and (i + 1) < total:
                if show_progress and hasattr(iterator, 'set_description'):
                    iterator.set_description(f"ğŸ“Š ä¼‘æ¯{batch_sleep}s")
                time.sleep(batch_sleep)
                if show_progress and hasattr(iterator, 'set_description'):
                    iterator.set_description("ğŸ“Š è·å–æ—¥çº¿æ•°æ®")
        
        if not all_data:
            return pd.DataFrame()
        
        result = pd.concat(all_data, ignore_index=True)
        logger.info(f"æ‰¹é‡è·å–æ—¥çº¿æ•°æ®å®Œæˆ: {success_count}/{total} åªè‚¡ç¥¨æˆåŠŸ, {len(result)} æ¡è®°å½•")
        return result
    
    # ==================== è´¢åŠ¡æŒ‡æ ‡ ====================
    
    def fetch_financial_indicator(
        self,
        stock_code: str,
        period: Optional[str] = None
    ) -> Optional[pd.DataFrame]:
        """
        è·å–å•åªè‚¡ç¥¨è´¢åŠ¡æŒ‡æ ‡
        
        Parameters
        ----------
        stock_code : str
            è‚¡ç¥¨ä»£ç ï¼ˆ6ä½ï¼‰
        period : Optional[str]
            æŠ¥å‘ŠæœŸï¼Œæ ¼å¼ YYYYMMDDï¼Œå¦‚ "20231231"
            å¦‚æœä¸æä¾›ï¼Œè¿”å›æœ€è¿‘ 8 ä¸ªå­£åº¦çš„æ•°æ®
        
        Returns
        -------
        Optional[pd.DataFrame]
            è´¢åŠ¡æŒ‡æ ‡æ•°æ®ï¼ŒåŒ…å«ï¼š
            - roe: å‡€èµ„äº§æ”¶ç›Šç‡
            - roe_dt: æ‰£éå‡€èµ„äº§æ”¶ç›Šç‡
            - roa: æ€»èµ„äº§æ”¶ç›Šç‡
            - gross_margin: æ¯›åˆ©ç‡
            - profit_to_gr: å‡€åˆ©ç‡
            - eps: æ¯è‚¡æ”¶ç›Š
            - bps: æ¯è‚¡å‡€èµ„äº§
        """
        ts_code = self._to_ts_code(stock_code)
        
        # å°è¯•ç¼“å­˜
        cache_file = self.cache_dir / f"fina_{stock_code}.parquet"
        cache_valid = False
        
        if cache_file.exists():
            try:
                cache_mtime = datetime.fromtimestamp(cache_file.stat().st_mtime)
                # ç¼“å­˜æœ‰æ•ˆæœŸ 7 å¤©
                if (datetime.now() - cache_mtime).days < 7:
                    df = pd.read_parquet(cache_file)
                    if not df.empty:
                        logger.debug(f"ä»ç¼“å­˜åŠ è½½è´¢åŠ¡æŒ‡æ ‡: {stock_code}")
                        cache_valid = True
                        return self._standardize_financial_columns(df)
            except Exception:
                pass
        
        # API è·å–
        df = self._fetch_with_retry(
            self.pro.fina_indicator,
            ts_code=ts_code,
            period=period
        )
        
        if df is None or df.empty:
            logger.debug(f"è·å–è´¢åŠ¡æŒ‡æ ‡å¤±è´¥: {stock_code}")
            return None
        
        # ä¿å­˜ç¼“å­˜
        try:
            df.to_parquet(cache_file, index=False)
        except Exception:
            pass
        
        return self._standardize_financial_columns(df)
    
    def fetch_daily_basic(
        self,
        trade_date: Optional[str] = None,
        stock_list: Optional[List[str]] = None
    ) -> Optional[pd.DataFrame]:
        """
        è·å–æ¯æ—¥åŸºç¡€æŒ‡æ ‡ï¼ˆPE, PB, å¸‚å€¼ç­‰ï¼‰
        
        è¿™æ˜¯è·å–ä¼°å€¼æ•°æ®æœ€é«˜æ•ˆçš„æ–¹å¼ï¼Œä¸€æ¬¡è¯·æ±‚è·å–å…¨å¸‚åœºæ•°æ®ã€‚
        
        Parameters
        ----------
        trade_date : Optional[str]
            äº¤æ˜“æ—¥æœŸï¼Œæ ¼å¼ YYYYMMDDï¼Œé»˜è®¤æœ€è¿‘äº¤æ˜“æ—¥
        stock_list : Optional[List[str]]
            è‚¡ç¥¨åˆ—è¡¨ï¼Œç”¨äºè¿‡æ»¤ç»“æœ
        
        Returns
        -------
        Optional[pd.DataFrame]
            åŸºç¡€æŒ‡æ ‡æ•°æ®ï¼ŒåŒ…å«ï¼š
            - pe_ttm: å¸‚ç›ˆç‡ TTM
            - pb: å¸‚å‡€ç‡
            - ps_ttm: å¸‚é”€ç‡ TTM
            - dv_ttm: è‚¡æ¯ç‡ TTM
            - total_mv: æ€»å¸‚å€¼ï¼ˆä¸‡å…ƒï¼‰
            - circ_mv: æµé€šå¸‚å€¼ï¼ˆä¸‡å…ƒï¼‰
            - turnover_rate: æ¢æ‰‹ç‡
        """
        if trade_date is None:
            trade_date = (datetime.now() - timedelta(days=1)).strftime("%Y%m%d")
        
        # å°è¯•ç¼“å­˜
        cache_file = self.cache_dir / f"daily_basic_{trade_date}.parquet"
        
        if cache_file.exists():
            try:
                df = pd.read_parquet(cache_file)
                if not df.empty:
                    logger.info(f"ä»ç¼“å­˜åŠ è½½æ¯æ—¥åŸºç¡€æŒ‡æ ‡: {trade_date}, {len(df)} æ¡")
                    if stock_list:
                        df = df[df["ts_code"].str[:6].isin(stock_list)]
                    return self._standardize_basic_columns(df)
            except Exception as e:
                logger.warning(f"ç¼“å­˜è¯»å–å¤±è´¥: {e}")
        
        # API è·å–
        df = self._fetch_with_retry(
            self.pro.daily_basic,
            trade_date=trade_date
        )
        
        if df is None or df.empty:
            # å°è¯•å‰å‡ å¤©
            for days_ago in range(1, 8):
                alt_date = (datetime.strptime(trade_date, "%Y%m%d") - timedelta(days=days_ago)).strftime("%Y%m%d")
                df = self._fetch_with_retry(
                    self.pro.daily_basic,
                    trade_date=alt_date
                )
                if df is not None and not df.empty:
                    logger.info(f"ä½¿ç”¨ {alt_date} çš„åŸºç¡€æŒ‡æ ‡æ•°æ®")
                    break
        
        if df is None or df.empty:
            logger.warning(f"æ— æ³•è·å–æ¯æ—¥åŸºç¡€æŒ‡æ ‡: {trade_date}")
            return None
        
        # ä¿å­˜ç¼“å­˜
        try:
            df.to_parquet(cache_file, index=False)
            logger.info(f"æ¯æ—¥åŸºç¡€æŒ‡æ ‡å·²ç¼“å­˜: {cache_file}")
        except Exception as e:
            logger.warning(f"ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
        
        if stock_list:
            df = df[df["ts_code"].str[:6].isin(stock_list)]
        
        return self._standardize_basic_columns(df)
    
    def fetch_financial_batch(
        self,
        stock_list: List[str],
        show_progress: bool = True,
        batch_size: int = 150,
        batch_sleep: float = 8.0
    ) -> pd.DataFrame:
        """
        æ‰¹é‡è·å–è´¢åŠ¡æŒ‡æ ‡ï¼ˆå¸¦é™æµä¿æŠ¤ï¼‰
        
        Parameters
        ----------
        stock_list : List[str]
            è‚¡ç¥¨ä»£ç åˆ—è¡¨
        show_progress : bool
            æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
        batch_size : int
            æ¯æ‰¹æ¬¡å¤„ç†çš„è‚¡ç¥¨æ•°é‡ï¼ˆé»˜è®¤ 100ï¼‰
        batch_sleep : float
            æ¯æ‰¹æ¬¡ä¹‹é—´çš„ä¼‘æ¯æ—¶é—´ï¼ˆç§’ï¼‰
        
        Returns
        -------
        pd.DataFrame
            åˆå¹¶åçš„è´¢åŠ¡æŒ‡æ ‡æ•°æ®
        """
        all_data = []
        total = len(stock_list)
        success_count = 0
        
        # ä½¿ç”¨ tqdm è¿›åº¦æ¡
        if show_progress:
            try:
                from tqdm import tqdm
                iterator = tqdm(
                    enumerate(stock_list), 
                    total=total, 
                    desc="ğŸ“ˆ è·å–è´¢åŠ¡æŒ‡æ ‡",
                    unit="åª",
                    ncols=80
                )
            except ImportError:
                iterator = enumerate(stock_list)
                logger.info(f"å¼€å§‹è·å–è´¢åŠ¡æŒ‡æ ‡: {total} åªè‚¡ç¥¨...")
        else:
            iterator = enumerate(stock_list)
        
        for i, stock in iterator:
            df = self.fetch_financial_indicator(stock)
            if df is not None and not df.empty:
                # åªå–æœ€æ–°ä¸€æœŸ
                df = df.sort_values("end_date", ascending=False).head(1)
                df["stock_code"] = stock
                all_data.append(df)
                success_count += 1
            
            # æ›´æ–°è¿›åº¦æ¡åç¼€
            if show_progress and hasattr(iterator, 'set_postfix'):
                iterator.set_postfix({"æˆåŠŸ": success_count, "å½“å‰": stock})
            
            # æ‰¹æ¬¡ä¼‘æ¯ï¼ˆé¿å…è§¦å‘é¢‘ç‡é™åˆ¶ï¼‰
            if (i + 1) % batch_size == 0 and (i + 1) < total:
                if show_progress and hasattr(iterator, 'set_description'):
                    iterator.set_description(f"ğŸ“ˆ ä¼‘æ¯{batch_sleep}s")
                time.sleep(batch_sleep)
                if show_progress and hasattr(iterator, 'set_description'):
                    iterator.set_description("ğŸ“ˆ è·å–è´¢åŠ¡æŒ‡æ ‡")
        
        if not all_data:
            return pd.DataFrame()
        
        # è¿‡æ»¤æ‰å…¨ç©ºçš„ DataFrameï¼Œé¿å… FutureWarning
        valid_data = [df for df in all_data if not df.isna().all().all()]
        if not valid_data:
            return pd.DataFrame()
        
        result = pd.concat(valid_data, ignore_index=True)
        logger.info(f"æ‰¹é‡è·å–è´¢åŠ¡æŒ‡æ ‡å®Œæˆ: {success_count}/{total} åªè‚¡ç¥¨æˆåŠŸ, {len(result)} æ¡è®°å½•")
        return result
    
    # ==================== æŒ‡æ•°æ—¥çº¿ ====================
    
    def fetch_index_daily(
        self,
        index_code: str,
        start_date: str,
        end_date: str
    ) -> Optional[pd.DataFrame]:
        """
        è·å–æŒ‡æ•°æ—¥çº¿æ•°æ®
        
        Parameters
        ----------
        index_code : str
            æŒ‡æ•°ä»£ç ï¼Œå¦‚ "000300" æˆ– "hs300"
        start_date : str
            å¼€å§‹æ—¥æœŸ
        end_date : str
            ç»“æŸæ—¥æœŸ
        
        Returns
        -------
        Optional[pd.DataFrame]
            æŒ‡æ•°æ—¥çº¿æ•°æ®
        """
        # è½¬æ¢æŒ‡æ•°ä»£ç 
        if index_code.lower() in self.INDEX_CODE_MAPPING:
            ts_code = self.INDEX_CODE_MAPPING[index_code.lower()]
        elif "." in index_code:
            ts_code = index_code
        else:
            # å‡è®¾æ˜¯ä¸Šè¯æŒ‡æ•°
            ts_code = f"{index_code}.SH"
        
        start_date = start_date.replace("-", "")
        end_date = end_date.replace("-", "")
        
        df = self._fetch_with_retry(
            self.pro.index_daily,
            ts_code=ts_code,
            start_date=start_date,
            end_date=end_date
        )
        
        if df is None or df.empty:
            return None
        
        return self._standardize_daily_columns(df)
    
    # ==================== è¾…åŠ©æ–¹æ³• ====================
    
    def _to_ts_code(self, stock_code: str) -> str:
        """
        è½¬æ¢è‚¡ç¥¨ä»£ç ä¸º Tushare æ ¼å¼
        
        Parameters
        ----------
        stock_code : str
            6ä½è‚¡ç¥¨ä»£ç 
        
        Returns
        -------
        str
            Tushare æ ¼å¼ä»£ç ï¼Œå¦‚ "000001.SZ"
        """
        if "." in stock_code:
            return stock_code
        
        code = stock_code.strip()
        
        # æ ¹æ®é¦–ä½åˆ¤æ–­äº¤æ˜“æ‰€
        if code.startswith(("6", "5")):
            return f"{code}.SH"
        elif code.startswith(("0", "3", "2")):
            return f"{code}.SZ"
        elif code.startswith("8") or code.startswith("4"):
            return f"{code}.BJ"  # åŒ—äº¤æ‰€
        else:
            return f"{code}.SZ"
    
    def _standardize_daily_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ ‡å‡†åŒ–æ—¥çº¿æ•°æ®åˆ—å"""
        column_mapping = {
            "trade_date": "date",
            "ts_code": "ts_code",
            "open": "open",
            "high": "high",
            "low": "low",
            "close": "close",
            "vol": "volume",
            "amount": "amount",
            "pct_chg": "pct_change",
            "change": "change",
        }
        
        df = df.rename(columns=column_mapping)
        
        if "date" in df.columns:
            df["date"] = pd.to_datetime(df["date"])
            df = df.sort_values("date")
        
        # æˆäº¤é‡å•ä½è½¬æ¢ï¼ˆTushare å•ä½æ˜¯æ‰‹ï¼Œè½¬ä¸ºè‚¡ï¼‰
        if "volume" in df.columns:
            df["volume"] = df["volume"] * 100
        
        # æˆäº¤é¢å•ä½è½¬æ¢ï¼ˆTushare å•ä½æ˜¯åƒå…ƒï¼Œè½¬ä¸ºå…ƒï¼‰
        if "amount" in df.columns:
            df["amount"] = df["amount"] * 1000
        
        return df
    
    def _standardize_basic_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ ‡å‡†åŒ–æ¯æ—¥åŸºç¡€æŒ‡æ ‡åˆ—å"""
        column_mapping = {
            "trade_date": "date",
            "ts_code": "ts_code",
            "pe_ttm": "pe_ttm",
            "pe": "pe",
            "pb": "pb",
            "ps_ttm": "ps_ttm",
            "dv_ttm": "dividend_yield",
            "dv_ratio": "dividend_yield",
            "total_mv": "total_mv",
            "circ_mv": "circ_mv",
            "turnover_rate": "turn",
            "turnover_rate_f": "turn_free",
        }
        
        df = df.rename(columns=column_mapping)
        
        # æå– 6 ä½è‚¡ç¥¨ä»£ç 
        if "ts_code" in df.columns:
            df["stock_code"] = df["ts_code"].str[:6]
        
        if "date" in df.columns:
            df["date"] = pd.to_datetime(df["date"])
        
        # å¸‚å€¼å•ä½è½¬æ¢ï¼ˆä¸‡å…ƒ -> å…ƒï¼‰
        for col in ["total_mv", "circ_mv"]:
            if col in df.columns:
                df[col] = df[col] * 10000
        
        return df
    
    def _standardize_financial_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ ‡å‡†åŒ–è´¢åŠ¡æŒ‡æ ‡åˆ—å"""
        column_mapping = {
            "ts_code": "ts_code",
            "ann_date": "ann_date",
            "end_date": "end_date",
            "roe": "roe",
            "roe_dt": "roe_dt",
            "roe_yearly": "roe_ttm",
            "roa": "roa",
            "grossprofit_margin": "gross_margin",
            "profit_to_gr": "net_margin",
            "eps": "eps",
            "bps": "bps",
            "netprofit_margin": "net_margin",
            "current_ratio": "current_ratio",
            "quick_ratio": "quick_ratio",
        }
        
        df = df.rename(columns=column_mapping)
        
        # æå– 6 ä½è‚¡ç¥¨ä»£ç 
        if "ts_code" in df.columns:
            df["stock_code"] = df["ts_code"].str[:6]
        
        return df
    
    # ==================== æ–°é—»èµ„è®¯ ====================
    
    def fetch_news(
        self,
        stock_code: Optional[str] = None,
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
        src: str = "sina"
    ) -> Optional[pd.DataFrame]:
        """
        è·å–æ–°é—»èµ„è®¯æ•°æ®
        
        ä½¿ç”¨ Tushare Pro news æ¥å£è·å–è´¢ç»æ–°é—»ã€‚
        
        Parameters
        ----------
        stock_code : Optional[str]
            è‚¡ç¥¨ä»£ç ï¼ˆ6ä½ï¼‰ï¼Œå¦‚æœæä¾›åˆ™è¿‡æ»¤ç›¸å…³æ–°é—»
        start_date : Optional[str]
            å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼ YYYYMMDD
        end_date : Optional[str]
            ç»“æŸæ—¥æœŸï¼Œæ ¼å¼ YYYYMMDD
        src : str
            æ–°é—»æ¥æºï¼Œå¯é€‰ï¼šsina(æ–°æµª), wallstreetcn(åå°”è¡—è§é—»), 
            10jqka(åŒèŠ±é¡º), eastmoney(ä¸œæ–¹è´¢å¯Œ), yuncaijing(äº‘è´¢ç»)
            é»˜è®¤ sina
        
        Returns
        -------
        Optional[pd.DataFrame]
            æ–°é—»æ•°æ®ï¼ŒåŒ…å« datetime, title, content, channels ç­‰å­—æ®µ
            å¤±è´¥è¿”å› None
        
        Notes
        -----
        - Tushare Pro æ–°é—»æ¥å£éœ€è¦è¾ƒé«˜ç§¯åˆ†æƒé™
        - å¦‚æœæ¥å£ä¸å¯ç”¨ï¼Œä¼šè¿”å›ç©º DataFrame
        
        Examples
        --------
        >>> loader = TushareDataLoader()
        >>> news = loader.fetch_news(start_date="20240101", end_date="20240115")
        >>> print(news[['datetime', 'title']].head())
        """
        # æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼
        if start_date:
            start_date = start_date.replace("-", "")
        if end_date:
            end_date = end_date.replace("-", "")
        
        global _GLOBAL_NEWS_API_LAST_CALL, _GLOBAL_NEWS_RATE_LIMIT_COUNT
        
        # æ£€æŸ¥æ˜¯å¦åœ¨é…ç½®ä¸­è·³è¿‡æ–°é—»è·å–
        if getattr(self, '_skip_news', False):
            logger.debug("æ–°é—»è·å–å·²åœ¨é…ç½®ä¸­ç¦ç”¨ (tushare.skip_news=true)")
            return pd.DataFrame()
        
        # æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡æ–°é—»è·å–ï¼ˆé¢‘ç‡é™åˆ¶ä¿æŠ¤ï¼‰
        if _GLOBAL_NEWS_RATE_LIMIT_COUNT >= 3:
            logger.warning("æ–°é—»æ¥å£é¢‘ç¹è§¦å‘é™åˆ¶ï¼Œæœ¬æ¬¡è·³è¿‡ï¼ˆéœ€è¦æ›´é«˜ç§¯åˆ†æƒé™ï¼‰")
            return pd.DataFrame()
        
        # å°è¯•ç¼“å­˜ï¼ˆæ–°é—»æŒ‰æ—¥æœŸå’Œæ¥æºç¼“å­˜ï¼‰
        cache_key = f"news_{src}_{start_date}_{end_date}"
        if stock_code:
            cache_key += f"_{stock_code.replace('.', '')[:6]}"
        cache_file = self.cache_dir / f"{cache_key}.parquet"
        
        if cache_file.exists():
            try:
                # æ£€æŸ¥ç¼“å­˜æ˜¯å¦åœ¨24å°æ—¶å†…
                cache_mtime = datetime.fromtimestamp(cache_file.stat().st_mtime)
                if (datetime.now() - cache_mtime).total_seconds() < 86400:  # 24å°æ—¶
                    df = pd.read_parquet(cache_file)
                    if not df.empty:
                        logger.info(f"ä»ç¼“å­˜åŠ è½½æ–°é—»: {len(df)} æ¡")
                        return df
            except Exception:
                pass
        
        # æ–°é—»æ¥å£ç‰¹æ®Šé™æµï¼šæ¯åˆ†é’Ÿæœ€å¤š 1 æ¬¡ï¼ˆä½¿ç”¨å…¨å±€å˜é‡è·¨å®ä¾‹å…±äº«ï¼‰
        elapsed = time.time() - _GLOBAL_NEWS_API_LAST_CALL
        if elapsed < self.NEWS_API_INTERVAL:
            wait_time = self.NEWS_API_INTERVAL - elapsed
            logger.info(f"â³ æ–°é—»æ¥å£é™æµï¼ˆæ¯åˆ†é’Ÿ1æ¬¡ï¼‰ï¼Œç­‰å¾… {wait_time:.0f} ç§’...")
            time.sleep(wait_time)
        
        logger.info(f"è·å–æ–°é—»èµ„è®¯: src={src}, {start_date} ~ {end_date}")
        
        try:
            # æ›´æ–°å…¨å±€æœ€åè°ƒç”¨æ—¶é—´
            _GLOBAL_NEWS_API_LAST_CALL = time.time()
            
            df = self._fetch_with_retry(
                self.pro.news,
                src=src,
                start_date=start_date,
                end_date=end_date
            )
            
            # æˆåŠŸåˆ™é‡ç½®å…¨å±€è®¡æ•°å™¨
            _GLOBAL_NEWS_RATE_LIMIT_COUNT = 0
            
            if df is None or df.empty:
                logger.debug("æ— æ–°é—»æ•°æ®")
                return pd.DataFrame()
            
            # å¦‚æœæŒ‡å®šäº†è‚¡ç¥¨ä»£ç ï¼Œå°è¯•è¿‡æ»¤ç›¸å…³æ–°é—»
            if stock_code:
                # åœ¨æ ‡é¢˜æˆ–å†…å®¹ä¸­æœç´¢è‚¡ç¥¨ä»£ç æˆ–åç§°
                stock_code_clean = stock_code.replace(".", "")[:6]
                mask = (
                    df["title"].str.contains(stock_code_clean, na=False) |
                    df["content"].str.contains(stock_code_clean, na=False)
                )
                df = df[mask]
            
            # ä¿å­˜ç¼“å­˜
            if not df.empty:
                try:
                    df.to_parquet(cache_file, index=False)
                    logger.debug(f"æ–°é—»å·²ç¼“å­˜: {cache_file.name}")
                except Exception:
                    pass
            
            logger.info(f"è·å–æ–°é—»æˆåŠŸ: {len(df)} æ¡")
            return df
            
        except Exception as e:
            error_msg = str(e)
            # è®°å½•é¢‘ç‡é™åˆ¶ï¼ˆä½¿ç”¨å…¨å±€å˜é‡ï¼‰
            if "æ¯å°æ—¶" in error_msg:
                # æ¯å°æ—¶é™åˆ¶ - æœ¬æ¬¡ä¼šè¯å†…ä¸å†å°è¯•
                _GLOBAL_NEWS_RATE_LIMIT_COUNT = 10  # è®¾ç½®é«˜å€¼ç›´æ¥è·³è¿‡
                logger.warning(f"âš ï¸ æ–°é—»æ¥å£æ¯å°æ—¶é™åˆ¶å·²è¾¾ä¸Šé™ï¼Œæœ¬æ¬¡è·³è¿‡æ–°é—»è·å–")
                logger.warning(f"   æç¤ºï¼šå¯åœ¨é…ç½®ä¸­è®¾ç½® llm.enable_sentiment_filter: false æš‚æ—¶ç¦ç”¨æƒ…ç»ªåˆ†æ")
            elif "æ¯åˆ†é’Ÿ" in error_msg or "é¢‘ç‡" in error_msg.lower() or "æŠ±æ­‰" in error_msg:
                _GLOBAL_NEWS_RATE_LIMIT_COUNT += 1
                logger.warning(f"æ–°é—»æ¥å£é¢‘ç‡é™åˆ¶ ({_GLOBAL_NEWS_RATE_LIMIT_COUNT}/3): {e}")
            else:
                logger.warning(f"è·å–æ–°é—»å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def fetch_all_news_once(
        self,
        days_back: int = 7,
        src: str = "sina"
    ) -> pd.DataFrame:
        """
        ä¸€æ¬¡æ€§è·å–æ‰€æœ‰æ–°é—»ï¼ˆä¼˜åŒ–ï¼šé¿å…å¤šæ¬¡ API è°ƒç”¨ï¼‰
        
        è·å–æœ€è¿‘å‡ å¤©çš„æ‰€æœ‰æ–°é—»ï¼Œç¼“å­˜åä¾›å¤šåªè‚¡ç¥¨ä½¿ç”¨ã€‚
        æ–°é—»æ¥å£æ¯åˆ†é’Ÿåªèƒ½è°ƒç”¨1æ¬¡ï¼Œå› æ­¤ä¸€æ¬¡è·å–å…¨éƒ¨æ•°æ®æ›´é«˜æ•ˆã€‚
        
        Parameters
        ----------
        days_back : int
            å›æº¯å¤©æ•°ï¼Œé»˜è®¤ 7 å¤©
        src : str
            æ–°é—»æºï¼Œé»˜è®¤ sina
        
        Returns
        -------
        pd.DataFrame
            æ‰€æœ‰æ–°é—»æ•°æ®
        """
        end_date = datetime.now().strftime("%Y%m%d")
        start_date = (datetime.now() - timedelta(days=days_back)).strftime("%Y%m%d")
        
        # ä½¿ç”¨å®ä¾‹å˜é‡ç¼“å­˜ï¼Œé¿å…é‡å¤è°ƒç”¨
        cache_key = f"_cached_all_news_{src}_{start_date}_{end_date}"
        if hasattr(self, cache_key):
            cached = getattr(self, cache_key)
            if cached is not None:
                logger.debug(f"ä½¿ç”¨å†…å­˜ç¼“å­˜çš„æ–°é—»æ•°æ®: {len(cached)} æ¡")
                return cached
        
        # è·å–æ‰€æœ‰æ–°é—»ï¼ˆä¸å¸¦è‚¡ç¥¨ä»£ç è¿‡æ»¤ï¼‰
        df = self.fetch_news(
            stock_code=None,  # ä¸è¿‡æ»¤ï¼Œè·å–å…¨éƒ¨
            start_date=start_date,
            end_date=end_date,
            src=src
        )
        
        # ç¼“å­˜åˆ°å®ä¾‹å˜é‡
        setattr(self, cache_key, df if df is not None else pd.DataFrame())
        
        if df is not None and not df.empty:
            logger.info(f"ğŸ“° ä¸€æ¬¡æ€§è·å–æ–°é—»å®Œæˆ: {len(df)} æ¡ï¼Œå¯ä¾›æ‰€æœ‰è‚¡ç¥¨ä½¿ç”¨")
        
        return df if df is not None else pd.DataFrame()
    
    def fetch_stock_news(
        self,
        stock_code: str,
        days_back: int = 7
    ) -> str:
        """
        è·å–å•åªè‚¡ç¥¨ç›¸å…³æ–°é—»ï¼ˆç”¨äºæƒ…æ„Ÿåˆ†æï¼‰
        
        ä»ç¼“å­˜çš„å…¨é‡æ–°é—»ä¸­ç­›é€‰ä¸æŒ‡å®šè‚¡ç¥¨ç›¸å…³çš„æ–°é—»ã€‚
        ä¼˜åŒ–ï¼šåªè°ƒç”¨ä¸€æ¬¡ API è·å–å…¨é‡æ–°é—»ï¼Œç„¶åæœ¬åœ°ç­›é€‰ã€‚
        
        Parameters
        ----------
        stock_code : str
            è‚¡ç¥¨ä»£ç ï¼ˆ6ä½ï¼‰
        days_back : int
            å›æº¯å¤©æ•°ï¼Œé»˜è®¤ 7 å¤©
        
        Returns
        -------
        str
            åˆå¹¶çš„æ–°é—»æ–‡æœ¬ï¼Œç”¨äºæƒ…æ„Ÿåˆ†æ
            æ— æ–°é—»æ—¶è¿”å›ç©ºå­—ç¬¦ä¸²
        """
        # å…ˆè·å–å…¨é‡æ–°é—»ï¼ˆä¼šè‡ªåŠ¨ç¼“å­˜ï¼Œåªè°ƒç”¨ä¸€æ¬¡ APIï¼‰
        all_news_df = self.fetch_all_news_once(days_back=days_back)
        
        if all_news_df.empty:
            logger.debug(f"æ— æ–°é—»æ•°æ®å¯ç”¨")
            return ""
        
        # ä»å…¨é‡æ–°é—»ä¸­ç­›é€‰ä¸è¯¥è‚¡ç¥¨ç›¸å…³çš„
        stock_code_clean = stock_code.replace(".", "")[:6]
        
        # åœ¨æ ‡é¢˜æˆ–å†…å®¹ä¸­æœç´¢è‚¡ç¥¨ä»£ç 
        mask = pd.Series([False] * len(all_news_df))
        if "title" in all_news_df.columns:
            mask = mask | all_news_df["title"].str.contains(stock_code_clean, na=False)
        if "content" in all_news_df.columns:
            mask = mask | all_news_df["content"].str.contains(stock_code_clean, na=False)
        
        filtered_df = all_news_df[mask]
        
        if filtered_df.empty:
            logger.debug(f"è‚¡ç¥¨ {stock_code} æ— ç›¸å…³æ–°é—»")
            return ""
        
        # æå–æ ‡é¢˜å’Œå†…å®¹
        all_news = []
        for _, row in filtered_df.head(5).iterrows():
            title = row.get("title", "")
            content = row.get("content", "")
            if title:
                all_news.append(str(title))
            if content and len(str(content)) < 500:
                all_news.append(str(content)[:200])
        
        if not all_news:
            return ""
        
        # åˆå¹¶æ–°é—»æ–‡æœ¬
        combined = " | ".join(all_news)
        
        # æˆªæ–­
        if len(combined) > 1500:
            combined = combined[:1500] + "..."
        
        logger.debug(f"è·å–è‚¡ç¥¨æ–°é—»æˆåŠŸ: {stock_code}, {len(all_news)} æ¡")
        return combined
    
    # ==================== äº¤æ˜“æ—¥å† ====================
    
    def fetch_trade_calendar(
        self,
        start_date: str,
        end_date: str,
        exchange: str = "SSE"
    ) -> pd.DatetimeIndex:
        """
        è·å–äº¤æ˜“æ—¥å†
        
        Parameters
        ----------
        start_date : str
            å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼ YYYY-MM-DD æˆ– YYYYMMDD
        end_date : str
            ç»“æŸæ—¥æœŸï¼Œæ ¼å¼ YYYY-MM-DD æˆ– YYYYMMDD
        exchange : str
            äº¤æ˜“æ‰€ï¼ŒSSE(ä¸Šäº¤æ‰€ï¼Œé»˜è®¤) æˆ– SZSE(æ·±äº¤æ‰€)
        
        Returns
        -------
        pd.DatetimeIndex
            äº¤æ˜“æ—¥æœŸç´¢å¼•
        
        Examples
        --------
        >>> loader = TushareDataLoader()
        >>> calendar = loader.fetch_trade_calendar("2024-01-01", "2024-12-31")
        >>> print(f"2024å¹´å…± {len(calendar)} ä¸ªäº¤æ˜“æ—¥")
        """
        # æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼
        start_date = start_date.replace("-", "")
        end_date = end_date.replace("-", "")
        
        logger.info(f"è·å–äº¤æ˜“æ—¥å†: {start_date} ~ {end_date}")
        
        # å°è¯•ç¼“å­˜
        cache_file = self.cache_dir / f"trade_cal_{start_date[:4]}.parquet"
        
        if cache_file.exists():
            try:
                df = pd.read_parquet(cache_file)
                # è¿‡æ»¤æ—¥æœŸèŒƒå›´
                df = df[
                    (df["cal_date"] >= start_date) & 
                    (df["cal_date"] <= end_date) &
                    (df["is_open"] == 1)
                ]
                if not df.empty:
                    calendar = pd.to_datetime(df["cal_date"])
                    logger.debug(f"ä»ç¼“å­˜åŠ è½½äº¤æ˜“æ—¥å†: {len(calendar)} å¤©")
                    return pd.DatetimeIndex(sorted(calendar))
            except Exception:
                pass
        
        # API è·å–
        df = self._fetch_with_retry(
            self.pro.trade_cal,
            exchange=exchange,
            start_date=start_date,
            end_date=end_date
        )
        
        if df is None or df.empty:
            logger.warning("æ— æ³•è·å–äº¤æ˜“æ—¥å†ï¼Œä½¿ç”¨å·¥ä½œæ—¥è¿‘ä¼¼")
            return pd.bdate_range(start=start_date, end=end_date)
        
        # ä¿å­˜ç¼“å­˜ï¼ˆæ•´å¹´æ•°æ®ï¼‰
        try:
            full_year_df = self._fetch_with_retry(
                self.pro.trade_cal,
                exchange=exchange,
                start_date=f"{start_date[:4]}0101",
                end_date=f"{start_date[:4]}1231"
            )
            if full_year_df is not None and not full_year_df.empty:
                full_year_df.to_parquet(cache_file, index=False)
        except Exception:
            pass
        
        # è¿‡æ»¤äº¤æ˜“æ—¥
        trade_days = df[df["is_open"] == 1]["cal_date"]
        calendar = pd.to_datetime(trade_days)
        calendar = pd.DatetimeIndex(sorted(calendar))
        calendar.name = "date"
        
        logger.info(f"è·å–äº¤æ˜“æ—¥å†æˆåŠŸ: {len(calendar)} ä¸ªäº¤æ˜“æ—¥")
        return calendar
    
    def is_trade_day(self, date: Optional[str] = None) -> bool:
        """
        åˆ¤æ–­æŒ‡å®šæ—¥æœŸæ˜¯å¦ä¸ºäº¤æ˜“æ—¥
        
        Parameters
        ----------
        date : Optional[str]
            æ—¥æœŸï¼Œæ ¼å¼ YYYY-MM-DD æˆ– YYYYMMDDï¼Œé»˜è®¤ä»Šå¤©
        
        Returns
        -------
        bool
            æ˜¯å¦ä¸ºäº¤æ˜“æ—¥
        """
        from datetime import datetime
        
        if date is None:
            date = datetime.now().strftime("%Y%m%d")
        else:
            date = date.replace("-", "")
        
        calendar = self.fetch_trade_calendar(date, date)
        return len(calendar) > 0
    
    # ==================== è¡Œä¸šåˆ†ç±» ====================
    
    def fetch_industry_mapping(
        self,
        use_cache: bool = True
    ) -> Dict[str, str]:
        """
        è·å–è‚¡ç¥¨è¡Œä¸šåˆ†ç±»æ˜ å°„
        
        è¿”å›è‚¡ç¥¨ä»£ç åˆ°è¡Œä¸šåç§°çš„æ˜ å°„å­—å…¸ã€‚
        
        Parameters
        ----------
        use_cache : bool
            æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼Œé»˜è®¤ True
        
        Returns
        -------
        Dict[str, str]
            è‚¡ç¥¨ä»£ç ï¼ˆ6ä½ï¼‰åˆ°è¡Œä¸šåç§°çš„æ˜ å°„
        
        Examples
        --------
        >>> loader = TushareDataLoader()
        >>> industry_map = loader.fetch_industry_mapping()
        >>> print(industry_map.get("000001"))  # é“¶è¡Œ
        """
        logger.info("è·å–è‚¡ç¥¨è¡Œä¸šåˆ†ç±»æ˜ å°„")
        
        # å°è¯•ç¼“å­˜
        today = datetime.now().strftime("%Y%m%d")
        cache_file = self.cache_dir / f"industry_mapping_{today[:6]}.parquet"
        
        if use_cache and cache_file.exists():
            try:
                df = pd.read_parquet(cache_file)
                mapping = dict(zip(df["stock_code"], df["industry"]))
                logger.info(f"ä»ç¼“å­˜åŠ è½½è¡Œä¸šæ˜ å°„: {len(mapping)} åªè‚¡ç¥¨")
                return mapping
            except Exception as e:
                logger.warning(f"ç¼“å­˜è¯»å–å¤±è´¥: {e}")
        
        # API è·å–
        df = self._fetch_with_retry(
            self.pro.stock_basic,
            list_status="L",
            fields="ts_code,symbol,name,industry,market,list_date"
        )
        
        if df is None or df.empty:
            logger.warning("æ— æ³•è·å–è¡Œä¸šåˆ†ç±»æ•°æ®")
            return {}
        
        # æå– 6 ä½è‚¡ç¥¨ä»£ç 
        df["stock_code"] = df["ts_code"].str[:6]
        
        # ä¿å­˜ç¼“å­˜
        try:
            df[["stock_code", "industry"]].to_parquet(cache_file, index=False)
            logger.info(f"è¡Œä¸šæ˜ å°„å·²ç¼“å­˜: {cache_file}")
        except Exception as e:
            logger.warning(f"ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
        
        # æ„å»ºæ˜ å°„
        mapping = dict(zip(df["stock_code"], df["industry"]))
        logger.info(f"è·å–è¡Œä¸šæ˜ å°„æˆåŠŸ: {len(mapping)} åªè‚¡ç¥¨")
        return mapping
    
    def fetch_sw_industry_mapping(
        self,
        level: int = 1
    ) -> Dict[str, str]:
        """
        è·å–ç”³ä¸‡è¡Œä¸šåˆ†ç±»æ˜ å°„
        
        Parameters
        ----------
        level : int
            è¡Œä¸šåˆ†ç±»çº§åˆ«ï¼š1(ä¸€çº§), 2(äºŒçº§), 3(ä¸‰çº§)
            é»˜è®¤ 1ï¼ˆä¸€çº§è¡Œä¸šï¼‰
        
        Returns
        -------
        Dict[str, str]
            è‚¡ç¥¨ä»£ç ï¼ˆ6ä½ï¼‰åˆ°ç”³ä¸‡è¡Œä¸šåç§°çš„æ˜ å°„
        
        Notes
        -----
        ç”³ä¸‡è¡Œä¸šåˆ†ç±»æ˜¯ A è‚¡æœ€å¸¸ç”¨çš„è¡Œä¸šåˆ†ç±»æ ‡å‡†ã€‚
        Tushare éœ€è¦è¾ƒé«˜æƒé™æ‰èƒ½ä½¿ç”¨ç”³ä¸‡è¡Œä¸šæ¥å£ã€‚
        """
        logger.info(f"è·å–ç”³ä¸‡ {level} çº§è¡Œä¸šåˆ†ç±»")
        
        # å°è¯•ä½¿ç”¨ stock_basic çš„ industry å­—æ®µï¼ˆé€šç”¨è¡Œä¸šåˆ†ç±»ï¼‰
        # å¦‚æœéœ€è¦ç²¾ç¡®çš„ç”³ä¸‡åˆ†ç±»ï¼Œéœ€è¦ä½¿ç”¨ index_member æ¥å£
        
        try:
            # å°è¯•è·å–ç”³ä¸‡æŒ‡æ•°æˆåˆ†
            df = self._fetch_with_retry(
                self.pro.index_classify,
                level=f"L{level}",
                src="SW"
            )
            
            if df is not None and not df.empty:
                # è·å–æ¯ä¸ªè¡Œä¸šçš„æˆåˆ†è‚¡
                result = {}
                for _, row in df.iterrows():
                    index_code = row.get("index_code", "")
                    industry_name = row.get("industry_name", "")
                    
                    if index_code:
                        members = self._fetch_with_retry(
                            self.pro.index_member,
                            index_code=index_code
                        )
                        if members is not None and not members.empty:
                            for stock in members["con_code"].str[:6]:
                                result[stock] = industry_name
                
                if result:
                    logger.info(f"è·å–ç”³ä¸‡è¡Œä¸šåˆ†ç±»æˆåŠŸ: {len(result)} åªè‚¡ç¥¨")
                    return result
                    
        except Exception as e:
            logger.debug(f"ç”³ä¸‡åˆ†ç±»æ¥å£ä¸å¯ç”¨: {e}")
        
        # é™çº§åˆ°æ™®é€šè¡Œä¸šåˆ†ç±»
        logger.info("ä½¿ç”¨æ™®é€šè¡Œä¸šåˆ†ç±»æ›¿ä»£ç”³ä¸‡åˆ†ç±»")
        return self.fetch_industry_mapping()


# ==================== ä¾¿æ·å‡½æ•° ====================

def create_tushare_loader(config: Optional[Dict] = None) -> TushareDataLoader:
    """
    åˆ›å»º Tushare æ•°æ®åŠ è½½å™¨
    
    Parameters
    ----------
    config : Optional[Dict]
        é…ç½®å­—å…¸ï¼ŒåŒ…å« tushare.api_token
    
    Returns
    -------
    TushareDataLoader
        æ•°æ®åŠ è½½å™¨å®ä¾‹
    """
    api_token = None
    
    if config:
        api_token = config.get("tushare", {}).get("api_token")
    
    if not api_token:
        api_token = os.environ.get("TUSHARE_TOKEN")
    
    return TushareDataLoader(api_token=api_token)

