"""
Tushare æ•°æ®åŠ è½½å™¨æ¨¡å—

è¯¥æ¨¡å—æä¾›åŸºäº Tushare Pro çš„æ•°æ®è·å–åŠŸèƒ½ï¼Œæ›¿ä»£ä¸ç¨³å®šçš„ AkShareã€‚
æ”¯æŒè·å–æ—¥çº¿æ•°æ®ã€è´¢åŠ¡æŒ‡æ ‡ã€æŒ‡æ•°æˆåˆ†è‚¡ç­‰ã€‚

Features
--------
- æ—¥çº¿è¡Œæƒ…æ•°æ® (daily, daily_basic)
- è´¢åŠ¡æŒ‡æ ‡æ•°æ® (fina_indicator)
- æŒ‡æ•°æˆåˆ†è‚¡æƒé‡ (index_weight)
- æœ¬åœ°ç¼“å­˜æœºåˆ¶
- è‡ªåŠ¨é‡è¯•å’Œé™æµ

Notes
-----
ä½¿ç”¨å‰éœ€è¦é…ç½® Tushare API Tokenï¼š
1. åœ¨ config/strategy_config.yaml ä¸­è®¾ç½® tushare.api_token
2. æˆ–é€šè¿‡ç¯å¢ƒå˜é‡ TUSHARE_TOKEN è®¾ç½®
"""

from typing import Optional, List, Dict, Any, Union
from datetime import datetime, timedelta
from pathlib import Path
import logging
import time
import os

import pandas as pd
import numpy as np

logger = logging.getLogger(__name__)

# å…¨å±€å˜é‡ï¼šè¿½è¸ªæ–°é—» API æœ€åè°ƒç”¨æ—¶é—´ï¼ˆè·¨å®ä¾‹å…±äº«ï¼‰
_GLOBAL_NEWS_API_LAST_CALL = 0.0
_GLOBAL_NEWS_RATE_LIMIT_COUNT = 0


class TushareDataLoader:
    """
    Tushare Pro æ•°æ®åŠ è½½å™¨
    
    æä¾›ç¨³å®šå¯é çš„ A è‚¡æ•°æ®è·å–æœåŠ¡ï¼ŒåŒ…æ‹¬ï¼š
    - æ—¥çº¿è¡Œæƒ…æ•°æ® (OHLCV + åŸºç¡€æŒ‡æ ‡)
    - è´¢åŠ¡æŒ‡æ ‡æ•°æ® (PE, PB, ROE ç­‰)
    - æŒ‡æ•°æˆåˆ†è‚¡æƒé‡
    - è‚¡ç¥¨åŸºç¡€ä¿¡æ¯
    
    Parameters
    ----------
    api_token : Optional[str]
        Tushare API Tokenï¼Œå¦‚æœä¸æä¾›åˆ™ä»ç¯å¢ƒå˜é‡ TUSHARE_TOKEN è¯»å–
    cache_dir : str
        æœ¬åœ°ç¼“å­˜ç›®å½•ï¼Œé»˜è®¤ "data/tushare_cache"
    
    Attributes
    ----------
    pro : tushare.pro_api
        Tushare Pro API å®ä¾‹
    cache_dir : Path
        ç¼“å­˜ç›®å½•è·¯å¾„
    
    Examples
    --------
    >>> loader = TushareDataLoader(api_token="your_token")
    >>> df = loader.fetch_daily_data("000001.SZ", "20240101", "20241231")
    >>> financial = loader.fetch_financial_indicator("000001.SZ")
    """
    
    # API è¯·æ±‚é™æµå‚æ•°
    # æ™®é€šç”¨æˆ·é™åˆ¶: 200 æ¬¡/åˆ†é’Ÿ = 3.33 æ¬¡/ç§’
    # ä»˜è´¹ç”¨æˆ·é™åˆ¶æ›´é«˜ï¼Œå¯é€‚å½“é™ä½é—´éš”
    REQUEST_INTERVAL = 0.12  # æ¯æ¬¡è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰- æ¿€è¿›æ¨¡å¼
    MAX_RETRIES = 3
    RETRY_DELAY = 2.0  # é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰
    RATE_LIMIT_DELAY = 30.0  # è§¦å‘é¢‘ç‡é™åˆ¶åç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
    HTTP_TIMEOUT = 60  # HTTP è¯·æ±‚è¶…æ—¶ï¼ˆç§’ï¼‰
    
    # æ–°é—»æ¥å£ç‰¹æ®Šé™åˆ¶ï¼šæ¯åˆ†é’Ÿæœ€å¤š 1 æ¬¡
    NEWS_API_INTERVAL = 61.0  # æ–°é—»æ¥å£è°ƒç”¨é—´éš”ï¼ˆç§’ï¼‰
    
    # è‚¡ç¥¨æ± ä»£ç æ˜ å°„
    INDEX_CODE_MAPPING = {
        "hs300": "000300.SH",
        "zz500": "000905.SH",
        "zz1000": "000852.SH",
        "sz50": "000016.SH",
        "cyb": "399006.SZ",  # åˆ›ä¸šæ¿æŒ‡
    }
    
    def __init__(
        self,
        api_token: Optional[str] = None,
        cache_dir: str = "data/tushare_cache"
    ) -> None:
        """
        åˆå§‹åŒ– Tushare æ•°æ®åŠ è½½å™¨
        
        Parameters
        ----------
        api_token : Optional[str]
            Tushare API Token
        cache_dir : str
            ç¼“å­˜ç›®å½•
        """
        # è·å– API Token (ä¼˜å…ˆçº§: å‚æ•° > ç¯å¢ƒå˜é‡ > é…ç½®æ–‡ä»¶)
        self.api_token = api_token or os.environ.get("TUSHARE_TOKEN", "")
        
        # å°è¯•ä»é…ç½®æ–‡ä»¶è¯»å–
        self._skip_news = False  # é»˜è®¤ä¸è·³è¿‡æ–°é—»
        try:
            import yaml
            config_path = Path("config/strategy_config.yaml")
            if config_path.exists():
                with open(config_path, "r", encoding="utf-8") as f:
                    config = yaml.safe_load(f)
                tushare_config = config.get("tushare", {})
                
                # è¯»å– Tokenï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
                if not self.api_token:
                    self.api_token = tushare_config.get("api_token", "")
                    if self.api_token:
                        logger.info("ä»é…ç½®æ–‡ä»¶åŠ è½½ Tushare Token")
                
                # è¯»å– skip_news é…ç½®
                self._skip_news = tushare_config.get("skip_news", False)
                if self._skip_news:
                    logger.info("ğŸ“° æ–°é—»è·å–å·²ç¦ç”¨ (tushare.skip_news=true)")
        except Exception as e:
            logger.debug(f"ä»é…ç½®æ–‡ä»¶è¯»å–é…ç½®å¤±è´¥: {e}")
        
        if not self.api_token:
            raise ValueError(
                "Tushare API Token æœªé…ç½®ï¼\n"
                "è¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼ä¹‹ä¸€é…ç½®ï¼š\n"
                "1. æ„é€ å‡½æ•°å‚æ•° api_token\n"
                "2. ç¯å¢ƒå˜é‡ TUSHARE_TOKEN\n"
                "3. config/strategy_config.yaml ä¸­çš„ tushare.api_token\n"
                "è·å– Token: https://tushare.pro/register"
            )
        
        # åˆå§‹åŒ– Tushare Pro API
        try:
            import tushare as ts
            
            # è®¾ç½® Token å¹¶åˆå§‹åŒ– API
            ts.set_token(self.api_token)
            self.pro = ts.pro_api()
            
            # é…ç½®æ›´é•¿çš„ HTTP è¶…æ—¶ï¼ˆé€šè¿‡ä¿®æ”¹åº•å±‚ DataApiï¼‰
            try:
                if hasattr(self.pro, '_DataApi__http'):
                    # æ–°ç‰ˆ Tushare ä½¿ç”¨ __http å±æ€§
                    self.pro._DataApi__http.timeout = self.HTTP_TIMEOUT
                elif hasattr(self.pro, 'timeout'):
                    self.pro.timeout = self.HTTP_TIMEOUT
                logger.info(f"Tushare Pro API åˆå§‹åŒ–æˆåŠŸ (timeout={self.HTTP_TIMEOUT}s)")
            except Exception:
                logger.info("Tushare Pro API åˆå§‹åŒ–æˆåŠŸ")
                
        except ImportError:
            raise ImportError("è¯·å®‰è£… tushare: pip install tushare")
        except Exception as e:
            raise RuntimeError(f"Tushare API åˆå§‹åŒ–å¤±è´¥: {e}")
        
        # è®¾ç½®ç¼“å­˜ç›®å½•
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # è¯·æ±‚è®¡æ•°å™¨ï¼ˆç”¨äºé™æµï¼‰
        self._last_request_time = 0.0
    
    def _rate_limit(self) -> None:
        """API è¯·æ±‚é™æµ"""
        elapsed = time.time() - self._last_request_time
        if elapsed < self.REQUEST_INTERVAL:
            time.sleep(self.REQUEST_INTERVAL - elapsed)
        self._last_request_time = time.time()
    
    def _fetch_with_retry(
        self,
        func,
        *args,
        **kwargs
    ) -> Optional[pd.DataFrame]:
        """
        å¸¦é‡è¯•çš„ API è¯·æ±‚
        
        Parameters
        ----------
        func : callable
            Tushare API å‡½æ•°
        *args, **kwargs
            å‡½æ•°å‚æ•°
        
        Returns
        -------
        Optional[pd.DataFrame]
            è¿”å›æ•°æ®ï¼Œå¤±è´¥è¿”å› None
        """
        for attempt in range(self.MAX_RETRIES):
            try:
                self._rate_limit()
                result = func(*args, **kwargs)
                if result is not None and not result.empty:
                    return result
                # ç©ºç»“æœä¹Ÿç®—æˆåŠŸï¼Œä¸éœ€è¦é‡è¯•
                if result is not None:
                    return result
            except Exception as e:
                error_msg = str(e)
                error_msg_lower = error_msg.lower()
                # æ£€æŸ¥æ˜¯å¦è§¦å‘é¢‘ç‡é™åˆ¶ï¼ˆå¤šç§é”™è¯¯æ ¼å¼ï¼‰
                rate_limit_keywords = ["æ¯åˆ†é’Ÿæœ€å¤šè®¿é—®", "æŠ±æ­‰", "é¢‘ç‡", "rate limit", "too many", "é™åˆ¶"]
                if any(kw in error_msg or kw in error_msg_lower for kw in rate_limit_keywords):
                    logger.warning(f"è§¦å‘ API é¢‘ç‡é™åˆ¶ï¼Œç­‰å¾… {self.RATE_LIMIT_DELAY} ç§’åé‡è¯•... é”™è¯¯: {error_msg[:100]}")
                    time.sleep(self.RATE_LIMIT_DELAY)
                # ç½‘ç»œè¶…æ—¶ï¼šä½¿ç”¨æŒ‡æ•°é€€é¿
                elif "timeout" in error_msg_lower or "timed out" in error_msg_lower:
                    wait_time = self.RETRY_DELAY * (2 ** attempt)  # æŒ‡æ•°é€€é¿: 2, 4, 8 ç§’
                    logger.warning(
                        f"ç½‘ç»œè¶…æ—¶ (å°è¯• {attempt + 1}/{self.MAX_RETRIES}), "
                        f"ç­‰å¾… {wait_time:.1f}s åé‡è¯•..."
                    )
                    if attempt < self.MAX_RETRIES - 1:
                        time.sleep(wait_time)
                # è¿æ¥é”™è¯¯ï¼šå¯èƒ½æ˜¯ç½‘ç»œé—®é¢˜
                elif "connection" in error_msg_lower or "connect" in error_msg_lower:
                    wait_time = self.RETRY_DELAY * (2 ** attempt)
                    logger.warning(
                        f"è¿æ¥å¤±è´¥ (å°è¯• {attempt + 1}/{self.MAX_RETRIES}): {e}, "
                        f"ç­‰å¾… {wait_time:.1f}s åé‡è¯•..."
                    )
                    if attempt < self.MAX_RETRIES - 1:
                        time.sleep(wait_time)
                else:
                    logger.warning(f"API è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{self.MAX_RETRIES}): {e}")
                    if attempt < self.MAX_RETRIES - 1:
                        time.sleep(self.RETRY_DELAY * (attempt + 1))
        return None
    
    # ==================== æŒ‡æ•°æˆåˆ†è‚¡ ====================
    
    def fetch_index_constituents(
        self,
        index_code: str = "hs300",
        trade_date: Optional[str] = None
    ) -> List[str]:
        """
        è·å–æŒ‡æ•°æˆåˆ†è‚¡åˆ—è¡¨
        
        Parameters
        ----------
        index_code : str
            æŒ‡æ•°ä»£ç ï¼Œæ”¯æŒ: hs300, zz500, zz1000, sz50, cyb
            æˆ–ç›´æ¥ä½¿ç”¨ Tushare ä»£ç å¦‚ "000300.SH"
        trade_date : Optional[str]
            äº¤æ˜“æ—¥æœŸï¼Œæ ¼å¼ YYYYMMDDï¼Œé»˜è®¤æœ€è¿‘äº¤æ˜“æ—¥
        
        Returns
        -------
        List[str]
            æˆåˆ†è‚¡ä»£ç åˆ—è¡¨ï¼ˆ6ä½ä»£ç ï¼Œå¦‚ "000001"ï¼‰
        
        Examples
        --------
        >>> loader = TushareDataLoader()
        >>> stocks = loader.fetch_index_constituents("hs300")
        >>> print(len(stocks))  # çº¦ 300 åª
        """
        # è½¬æ¢æŒ‡æ•°ä»£ç 
        ts_index_code = self.INDEX_CODE_MAPPING.get(index_code.lower(), index_code)
        
        # é»˜è®¤ä½¿ç”¨æœ€è¿‘äº¤æ˜“æ—¥
        if trade_date is None:
            trade_date = (datetime.now() - timedelta(days=7)).strftime("%Y%m%d")
        
        logger.info(f"è·å–æŒ‡æ•°æˆåˆ†è‚¡: {ts_index_code}, æ—¥æœŸ: {trade_date}")
        
        # å°è¯•ç¼“å­˜
        cache_file = self.cache_dir / f"index_{index_code}_{trade_date[:6]}.parquet"
        if cache_file.exists():
            try:
                df = pd.read_parquet(cache_file)
                if not df.empty:
                    logger.info(f"ä»ç¼“å­˜åŠ è½½æŒ‡æ•°æˆåˆ†è‚¡: {len(df)} åª")
                    # è¿”å› 6 ä½ä»£ç 
                    return df["con_code"].str[:6].tolist()
            except Exception as e:
                logger.warning(f"ç¼“å­˜è¯»å–å¤±è´¥: {e}")
        
        # API è·å–
        df = self._fetch_with_retry(
            self.pro.index_weight,
            index_code=ts_index_code,
            start_date=trade_date,
            end_date=trade_date
        )
        
        if df is None or df.empty:
            # å°è¯•æœ€è¿‘ä¸€ä¸ªæœˆçš„æ•°æ®
            end_date = trade_date
            start_date = (datetime.strptime(trade_date, "%Y%m%d") - timedelta(days=30)).strftime("%Y%m%d")
            df = self._fetch_with_retry(
                self.pro.index_weight,
                index_code=ts_index_code,
                start_date=start_date,
                end_date=end_date
            )
        
        if df is None or df.empty:
            logger.warning(f"æ— æ³•è·å–æŒ‡æ•°æˆåˆ†è‚¡: {ts_index_code}")
            return []
        
        # å–æœ€æ–°æ—¥æœŸçš„æˆåˆ†è‚¡
        df = df.sort_values("trade_date", ascending=False)
        latest_date = df["trade_date"].iloc[0]
        df = df[df["trade_date"] == latest_date]
        
        # ä¿å­˜ç¼“å­˜
        try:
            df.to_parquet(cache_file, index=False)
            logger.info(f"æŒ‡æ•°æˆåˆ†è‚¡å·²ç¼“å­˜: {cache_file}")
        except Exception as e:
            logger.warning(f"ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
        
        # è¿”å› 6 ä½ä»£ç 
        stock_list = df["con_code"].str[:6].tolist()
        logger.info(f"è·å–åˆ° {len(stock_list)} åªæˆåˆ†è‚¡")
        return stock_list
    
    def fetch_all_stocks(
        self,
        exchange: Optional[str] = None,
        list_status: str = "L"
    ) -> List[str]:
        """
        è·å–å…¨å¸‚åœºè‚¡ç¥¨åˆ—è¡¨
        
        ä½¿ç”¨ Tushare stock_basic æ¥å£è·å–æ‰€æœ‰ä¸Šå¸‚è‚¡ç¥¨ã€‚
        
        Parameters
        ----------
        exchange : Optional[str]
            äº¤æ˜“æ‰€ç­›é€‰ï¼š
            - None: å…¨éƒ¨ï¼ˆé»˜è®¤ï¼‰
            - "SSE": ä¸Šäº¤æ‰€
            - "SZSE": æ·±äº¤æ‰€
        list_status : str
            ä¸Šå¸‚çŠ¶æ€ï¼š
            - "L": ä¸Šå¸‚ä¸­ï¼ˆé»˜è®¤ï¼‰
            - "D": é€€å¸‚
            - "P": æš‚åœä¸Šå¸‚
        
        Returns
        -------
        List[str]
            è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼ˆ6ä½ä»£ç ï¼‰
        
        Notes
        -----
        - é»˜è®¤åªè·å–ä¸Šå¸‚ä¸­çš„è‚¡ç¥¨
        - ä¼šè‡ªåŠ¨è¿‡æ»¤ STã€é€€å¸‚é£é™©è­¦ç¤ºè‚¡ç¥¨
        - ç»“æœä¼šç¼“å­˜åˆ°æœ¬åœ°ï¼ˆå½“æ—¥æœ‰æ•ˆï¼‰
        
        Examples
        --------
        >>> loader = TushareDataLoader()
        >>> all_stocks = loader.fetch_all_stocks()
        >>> print(f"å…¨å¸‚åœºå…± {len(all_stocks)} åªè‚¡ç¥¨")
        """
        logger.info(f"ğŸ” è·å–å…¨å¸‚åœºè‚¡ç¥¨åˆ—è¡¨: exchange={exchange}, list_status={list_status}")
        
        # å°è¯•ä»Šæ—¥ç¼“å­˜
        today = datetime.now().strftime("%Y%m%d")
        cache_file = self.cache_dir / f"stock_basic_{today}.parquet"
        
        if cache_file.exists():
            try:
                df = pd.read_parquet(cache_file)
                # åº”ç”¨ç­›é€‰æ¡ä»¶
                if exchange:
                    df = df[df["exchange"] == exchange]
                stock_list = df["ts_code"].str[:6].tolist()
                logger.info(f"ä»ç¼“å­˜åŠ è½½å…¨å¸‚åœºè‚¡ç¥¨åˆ—è¡¨: {len(stock_list)} åª")
                return stock_list
            except Exception as e:
                logger.warning(f"è¯»å–ç¼“å­˜å¤±è´¥: {e}")
        
        # è°ƒç”¨ API è·å–è‚¡ç¥¨åŸºç¡€ä¿¡æ¯
        df = self._fetch_with_retry(
            self.pro.stock_basic,
            exchange=exchange or "",
            list_status=list_status,
            fields="ts_code,symbol,name,area,industry,market,list_date,exchange"
        )
        
        if df is None or df.empty:
            # ç½‘ç»œå¤±è´¥æ—¶ï¼Œå°è¯•ä½¿ç”¨æœ€è¿‘çš„ç¼“å­˜æ–‡ä»¶
            logger.warning("API è¯·æ±‚å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨å†å²ç¼“å­˜...")
            cache_files = sorted(
                self.cache_dir.glob("stock_basic_*.parquet"),
                reverse=True
            )
            for old_cache in cache_files[:5]:  # æœ€å¤šæ£€æŸ¥æœ€è¿‘5ä¸ªç¼“å­˜
                try:
                    df = pd.read_parquet(old_cache)
                    if not df.empty:
                        if exchange:
                            df = df[df["exchange"] == exchange]
                        stock_list = df["ts_code"].str[:6].tolist()
                        logger.info(
                            f"ä½¿ç”¨å†å²ç¼“å­˜ {old_cache.name}: {len(stock_list)} åªè‚¡ç¥¨"
                        )
                        return stock_list
                except Exception:
                    continue
            logger.warning("æ— å¯ç”¨ç¼“å­˜ï¼Œæ— æ³•è·å–å…¨å¸‚åœºè‚¡ç¥¨åˆ—è¡¨")
            return []
        
        # è¿‡æ»¤ ST å’Œé€€å¸‚é£é™©è‚¡ç¥¨
        if "name" in df.columns:
            st_mask = df["name"].str.contains(r"ST|\*ST|é€€|S\s|PT", na=False, regex=True)
            before_count = len(df)
            df = df[~st_mask]
            filtered_count = before_count - len(df)
            if filtered_count > 0:
                logger.info(f"è¿‡æ»¤ ST/é€€å¸‚é£é™©è‚¡ç¥¨: {filtered_count} åª")
        
        # ä¿å­˜ç¼“å­˜
        try:
            df.to_parquet(cache_file, index=False)
            logger.info(f"å…¨å¸‚åœºè‚¡ç¥¨åˆ—è¡¨å·²ç¼“å­˜: {cache_file}")
        except Exception as e:
            logger.warning(f"ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
        
        # è¿”å› 6 ä½ä»£ç 
        stock_list = df["ts_code"].str[:6].tolist()
        logger.info(f"è·å–å…¨å¸‚åœºè‚¡ç¥¨åˆ—è¡¨å®Œæˆ: {len(stock_list)} åª")
        return stock_list
    
    # ==================== æ—¥çº¿æ•°æ® ====================
    
    def fetch_daily_data(
        self,
        stock_code: str,
        start_date: str,
        end_date: str,
        adj: str = "qfq"
    ) -> Optional[pd.DataFrame]:
        """
        è·å–å•åªè‚¡ç¥¨æ—¥çº¿æ•°æ®
        
        Parameters
        ----------
        stock_code : str
            è‚¡ç¥¨ä»£ç ï¼ˆ6ä½ï¼Œå¦‚ "000001"ï¼‰
        start_date : str
            å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼ YYYYMMDD æˆ– YYYY-MM-DD
        end_date : str
            ç»“æŸæ—¥æœŸï¼Œæ ¼å¼ YYYYMMDD æˆ– YYYY-MM-DD
        adj : str
            å¤æƒæ–¹å¼: qfq(å‰å¤æƒ), hfq(åå¤æƒ), None(ä¸å¤æƒ)
        
        Returns
        -------
        Optional[pd.DataFrame]
            æ—¥çº¿æ•°æ®ï¼ŒåŒ…å« date, open, high, low, close, volume, amount ç­‰
        """
        # æ ‡å‡†åŒ–è‚¡ç¥¨ä»£ç 
        ts_code = self._to_ts_code(stock_code)
        
        # æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼
        start_date = start_date.replace("-", "")
        end_date = end_date.replace("-", "")
        
        # å°è¯•ç¼“å­˜
        cache_file = self.cache_dir / f"daily_{stock_code}_{start_date}_{end_date}.parquet"
        if cache_file.exists():
            try:
                df = pd.read_parquet(cache_file)
                if not df.empty:
                    logger.debug(f"ä»ç¼“å­˜åŠ è½½æ—¥çº¿æ•°æ®: {stock_code}")
                    return self._standardize_daily_columns(df)
            except Exception:
                pass
        
        # API è·å–
        df = self._fetch_with_retry(
            self.pro.daily,
            ts_code=ts_code,
            start_date=start_date,
            end_date=end_date
        )
        
        if df is None or df.empty:
            logger.debug(f"è·å–æ—¥çº¿æ•°æ®å¤±è´¥: {stock_code}")
            return None
        
        # å‰å¤æƒå¤„ç†
        if adj == "qfq":
            adj_factor = self._fetch_with_retry(
                self.pro.adj_factor,
                ts_code=ts_code,
                start_date=start_date,
                end_date=end_date
            )
            if adj_factor is not None and not adj_factor.empty:
                df = df.merge(adj_factor[["trade_date", "adj_factor"]], on="trade_date", how="left")
                df["adj_factor"] = df["adj_factor"].fillna(1.0)
                latest_factor = df["adj_factor"].iloc[0]
                factor = df["adj_factor"] / latest_factor
                for col in ["open", "high", "low", "close"]:
                    if col in df.columns:
                        df[col] = df[col] * factor
        
        # ä¿å­˜ç¼“å­˜
        try:
            df.to_parquet(cache_file, index=False)
        except Exception:
            pass
        
        return self._standardize_daily_columns(df)
    
    def fetch_daily_data_batch(
        self,
        stock_list: List[str],
        start_date: str,
        end_date: str,
        adj: str = "qfq",
        show_progress: bool = True,
        batch_size: int = 200,
        batch_sleep: float = 5.0
    ) -> pd.DataFrame:
        """
        æ‰¹é‡è·å–æ—¥çº¿æ•°æ®ï¼ˆå¸¦é™æµä¿æŠ¤ï¼‰
        
        Parameters
        ----------
        stock_list : List[str]
            è‚¡ç¥¨ä»£ç åˆ—è¡¨
        start_date : str
            å¼€å§‹æ—¥æœŸ
        end_date : str
            ç»“æŸæ—¥æœŸ
        adj : str
            å¤æƒæ–¹å¼
        show_progress : bool
            æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
        batch_size : int
            æ¯æ‰¹æ¬¡å¤„ç†çš„è‚¡ç¥¨æ•°é‡ï¼ˆé»˜è®¤ 150ï¼‰
        batch_sleep : float
            æ¯æ‰¹æ¬¡ä¹‹é—´çš„ä¼‘æ¯æ—¶é—´ï¼ˆç§’ï¼‰
        
        Returns
        -------
        pd.DataFrame
            åˆå¹¶åçš„æ—¥çº¿æ•°æ®
        """
        all_data = []
        total = len(stock_list)
        success_count = 0
        
        # ä½¿ç”¨ tqdm è¿›åº¦æ¡
        if show_progress:
            try:
                from tqdm import tqdm
                iterator = tqdm(
                    enumerate(stock_list), 
                    total=total, 
                    desc="ğŸ“Š è·å–æ—¥çº¿æ•°æ®",
                    unit="åª",
                    ncols=80
                )
            except ImportError:
                iterator = enumerate(stock_list)
                logger.info(f"å¼€å§‹è·å–æ—¥çº¿æ•°æ®: {total} åªè‚¡ç¥¨...")
        else:
            iterator = enumerate(stock_list)
        
        for i, stock in iterator:
            df = self.fetch_daily_data(stock, start_date, end_date, adj)
            if df is not None and not df.empty:
                df["stock_code"] = stock
                all_data.append(df)
                success_count += 1
            
            # æ›´æ–°è¿›åº¦æ¡åç¼€
            if show_progress and hasattr(iterator, 'set_postfix'):
                iterator.set_postfix({"æˆåŠŸ": success_count, "å½“å‰": stock})
            
            # æ‰¹æ¬¡ä¼‘æ¯ï¼ˆé¿å…è§¦å‘é¢‘ç‡é™åˆ¶ï¼‰
            if (i + 1) % batch_size == 0 and (i + 1) < total:
                if show_progress and hasattr(iterator, 'set_description'):
                    iterator.set_description(f"ğŸ“Š ä¼‘æ¯{batch_sleep}s")
                time.sleep(batch_sleep)
                if show_progress and hasattr(iterator, 'set_description'):
                    iterator.set_description("ğŸ“Š è·å–æ—¥çº¿æ•°æ®")
        
        if not all_data:
            return pd.DataFrame()
        
        result = pd.concat(all_data, ignore_index=True)
        logger.info(f"æ‰¹é‡è·å–æ—¥çº¿æ•°æ®å®Œæˆ: {success_count}/{total} åªè‚¡ç¥¨æˆåŠŸ, {len(result)} æ¡è®°å½•")
        return result
    
    # ==================== è´¢åŠ¡æŒ‡æ ‡ ====================
    
    def fetch_financial_indicator(
        self,
        stock_code: str,
        period: Optional[str] = None
    ) -> Optional[pd.DataFrame]:
        """
        è·å–å•åªè‚¡ç¥¨è´¢åŠ¡æŒ‡æ ‡
        
        Parameters
        ----------
        stock_code : str
            è‚¡ç¥¨ä»£ç ï¼ˆ6ä½ï¼‰
        period : Optional[str]
            æŠ¥å‘ŠæœŸï¼Œæ ¼å¼ YYYYMMDDï¼Œå¦‚ "20231231"
            å¦‚æœä¸æä¾›ï¼Œè¿”å›æœ€è¿‘ 8 ä¸ªå­£åº¦çš„æ•°æ®
        
        Returns
        -------
        Optional[pd.DataFrame]
            è´¢åŠ¡æŒ‡æ ‡æ•°æ®ï¼ŒåŒ…å«ï¼š
            - roe: å‡€èµ„äº§æ”¶ç›Šç‡
            - roe_dt: æ‰£éå‡€èµ„äº§æ”¶ç›Šç‡
            - roa: æ€»èµ„äº§æ”¶ç›Šç‡
            - gross_margin: æ¯›åˆ©ç‡
            - profit_to_gr: å‡€åˆ©ç‡
            - eps: æ¯è‚¡æ”¶ç›Š
            - bps: æ¯è‚¡å‡€èµ„äº§
        """
        ts_code = self._to_ts_code(stock_code)
        
        # å°è¯•ç¼“å­˜
        cache_file = self.cache_dir / f"fina_{stock_code}.parquet"
        cache_valid = False
        
        if cache_file.exists():
            try:
                cache_mtime = datetime.fromtimestamp(cache_file.stat().st_mtime)
                # ç¼“å­˜æœ‰æ•ˆæœŸ 7 å¤©
                if (datetime.now() - cache_mtime).days < 7:
                    df = pd.read_parquet(cache_file)
                    if not df.empty:
                        logger.debug(f"ä»ç¼“å­˜åŠ è½½è´¢åŠ¡æŒ‡æ ‡: {stock_code}")
                        cache_valid = True
                        return self._standardize_financial_columns(df)
            except Exception:
                pass
        
        # API è·å–
        df = self._fetch_with_retry(
            self.pro.fina_indicator,
            ts_code=ts_code,
            period=period
        )
        
        if df is None or df.empty:
            logger.debug(f"è·å–è´¢åŠ¡æŒ‡æ ‡å¤±è´¥: {stock_code}")
            return None
        
        # ä¿å­˜ç¼“å­˜
        try:
            df.to_parquet(cache_file, index=False)
        except Exception:
            pass
        
        return self._standardize_financial_columns(df)
    
    def fetch_daily_basic(
        self,
        trade_date: Optional[str] = None,
        stock_list: Optional[List[str]] = None
    ) -> Optional[pd.DataFrame]:
        """
        è·å–æ¯æ—¥åŸºç¡€æŒ‡æ ‡ï¼ˆPE, PB, å¸‚å€¼ç­‰ï¼‰
        
        è¿™æ˜¯è·å–ä¼°å€¼æ•°æ®æœ€é«˜æ•ˆçš„æ–¹å¼ï¼Œä¸€æ¬¡è¯·æ±‚è·å–å…¨å¸‚åœºæ•°æ®ã€‚
        
        Parameters
        ----------
        trade_date : Optional[str]
            äº¤æ˜“æ—¥æœŸï¼Œæ ¼å¼ YYYYMMDDï¼Œé»˜è®¤æœ€è¿‘äº¤æ˜“æ—¥
        stock_list : Optional[List[str]]
            è‚¡ç¥¨åˆ—è¡¨ï¼Œç”¨äºè¿‡æ»¤ç»“æœ
        
        Returns
        -------
        Optional[pd.DataFrame]
            åŸºç¡€æŒ‡æ ‡æ•°æ®ï¼ŒåŒ…å«ï¼š
            - pe_ttm: å¸‚ç›ˆç‡ TTM
            - pb: å¸‚å‡€ç‡
            - ps_ttm: å¸‚é”€ç‡ TTM
            - dv_ttm: è‚¡æ¯ç‡ TTM
            - total_mv: æ€»å¸‚å€¼ï¼ˆä¸‡å…ƒï¼‰
            - circ_mv: æµé€šå¸‚å€¼ï¼ˆä¸‡å…ƒï¼‰
            - turnover_rate: æ¢æ‰‹ç‡
        """
        if trade_date is None:
            trade_date = (datetime.now() - timedelta(days=1)).strftime("%Y%m%d")
        
        # å°è¯•ç¼“å­˜
        cache_file = self.cache_dir / f"daily_basic_{trade_date}.parquet"
        
        if cache_file.exists():
            try:
                df = pd.read_parquet(cache_file)
                if not df.empty:
                    logger.info(f"ä»ç¼“å­˜åŠ è½½æ¯æ—¥åŸºç¡€æŒ‡æ ‡: {trade_date}, {len(df)} æ¡")
                    if stock_list:
                        df = df[df["ts_code"].str[:6].isin(stock_list)]
                    return self._standardize_basic_columns(df)
            except Exception as e:
                logger.warning(f"ç¼“å­˜è¯»å–å¤±è´¥: {e}")
        
        # API è·å–
        df = self._fetch_with_retry(
            self.pro.daily_basic,
            trade_date=trade_date
        )
        
        if df is None or df.empty:
            # å°è¯•å‰å‡ å¤©
            for days_ago in range(1, 8):
                alt_date = (datetime.strptime(trade_date, "%Y%m%d") - timedelta(days=days_ago)).strftime("%Y%m%d")
                df = self._fetch_with_retry(
                    self.pro.daily_basic,
                    trade_date=alt_date
                )
                if df is not None and not df.empty:
                    logger.info(f"ä½¿ç”¨ {alt_date} çš„åŸºç¡€æŒ‡æ ‡æ•°æ®")
                    break
        
        if df is None or df.empty:
            logger.warning(f"æ— æ³•è·å–æ¯æ—¥åŸºç¡€æŒ‡æ ‡: {trade_date}")
            return None
        
        # ä¿å­˜ç¼“å­˜
        try:
            df.to_parquet(cache_file, index=False)
            logger.info(f"æ¯æ—¥åŸºç¡€æŒ‡æ ‡å·²ç¼“å­˜: {cache_file}")
        except Exception as e:
            logger.warning(f"ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
        
        if stock_list:
            df = df[df["ts_code"].str[:6].isin(stock_list)]
        
        return self._standardize_basic_columns(df)
    
    def fetch_financial_batch(
        self,
        stock_list: List[str],
        show_progress: bool = True,
        batch_size: int = 150,
        batch_sleep: float = 8.0
    ) -> pd.DataFrame:
        """
        æ‰¹é‡è·å–è´¢åŠ¡æŒ‡æ ‡ï¼ˆå¸¦é™æµä¿æŠ¤ï¼‰
        
        Parameters
        ----------
        stock_list : List[str]
            è‚¡ç¥¨ä»£ç åˆ—è¡¨
        show_progress : bool
            æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
        batch_size : int
            æ¯æ‰¹æ¬¡å¤„ç†çš„è‚¡ç¥¨æ•°é‡ï¼ˆé»˜è®¤ 100ï¼‰
        batch_sleep : float
            æ¯æ‰¹æ¬¡ä¹‹é—´çš„ä¼‘æ¯æ—¶é—´ï¼ˆç§’ï¼‰
        
        Returns
        -------
        pd.DataFrame
            åˆå¹¶åçš„è´¢åŠ¡æŒ‡æ ‡æ•°æ®
        """
        all_data = []
        total = len(stock_list)
        success_count = 0
        
        # ä½¿ç”¨ tqdm è¿›åº¦æ¡
        if show_progress:
            try:
                from tqdm import tqdm
                iterator = tqdm(
                    enumerate(stock_list), 
                    total=total, 
                    desc="ğŸ“ˆ è·å–è´¢åŠ¡æŒ‡æ ‡",
                    unit="åª",
                    ncols=80
                )
            except ImportError:
                iterator = enumerate(stock_list)
                logger.info(f"å¼€å§‹è·å–è´¢åŠ¡æŒ‡æ ‡: {total} åªè‚¡ç¥¨...")
        else:
            iterator = enumerate(stock_list)
        
        for i, stock in iterator:
            df = self.fetch_financial_indicator(stock)
            if df is not None and not df.empty:
                # åªå–æœ€æ–°ä¸€æœŸ
                df = df.sort_values("end_date", ascending=False).head(1)
                df["stock_code"] = stock
                all_data.append(df)
                success_count += 1
            
            # æ›´æ–°è¿›åº¦æ¡åç¼€
            if show_progress and hasattr(iterator, 'set_postfix'):
                iterator.set_postfix({"æˆåŠŸ": success_count, "å½“å‰": stock})
            
            # æ‰¹æ¬¡ä¼‘æ¯ï¼ˆé¿å…è§¦å‘é¢‘ç‡é™åˆ¶ï¼‰
            if (i + 1) % batch_size == 0 and (i + 1) < total:
                if show_progress and hasattr(iterator, 'set_description'):
                    iterator.set_description(f"ğŸ“ˆ ä¼‘æ¯{batch_sleep}s")
                time.sleep(batch_sleep)
                if show_progress and hasattr(iterator, 'set_description'):
                    iterator.set_description("ğŸ“ˆ è·å–è´¢åŠ¡æŒ‡æ ‡")
        
        if not all_data:
            return pd.DataFrame()
        
        # è¿‡æ»¤æ‰å…¨ç©ºçš„ DataFrameï¼Œé¿å… FutureWarning
        valid_data = [df for df in all_data if not df.isna().all().all()]
        if not valid_data:
            return pd.DataFrame()
        
        result = pd.concat(valid_data, ignore_index=True)
        logger.info(f"æ‰¹é‡è·å–è´¢åŠ¡æŒ‡æ ‡å®Œæˆ: {success_count}/{total} åªè‚¡ç¥¨æˆåŠŸ, {len(result)} æ¡è®°å½•")
        return result
    
    # ==================== æŒ‡æ•°æ—¥çº¿ ====================
    
    def fetch_index_daily(
        self,
        index_code: str,
        start_date: str,
        end_date: str
    ) -> Optional[pd.DataFrame]:
        """
        è·å–æŒ‡æ•°æ—¥çº¿æ•°æ®
        
        Parameters
        ----------
        index_code : str
            æŒ‡æ•°ä»£ç ï¼Œå¦‚ "000300" æˆ– "hs300"
        start_date : str
            å¼€å§‹æ—¥æœŸ
        end_date : str
            ç»“æŸæ—¥æœŸ
        
        Returns
        -------
        Optional[pd.DataFrame]
            æŒ‡æ•°æ—¥çº¿æ•°æ®
        """
        # è½¬æ¢æŒ‡æ•°ä»£ç 
        if index_code.lower() in self.INDEX_CODE_MAPPING:
            ts_code = self.INDEX_CODE_MAPPING[index_code.lower()]
        elif "." in index_code:
            ts_code = index_code
        else:
            # å‡è®¾æ˜¯ä¸Šè¯æŒ‡æ•°
            ts_code = f"{index_code}.SH"
        
        start_date = start_date.replace("-", "")
        end_date = end_date.replace("-", "")
        
        df = self._fetch_with_retry(
            self.pro.index_daily,
            ts_code=ts_code,
            start_date=start_date,
            end_date=end_date
        )
        
        if df is None or df.empty:
            return None
        
        return self._standardize_daily_columns(df)
    
    # ==================== è¾…åŠ©æ–¹æ³• ====================
    
    def _to_ts_code(self, stock_code: str) -> str:
        """
        è½¬æ¢è‚¡ç¥¨ä»£ç ä¸º Tushare æ ¼å¼
        
        Parameters
        ----------
        stock_code : str
            6ä½è‚¡ç¥¨ä»£ç 
        
        Returns
        -------
        str
            Tushare æ ¼å¼ä»£ç ï¼Œå¦‚ "000001.SZ"
        """
        if "." in stock_code:
            return stock_code
        
        code = stock_code.strip()
        
        # æ ¹æ®é¦–ä½åˆ¤æ–­äº¤æ˜“æ‰€
        if code.startswith(("6", "5")):
            return f"{code}.SH"
        elif code.startswith(("0", "3", "2")):
            return f"{code}.SZ"
        elif code.startswith("8") or code.startswith("4"):
            return f"{code}.BJ"  # åŒ—äº¤æ‰€
        else:
            return f"{code}.SZ"
    
    def _standardize_daily_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ ‡å‡†åŒ–æ—¥çº¿æ•°æ®åˆ—å"""
        column_mapping = {
            "trade_date": "date",
            "ts_code": "ts_code",
            "open": "open",
            "high": "high",
            "low": "low",
            "close": "close",
            "vol": "volume",
            "amount": "amount",
            "pct_chg": "pct_change",
            "change": "change",
        }
        
        df = df.rename(columns=column_mapping)
        
        if "date" in df.columns:
            df["date"] = pd.to_datetime(df["date"])
            df = df.sort_values("date")
        
        # æˆäº¤é‡å•ä½è½¬æ¢ï¼ˆTushare å•ä½æ˜¯æ‰‹ï¼Œè½¬ä¸ºè‚¡ï¼‰
        if "volume" in df.columns:
            df["volume"] = df["volume"] * 100
        
        # æˆäº¤é¢å•ä½è½¬æ¢ï¼ˆTushare å•ä½æ˜¯åƒå…ƒï¼Œè½¬ä¸ºå…ƒï¼‰
        if "amount" in df.columns:
            df["amount"] = df["amount"] * 1000
        
        return df
    
    def _standardize_basic_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ ‡å‡†åŒ–æ¯æ—¥åŸºç¡€æŒ‡æ ‡åˆ—å"""
        column_mapping = {
            "trade_date": "date",
            "ts_code": "ts_code",
            "pe_ttm": "pe_ttm",
            "pe": "pe",
            "pb": "pb",
            "ps_ttm": "ps_ttm",
            "dv_ttm": "dividend_yield",
            "dv_ratio": "dividend_yield",
            "total_mv": "total_mv",
            "circ_mv": "circ_mv",
            "turnover_rate": "turn",
            "turnover_rate_f": "turn_free",
        }
        
        df = df.rename(columns=column_mapping)
        
        # æå– 6 ä½è‚¡ç¥¨ä»£ç 
        if "ts_code" in df.columns:
            df["stock_code"] = df["ts_code"].str[:6]
        
        if "date" in df.columns:
            df["date"] = pd.to_datetime(df["date"])
        
        # å¸‚å€¼å•ä½è½¬æ¢ï¼ˆä¸‡å…ƒ -> å…ƒï¼‰
        for col in ["total_mv", "circ_mv"]:
            if col in df.columns:
                df[col] = df[col] * 10000
        
        return df
    
    def _standardize_financial_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ ‡å‡†åŒ–è´¢åŠ¡æŒ‡æ ‡åˆ—å"""
        column_mapping = {
            "ts_code": "ts_code",
            "ann_date": "ann_date",
            "end_date": "end_date",
            "roe": "roe",
            "roe_dt": "roe_dt",
            "roe_yearly": "roe_ttm",
            "roa": "roa",
            "grossprofit_margin": "gross_margin",
            "profit_to_gr": "net_margin",
            "eps": "eps",
            "bps": "bps",
            "netprofit_margin": "net_margin",
            "current_ratio": "current_ratio",
            "quick_ratio": "quick_ratio",
        }
        
        df = df.rename(columns=column_mapping)
        
        # æå– 6 ä½è‚¡ç¥¨ä»£ç 
        if "ts_code" in df.columns:
            df["stock_code"] = df["ts_code"].str[:6]
        
        return df
    
    # ==================== æ–°é—»èµ„è®¯ ====================
    
    def fetch_news(
        self,
        stock_code: Optional[str] = None,
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
        src: str = "sina"
    ) -> Optional[pd.DataFrame]:
        """
        è·å–æ–°é—»èµ„è®¯æ•°æ®
        
        ä½¿ç”¨ Tushare Pro news æ¥å£è·å–è´¢ç»æ–°é—»ã€‚
        
        Parameters
        ----------
        stock_code : Optional[str]
            è‚¡ç¥¨ä»£ç ï¼ˆ6ä½ï¼‰ï¼Œå¦‚æœæä¾›åˆ™è¿‡æ»¤ç›¸å…³æ–°é—»
        start_date : Optional[str]
            å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼ YYYYMMDD
        end_date : Optional[str]
            ç»“æŸæ—¥æœŸï¼Œæ ¼å¼ YYYYMMDD
        src : str
            æ–°é—»æ¥æºï¼Œå¯é€‰ï¼šsina(æ–°æµª), wallstreetcn(åå°”è¡—è§é—»), 
            10jqka(åŒèŠ±é¡º), eastmoney(ä¸œæ–¹è´¢å¯Œ), yuncaijing(äº‘è´¢ç»)
            é»˜è®¤ sina
        
        Returns
        -------
        Optional[pd.DataFrame]
            æ–°é—»æ•°æ®ï¼ŒåŒ…å« datetime, title, content, channels ç­‰å­—æ®µ
            å¤±è´¥è¿”å› None
        
        Notes
        -----
        - Tushare Pro æ–°é—»æ¥å£éœ€è¦è¾ƒé«˜ç§¯åˆ†æƒé™
        - å¦‚æœæ¥å£ä¸å¯ç”¨ï¼Œä¼šè¿”å›ç©º DataFrame
        
        Examples
        --------
        >>> loader = TushareDataLoader()
        >>> news = loader.fetch_news(start_date="20240101", end_date="20240115")
        >>> print(news[['datetime', 'title']].head())
        """
        # æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼
        if start_date:
            start_date = start_date.replace("-", "")
        if end_date:
            end_date = end_date.replace("-", "")
        
        global _GLOBAL_NEWS_API_LAST_CALL, _GLOBAL_NEWS_RATE_LIMIT_COUNT
        
        # æ£€æŸ¥æ˜¯å¦åœ¨é…ç½®ä¸­è·³è¿‡æ–°é—»è·å–
        if getattr(self, '_skip_news', False):
            logger.debug("æ–°é—»è·å–å·²åœ¨é…ç½®ä¸­ç¦ç”¨ (tushare.skip_news=true)")
            return pd.DataFrame()
        
        # æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡æ–°é—»è·å–ï¼ˆé¢‘ç‡é™åˆ¶ä¿æŠ¤ï¼‰
        if _GLOBAL_NEWS_RATE_LIMIT_COUNT >= 3:
            logger.warning("æ–°é—»æ¥å£é¢‘ç¹è§¦å‘é™åˆ¶ï¼Œæœ¬æ¬¡è·³è¿‡ï¼ˆéœ€è¦æ›´é«˜ç§¯åˆ†æƒé™ï¼‰")
            return pd.DataFrame()
        
        # å°è¯•ç¼“å­˜ï¼ˆæ–°é—»æŒ‰æ—¥æœŸå’Œæ¥æºç¼“å­˜ï¼‰
        cache_key = f"news_{src}_{start_date}_{end_date}"
        if stock_code:
            cache_key += f"_{stock_code.replace('.', '')[:6]}"
        cache_file = self.cache_dir / f"{cache_key}.parquet"
        
        if cache_file.exists():
            try:
                # æ£€æŸ¥ç¼“å­˜æ˜¯å¦åœ¨24å°æ—¶å†…
                cache_mtime = datetime.fromtimestamp(cache_file.stat().st_mtime)
                if (datetime.now() - cache_mtime).total_seconds() < 86400:  # 24å°æ—¶
                    df = pd.read_parquet(cache_file)
                    if not df.empty:
                        logger.info(f"ä»ç¼“å­˜åŠ è½½æ–°é—»: {len(df)} æ¡")
                        return df
            except Exception:
                pass
        
        # æ–°é—»æ¥å£ç‰¹æ®Šé™æµï¼šæ¯åˆ†é’Ÿæœ€å¤š 1 æ¬¡ï¼ˆä½¿ç”¨å…¨å±€å˜é‡è·¨å®ä¾‹å…±äº«ï¼‰
        elapsed = time.time() - _GLOBAL_NEWS_API_LAST_CALL
        if elapsed < self.NEWS_API_INTERVAL:
            wait_time = self.NEWS_API_INTERVAL - elapsed
            logger.info(f"â³ æ–°é—»æ¥å£é™æµï¼ˆæ¯åˆ†é’Ÿ1æ¬¡ï¼‰ï¼Œç­‰å¾… {wait_time:.0f} ç§’...")
            time.sleep(wait_time)
        
        logger.info(f"è·å–æ–°é—»èµ„è®¯: src={src}, {start_date} ~ {end_date}")
        
        try:
            # æ›´æ–°å…¨å±€æœ€åè°ƒç”¨æ—¶é—´
            _GLOBAL_NEWS_API_LAST_CALL = time.time()
            
            df = self._fetch_with_retry(
                self.pro.news,
                src=src,
                start_date=start_date,
                end_date=end_date
            )
            
            # æˆåŠŸåˆ™é‡ç½®å…¨å±€è®¡æ•°å™¨
            _GLOBAL_NEWS_RATE_LIMIT_COUNT = 0
            
            if df is None or df.empty:
                logger.debug("æ— æ–°é—»æ•°æ®")
                return pd.DataFrame()
            
            # å¦‚æœæŒ‡å®šäº†è‚¡ç¥¨ä»£ç ï¼Œå°è¯•è¿‡æ»¤ç›¸å…³æ–°é—»
            if stock_code:
                # åœ¨æ ‡é¢˜æˆ–å†…å®¹ä¸­æœç´¢è‚¡ç¥¨ä»£ç æˆ–åç§°
                stock_code_clean = stock_code.replace(".", "")[:6]
                mask = (
                    df["title"].str.contains(stock_code_clean, na=False) |
                    df["content"].str.contains(stock_code_clean, na=False)
                )
                df = df[mask]
            
            # ä¿å­˜ç¼“å­˜
            if not df.empty:
                try:
                    df.to_parquet(cache_file, index=False)
                    logger.debug(f"æ–°é—»å·²ç¼“å­˜: {cache_file.name}")
                except Exception:
                    pass
            
            logger.info(f"è·å–æ–°é—»æˆåŠŸ: {len(df)} æ¡")
            return df
            
        except Exception as e:
            error_msg = str(e)
            # è®°å½•é¢‘ç‡é™åˆ¶ï¼ˆä½¿ç”¨å…¨å±€å˜é‡ï¼‰
            if "æ¯å°æ—¶" in error_msg:
                # æ¯å°æ—¶é™åˆ¶ - æœ¬æ¬¡ä¼šè¯å†…ä¸å†å°è¯•
                _GLOBAL_NEWS_RATE_LIMIT_COUNT = 10  # è®¾ç½®é«˜å€¼ç›´æ¥è·³è¿‡
                logger.warning(f"âš ï¸ æ–°é—»æ¥å£æ¯å°æ—¶é™åˆ¶å·²è¾¾ä¸Šé™ï¼Œæœ¬æ¬¡è·³è¿‡æ–°é—»è·å–")
                logger.warning(f"   æç¤ºï¼šå¯åœ¨é…ç½®ä¸­è®¾ç½® llm.enable_sentiment_filter: false æš‚æ—¶ç¦ç”¨æƒ…ç»ªåˆ†æ")
            elif "æ¯åˆ†é’Ÿ" in error_msg or "é¢‘ç‡" in error_msg.lower() or "æŠ±æ­‰" in error_msg:
                _GLOBAL_NEWS_RATE_LIMIT_COUNT += 1
                logger.warning(f"æ–°é—»æ¥å£é¢‘ç‡é™åˆ¶ ({_GLOBAL_NEWS_RATE_LIMIT_COUNT}/3): {e}")
            else:
                logger.warning(f"è·å–æ–°é—»å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def fetch_all_news_once(
        self,
        days_back: int = 7,
        src: str = "sina"
    ) -> pd.DataFrame:
        """
        ä¸€æ¬¡æ€§è·å–æ‰€æœ‰æ–°é—»ï¼ˆä¼˜åŒ–ï¼šé¿å…å¤šæ¬¡ API è°ƒç”¨ï¼‰
        
        è·å–æœ€è¿‘å‡ å¤©çš„æ‰€æœ‰æ–°é—»ï¼Œç¼“å­˜åä¾›å¤šåªè‚¡ç¥¨ä½¿ç”¨ã€‚
        æ–°é—»æ¥å£æ¯åˆ†é’Ÿåªèƒ½è°ƒç”¨1æ¬¡ï¼Œå› æ­¤ä¸€æ¬¡è·å–å…¨éƒ¨æ•°æ®æ›´é«˜æ•ˆã€‚
        
        Parameters
        ----------
        days_back : int
            å›æº¯å¤©æ•°ï¼Œé»˜è®¤ 7 å¤©
        src : str
            æ–°é—»æºï¼Œé»˜è®¤ sina
        
        Returns
        -------
        pd.DataFrame
            æ‰€æœ‰æ–°é—»æ•°æ®
        """
        end_date = datetime.now().strftime("%Y%m%d")
        start_date = (datetime.now() - timedelta(days=days_back)).strftime("%Y%m%d")
        
        # ä½¿ç”¨å®ä¾‹å˜é‡ç¼“å­˜ï¼Œé¿å…é‡å¤è°ƒç”¨
        cache_key = f"_cached_all_news_{src}_{start_date}_{end_date}"
        if hasattr(self, cache_key):
            cached = getattr(self, cache_key)
            if cached is not None:
                logger.debug(f"ä½¿ç”¨å†…å­˜ç¼“å­˜çš„æ–°é—»æ•°æ®: {len(cached)} æ¡")
                return cached
        
        # è·å–æ‰€æœ‰æ–°é—»ï¼ˆä¸å¸¦è‚¡ç¥¨ä»£ç è¿‡æ»¤ï¼‰
        df = self.fetch_news(
            stock_code=None,  # ä¸è¿‡æ»¤ï¼Œè·å–å…¨éƒ¨
            start_date=start_date,
            end_date=end_date,
            src=src
        )
        
        # ç¼“å­˜åˆ°å®ä¾‹å˜é‡
        setattr(self, cache_key, df if df is not None else pd.DataFrame())
        
        if df is not None and not df.empty:
            logger.info(f"ğŸ“° ä¸€æ¬¡æ€§è·å–æ–°é—»å®Œæˆ: {len(df)} æ¡ï¼Œå¯ä¾›æ‰€æœ‰è‚¡ç¥¨ä½¿ç”¨")
        
        return df if df is not None else pd.DataFrame()
    
    def fetch_stock_news(
        self,
        stock_code: str,
        days_back: int = 7
    ) -> str:
        """
        è·å–å•åªè‚¡ç¥¨ç›¸å…³æ–°é—»ï¼ˆç”¨äºæƒ…æ„Ÿåˆ†æï¼‰
        
        ä»ç¼“å­˜çš„å…¨é‡æ–°é—»ä¸­ç­›é€‰ä¸æŒ‡å®šè‚¡ç¥¨ç›¸å…³çš„æ–°é—»ã€‚
        ä¼˜åŒ–ï¼šåªè°ƒç”¨ä¸€æ¬¡ API è·å–å…¨é‡æ–°é—»ï¼Œç„¶åæœ¬åœ°ç­›é€‰ã€‚
        
        Parameters
        ----------
        stock_code : str
            è‚¡ç¥¨ä»£ç ï¼ˆ6ä½ï¼‰
        days_back : int
            å›æº¯å¤©æ•°ï¼Œé»˜è®¤ 7 å¤©
        
        Returns
        -------
        str
            åˆå¹¶çš„æ–°é—»æ–‡æœ¬ï¼Œç”¨äºæƒ…æ„Ÿåˆ†æ
            æ— æ–°é—»æ—¶è¿”å›ç©ºå­—ç¬¦ä¸²
        """
        # å…ˆè·å–å…¨é‡æ–°é—»ï¼ˆä¼šè‡ªåŠ¨ç¼“å­˜ï¼Œåªè°ƒç”¨ä¸€æ¬¡ APIï¼‰
        all_news_df = self.fetch_all_news_once(days_back=days_back)
        
        if all_news_df.empty:
            logger.debug(f"æ— æ–°é—»æ•°æ®å¯ç”¨")
            return ""
        
        # ä»å…¨é‡æ–°é—»ä¸­ç­›é€‰ä¸è¯¥è‚¡ç¥¨ç›¸å…³çš„
        stock_code_clean = stock_code.replace(".", "")[:6]
        
        # åœ¨æ ‡é¢˜æˆ–å†…å®¹ä¸­æœç´¢è‚¡ç¥¨ä»£ç 
        mask = pd.Series([False] * len(all_news_df))
        if "title" in all_news_df.columns:
            mask = mask | all_news_df["title"].str.contains(stock_code_clean, na=False)
        if "content" in all_news_df.columns:
            mask = mask | all_news_df["content"].str.contains(stock_code_clean, na=False)
        
        filtered_df = all_news_df[mask]
        
        if filtered_df.empty:
            logger.debug(f"è‚¡ç¥¨ {stock_code} æ— ç›¸å…³æ–°é—»")
            return ""
        
        # æå–æ ‡é¢˜å’Œå†…å®¹
        all_news = []
        for _, row in filtered_df.head(5).iterrows():
            title = row.get("title", "")
            content = row.get("content", "")
            if title:
                all_news.append(str(title))
            if content and len(str(content)) < 500:
                all_news.append(str(content)[:200])
        
        if not all_news:
            return ""
        
        # åˆå¹¶æ–°é—»æ–‡æœ¬
        combined = " | ".join(all_news)
        
        # æˆªæ–­
        if len(combined) > 1500:
            combined = combined[:1500] + "..."
        
        logger.debug(f"è·å–è‚¡ç¥¨æ–°é—»æˆåŠŸ: {stock_code}, {len(all_news)} æ¡")
        return combined
    
    # ==================== äº¤æ˜“æ—¥å† ====================
    
    def fetch_trade_calendar(
        self,
        start_date: str,
        end_date: str,
        exchange: str = "SSE"
    ) -> pd.DatetimeIndex:
        """
        è·å–äº¤æ˜“æ—¥å†
        
        Parameters
        ----------
        start_date : str
            å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼ YYYY-MM-DD æˆ– YYYYMMDD
        end_date : str
            ç»“æŸæ—¥æœŸï¼Œæ ¼å¼ YYYY-MM-DD æˆ– YYYYMMDD
        exchange : str
            äº¤æ˜“æ‰€ï¼ŒSSE(ä¸Šäº¤æ‰€ï¼Œé»˜è®¤) æˆ– SZSE(æ·±äº¤æ‰€)
        
        Returns
        -------
        pd.DatetimeIndex
            äº¤æ˜“æ—¥æœŸç´¢å¼•
        
        Examples
        --------
        >>> loader = TushareDataLoader()
        >>> calendar = loader.fetch_trade_calendar("2024-01-01", "2024-12-31")
        >>> print(f"2024å¹´å…± {len(calendar)} ä¸ªäº¤æ˜“æ—¥")
        """
        # æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼
        start_date = start_date.replace("-", "")
        end_date = end_date.replace("-", "")
        
        logger.info(f"è·å–äº¤æ˜“æ—¥å†: {start_date} ~ {end_date}")
        
        # å°è¯•ç¼“å­˜
        cache_file = self.cache_dir / f"trade_cal_{start_date[:4]}.parquet"
        
        if cache_file.exists():
            try:
                df = pd.read_parquet(cache_file)
                # è¿‡æ»¤æ—¥æœŸèŒƒå›´
                df = df[
                    (df["cal_date"] >= start_date) & 
                    (df["cal_date"] <= end_date) &
                    (df["is_open"] == 1)
                ]
                if not df.empty:
                    calendar = pd.to_datetime(df["cal_date"])
                    logger.debug(f"ä»ç¼“å­˜åŠ è½½äº¤æ˜“æ—¥å†: {len(calendar)} å¤©")
                    return pd.DatetimeIndex(sorted(calendar))
            except Exception:
                pass
        
        # API è·å–
        df = self._fetch_with_retry(
            self.pro.trade_cal,
            exchange=exchange,
            start_date=start_date,
            end_date=end_date
        )
        
        if df is None or df.empty:
            logger.warning("æ— æ³•è·å–äº¤æ˜“æ—¥å†ï¼Œä½¿ç”¨å·¥ä½œæ—¥è¿‘ä¼¼")
            return pd.bdate_range(start=start_date, end=end_date)
        
        # ä¿å­˜ç¼“å­˜ï¼ˆæ•´å¹´æ•°æ®ï¼‰
        try:
            full_year_df = self._fetch_with_retry(
                self.pro.trade_cal,
                exchange=exchange,
                start_date=f"{start_date[:4]}0101",
                end_date=f"{start_date[:4]}1231"
            )
            if full_year_df is not None and not full_year_df.empty:
                full_year_df.to_parquet(cache_file, index=False)
        except Exception:
            pass
        
        # è¿‡æ»¤äº¤æ˜“æ—¥
        trade_days = df[df["is_open"] == 1]["cal_date"]
        calendar = pd.to_datetime(trade_days)
        calendar = pd.DatetimeIndex(sorted(calendar))
        calendar.name = "date"
        
        logger.info(f"è·å–äº¤æ˜“æ—¥å†æˆåŠŸ: {len(calendar)} ä¸ªäº¤æ˜“æ—¥")
        return calendar
    
    def is_trade_day(self, date: Optional[str] = None) -> bool:
        """
        åˆ¤æ–­æŒ‡å®šæ—¥æœŸæ˜¯å¦ä¸ºäº¤æ˜“æ—¥
        
        Parameters
        ----------
        date : Optional[str]
            æ—¥æœŸï¼Œæ ¼å¼ YYYY-MM-DD æˆ– YYYYMMDDï¼Œé»˜è®¤ä»Šå¤©
        
        Returns
        -------
        bool
            æ˜¯å¦ä¸ºäº¤æ˜“æ—¥
        """
        from datetime import datetime
        
        if date is None:
            date = datetime.now().strftime("%Y%m%d")
        else:
            date = date.replace("-", "")
        
        calendar = self.fetch_trade_calendar(date, date)
        return len(calendar) > 0
    
    # ==================== è¡Œä¸šåˆ†ç±» ====================
    
    def fetch_industry_mapping(
        self,
        use_cache: bool = True
    ) -> Dict[str, str]:
        """
        è·å–è‚¡ç¥¨è¡Œä¸šåˆ†ç±»æ˜ å°„
        
        è¿”å›è‚¡ç¥¨ä»£ç åˆ°è¡Œä¸šåç§°çš„æ˜ å°„å­—å…¸ã€‚
        
        Parameters
        ----------
        use_cache : bool
            æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼Œé»˜è®¤ True
        
        Returns
        -------
        Dict[str, str]
            è‚¡ç¥¨ä»£ç ï¼ˆ6ä½ï¼‰åˆ°è¡Œä¸šåç§°çš„æ˜ å°„
        
        Examples
        --------
        >>> loader = TushareDataLoader()
        >>> industry_map = loader.fetch_industry_mapping()
        >>> print(industry_map.get("000001"))  # é“¶è¡Œ
        """
        logger.info("è·å–è‚¡ç¥¨è¡Œä¸šåˆ†ç±»æ˜ å°„")
        
        # å°è¯•ç¼“å­˜
        today = datetime.now().strftime("%Y%m%d")
        cache_file = self.cache_dir / f"industry_mapping_{today[:6]}.parquet"
        
        if use_cache and cache_file.exists():
            try:
                df = pd.read_parquet(cache_file)
                mapping = dict(zip(df["stock_code"], df["industry"]))
                logger.info(f"ä»ç¼“å­˜åŠ è½½è¡Œä¸šæ˜ å°„: {len(mapping)} åªè‚¡ç¥¨")
                return mapping
            except Exception as e:
                logger.warning(f"ç¼“å­˜è¯»å–å¤±è´¥: {e}")
        
        # API è·å–
        df = self._fetch_with_retry(
            self.pro.stock_basic,
            list_status="L",
            fields="ts_code,symbol,name,industry,market,list_date"
        )
        
        if df is None or df.empty:
            logger.warning("æ— æ³•è·å–è¡Œä¸šåˆ†ç±»æ•°æ®")
            return {}
        
        # æå– 6 ä½è‚¡ç¥¨ä»£ç 
        df["stock_code"] = df["ts_code"].str[:6]
        
        # ä¿å­˜ç¼“å­˜
        try:
            df[["stock_code", "industry"]].to_parquet(cache_file, index=False)
            logger.info(f"è¡Œä¸šæ˜ å°„å·²ç¼“å­˜: {cache_file}")
        except Exception as e:
            logger.warning(f"ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
        
        # æ„å»ºæ˜ å°„
        mapping = dict(zip(df["stock_code"], df["industry"]))
        logger.info(f"è·å–è¡Œä¸šæ˜ å°„æˆåŠŸ: {len(mapping)} åªè‚¡ç¥¨")
        return mapping
    
    def fetch_sw_industry_mapping(
        self,
        level: int = 1
    ) -> Dict[str, str]:
        """
        è·å–ç”³ä¸‡è¡Œä¸šåˆ†ç±»æ˜ å°„
        
        Parameters
        ----------
        level : int
            è¡Œä¸šåˆ†ç±»çº§åˆ«ï¼š1(ä¸€çº§), 2(äºŒçº§), 3(ä¸‰çº§)
            é»˜è®¤ 1ï¼ˆä¸€çº§è¡Œä¸šï¼‰
        
        Returns
        -------
        Dict[str, str]
            è‚¡ç¥¨ä»£ç ï¼ˆ6ä½ï¼‰åˆ°ç”³ä¸‡è¡Œä¸šåç§°çš„æ˜ å°„
        
        Notes
        -----
        ç”³ä¸‡è¡Œä¸šåˆ†ç±»æ˜¯ A è‚¡æœ€å¸¸ç”¨çš„è¡Œä¸šåˆ†ç±»æ ‡å‡†ã€‚
        Tushare éœ€è¦è¾ƒé«˜æƒé™æ‰èƒ½ä½¿ç”¨ç”³ä¸‡è¡Œä¸šæ¥å£ã€‚
        """
        logger.info(f"è·å–ç”³ä¸‡ {level} çº§è¡Œä¸šåˆ†ç±»")
        
        # å°è¯•ä½¿ç”¨ stock_basic çš„ industry å­—æ®µï¼ˆé€šç”¨è¡Œä¸šåˆ†ç±»ï¼‰
        # å¦‚æœéœ€è¦ç²¾ç¡®çš„ç”³ä¸‡åˆ†ç±»ï¼Œéœ€è¦ä½¿ç”¨ index_member æ¥å£
        
        try:
            # å°è¯•è·å–ç”³ä¸‡æŒ‡æ•°æˆåˆ†
            df = self._fetch_with_retry(
                self.pro.index_classify,
                level=f"L{level}",
                src="SW"
            )
            
            if df is not None and not df.empty:
                # è·å–æ¯ä¸ªè¡Œä¸šçš„æˆåˆ†è‚¡
                result = {}
                for _, row in df.iterrows():
                    index_code = row.get("index_code", "")
                    industry_name = row.get("industry_name", "")
                    
                    if index_code:
                        members = self._fetch_with_retry(
                            self.pro.index_member,
                            index_code=index_code
                        )
                        if members is not None and not members.empty:
                            for stock in members["con_code"].str[:6]:
                                result[stock] = industry_name
                
                if result:
                    logger.info(f"è·å–ç”³ä¸‡è¡Œä¸šåˆ†ç±»æˆåŠŸ: {len(result)} åªè‚¡ç¥¨")
                    return result
                    
        except Exception as e:
            logger.debug(f"ç”³ä¸‡åˆ†ç±»æ¥å£ä¸å¯ç”¨: {e}")
        
        # é™çº§åˆ°æ™®é€šè¡Œä¸šåˆ†ç±»
        logger.info("ä½¿ç”¨æ™®é€šè¡Œä¸šåˆ†ç±»æ›¿ä»£ç”³ä¸‡åˆ†ç±»")
        return self.fetch_industry_mapping()
    
    # ==================== èµ„é‡‘æµå‘ä¸åŒ—å‘èµ„é‡‘ ====================
    
    def fetch_moneyflow(
        self,
        stock_code: str,
        start_date: str,
        end_date: str
    ) -> Optional[pd.DataFrame]:
        """
        è·å–ä¸ªè‚¡èµ„é‡‘æµå‘æ•°æ®ï¼ˆå¤§å•/è¶…å¤§å•ï¼‰
        
        ä½¿ç”¨ Tushare Pro moneyflow æ¥å£è·å–ä¸ªè‚¡èµ„é‡‘æµå‘æ˜ç»†æ•°æ®ï¼Œ
        åŒ…å«å¤§å•ã€è¶…å¤§å•ã€ä¸­å•ã€å°å•çš„ä¹°å…¥å–å‡ºé‡‘é¢ã€‚
        
        Parameters
        ----------
        stock_code : str
            è‚¡ç¥¨ä»£ç ï¼ˆ6ä½ï¼‰ï¼Œå¦‚ "000001"
        start_date : str
            å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼ YYYYMMDD æˆ– YYYY-MM-DD
        end_date : str
            ç»“æŸæ—¥æœŸï¼Œæ ¼å¼ YYYYMMDD æˆ– YYYY-MM-DD
        
        Returns
        -------
        Optional[pd.DataFrame]
            èµ„é‡‘æµå‘æ•°æ®ï¼ŒåŒ…å«ï¼š
            - trade_date: äº¤æ˜“æ—¥æœŸ
            - buy_elg_vol: ç‰¹å¤§å•ä¹°å…¥é‡ï¼ˆæ‰‹ï¼‰
            - buy_elg_amount: ç‰¹å¤§å•ä¹°å…¥é¢ï¼ˆä¸‡å…ƒï¼‰
            - sell_elg_vol: ç‰¹å¤§å•å–å‡ºé‡ï¼ˆæ‰‹ï¼‰
            - sell_elg_amount: ç‰¹å¤§å•å–å‡ºé¢ï¼ˆä¸‡å…ƒï¼‰
            - buy_lg_vol: å¤§å•ä¹°å…¥é‡ï¼ˆæ‰‹ï¼‰
            - buy_lg_amount: å¤§å•ä¹°å…¥é¢ï¼ˆä¸‡å…ƒï¼‰
            - sell_lg_vol: å¤§å•å–å‡ºé‡ï¼ˆæ‰‹ï¼‰
            - sell_lg_amount: å¤§å•å–å‡ºé¢ï¼ˆä¸‡å…ƒï¼‰
            - net_mf_vol: å‡€æµå…¥é‡ï¼ˆæ‰‹ï¼‰
            - net_mf_amount: å‡€æµå…¥é¢ï¼ˆä¸‡å…ƒï¼‰
            å¤±è´¥è¿”å› None
        
        Notes
        -----
        - Tushare Pro moneyflow æ¥å£éœ€è¦è¾ƒé«˜ç§¯åˆ†æƒé™ï¼ˆçº¦ 2000 ç§¯åˆ†ï¼‰
        - å¦‚æ— æƒé™å¯è€ƒè™‘ä½¿ç”¨ moneyflow_hsgt æ›¿ä»£éƒ¨åˆ†åŠŸèƒ½
        - å¤§å•å®šä¹‰ï¼š50ä¸‡-100ä¸‡ä¸ºå¤§å•ï¼Œ100ä¸‡ä»¥ä¸Šä¸ºç‰¹å¤§å•
        
        Examples
        --------
        >>> loader = TushareDataLoader()
        >>> flow = loader.fetch_moneyflow("000001", "20240101", "20240115")
        >>> # è®¡ç®—ä¸»åŠ›å‡€æµå…¥ï¼ˆå¤§å•+è¶…å¤§å•ï¼‰
        >>> main_net = flow['buy_elg_amount'] - flow['sell_elg_amount'] + \\
        ...            flow['buy_lg_amount'] - flow['sell_lg_amount']
        """
        # æ ‡å‡†åŒ–è‚¡ç¥¨ä»£ç 
        ts_code = self._to_ts_code(stock_code)
        
        # æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼
        start_date = start_date.replace("-", "")
        end_date = end_date.replace("-", "")
        
        logger.debug(f"è·å–èµ„é‡‘æµå‘: {stock_code}, {start_date} ~ {end_date}")
        
        # å°è¯•ç¼“å­˜
        cache_file = self.cache_dir / f"moneyflow_{stock_code}_{start_date}_{end_date}.parquet"
        if cache_file.exists():
            try:
                df = pd.read_parquet(cache_file)
                if not df.empty:
                    logger.debug(f"ä»ç¼“å­˜åŠ è½½èµ„é‡‘æµå‘: {stock_code}")
                    return df
            except Exception:
                pass
        
        # API è·å–
        df = self._fetch_with_retry(
            self.pro.moneyflow,
            ts_code=ts_code,
            start_date=start_date,
            end_date=end_date
        )
        
        if df is None or df.empty:
            logger.debug(f"è·å–èµ„é‡‘æµå‘å¤±è´¥: {stock_code}")
            return None
        
        # æ·»åŠ  6 ä½è‚¡ç¥¨ä»£ç 
        df["stock_code"] = df["ts_code"].str[:6]
        
        # æ—¥æœŸæ ‡å‡†åŒ–
        if "trade_date" in df.columns:
            df["trade_date"] = pd.to_datetime(df["trade_date"])
            df = df.sort_values("trade_date")
        
        # ä¿å­˜ç¼“å­˜
        try:
            df.to_parquet(cache_file, index=False)
        except Exception:
            pass
        
        logger.debug(f"è·å–èµ„é‡‘æµå‘æˆåŠŸ: {stock_code}, {len(df)} æ¡")
        return df
    
    def fetch_moneyflow_batch(
        self,
        stock_list: List[str],
        start_date: str,
        end_date: str,
        show_progress: bool = True,
        batch_size: int = 150,
        batch_sleep: float = 8.0
    ) -> pd.DataFrame:
        """
        æ‰¹é‡è·å–èµ„é‡‘æµå‘æ•°æ®
        
        Parameters
        ----------
        stock_list : List[str]
            è‚¡ç¥¨ä»£ç åˆ—è¡¨
        start_date : str
            å¼€å§‹æ—¥æœŸ
        end_date : str
            ç»“æŸæ—¥æœŸ
        show_progress : bool
            æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡
        batch_size : int
            æ¯æ‰¹æ¬¡å¤„ç†çš„è‚¡ç¥¨æ•°é‡
        batch_sleep : float
            æ¯æ‰¹æ¬¡ä¹‹é—´çš„ä¼‘æ¯æ—¶é—´ï¼ˆç§’ï¼‰
        
        Returns
        -------
        pd.DataFrame
            åˆå¹¶åçš„èµ„é‡‘æµå‘æ•°æ®
        """
        all_data = []
        total = len(stock_list)
        success_count = 0
        
        if show_progress:
            try:
                from tqdm import tqdm
                iterator = tqdm(
                    enumerate(stock_list),
                    total=total,
                    desc="ğŸ’° è·å–èµ„é‡‘æµå‘",
                    unit="åª",
                    ncols=80
                )
            except ImportError:
                iterator = enumerate(stock_list)
                logger.info(f"å¼€å§‹è·å–èµ„é‡‘æµå‘: {total} åªè‚¡ç¥¨...")
        else:
            iterator = enumerate(stock_list)
        
        for i, stock in iterator:
            df = self.fetch_moneyflow(stock, start_date, end_date)
            if df is not None and not df.empty:
                all_data.append(df)
                success_count += 1
            
            if show_progress and hasattr(iterator, 'set_postfix'):
                iterator.set_postfix({"æˆåŠŸ": success_count, "å½“å‰": stock})
            
            # æ‰¹æ¬¡ä¼‘æ¯
            if (i + 1) % batch_size == 0 and (i + 1) < total:
                if show_progress and hasattr(iterator, 'set_description'):
                    iterator.set_description(f"ğŸ’° ä¼‘æ¯{batch_sleep}s")
                time.sleep(batch_sleep)
                if show_progress and hasattr(iterator, 'set_description'):
                    iterator.set_description("ğŸ’° è·å–èµ„é‡‘æµå‘")
        
        if not all_data:
            return pd.DataFrame()
        
        result = pd.concat(all_data, ignore_index=True)
        logger.info(f"æ‰¹é‡è·å–èµ„é‡‘æµå‘å®Œæˆ: {success_count}/{total} åª, {len(result)} æ¡è®°å½•")
        return result
    
    def fetch_hk_hold(
        self,
        trade_date: str,
        stock_code: Optional[str] = None
    ) -> Optional[pd.DataFrame]:
        """
        è·å–æ²ªæ·±æ¸¯é€šæŒè‚¡æ•°æ®ï¼ˆåŒ—å‘èµ„é‡‘æŒä»“ï¼‰
        
        ä½¿ç”¨ Tushare Pro hk_hold æ¥å£è·å–åŒ—å‘èµ„é‡‘ï¼ˆæ²ªè‚¡é€š/æ·±è‚¡é€šï¼‰
        åœ¨ A è‚¡çš„æŒè‚¡æ˜ç»†æ•°æ®ï¼Œæ˜¯è¿½è¸ª"çœŸå¤–èµ„"çš„æ ¸å¿ƒæ•°æ®ã€‚
        
        Parameters
        ----------
        trade_date : str
            äº¤æ˜“æ—¥æœŸï¼Œæ ¼å¼ YYYYMMDD æˆ– YYYY-MM-DD
        stock_code : Optional[str]
            è‚¡ç¥¨ä»£ç ï¼ˆ6ä½ï¼‰ï¼Œå¦‚æœæä¾›åˆ™åªè¿”å›è¯¥è‚¡ç¥¨çš„æ•°æ®
        
        Returns
        -------
        Optional[pd.DataFrame]
            åŒ—å‘æŒä»“æ•°æ®ï¼ŒåŒ…å«ï¼š
            - trade_date: äº¤æ˜“æ—¥æœŸ
            - ts_code: è‚¡ç¥¨ä»£ç 
            - stock_code: 6ä½è‚¡ç¥¨ä»£ç 
            - name: è‚¡ç¥¨åç§°
            - vol: æŒè‚¡æ•°é‡ï¼ˆè‚¡ï¼‰
            - ratio: æŒè‚¡å æ¯”ï¼ˆ%ï¼‰
            - exchange: äº¤æ˜“æ‰€ï¼ˆSH/SZï¼‰
            å¤±è´¥è¿”å› None
        
        Notes
        -----
        - æ•°æ®ä¸º T+1 æŠ«éœ²ï¼Œå³å½“æ—¥æŒä»“éœ€æ¬¡æ—¥è·å–
        - è¯¥æ•°æ®æå…¶ç²¾å‡†ï¼Œèƒ½æœ‰æ•ˆè¿‡æ»¤ç›˜ä¸­çš„å‡å¤–èµ„
        - å»ºè®®å…³æ³¨æŒä»“å æ¯”çš„ç¯æ¯”å˜åŒ–ï¼ˆhk_hold_ratio_changeï¼‰
        
        Examples
        --------
        >>> loader = TushareDataLoader()
        >>> # è·å–å…¨å¸‚åœºåŒ—å‘æŒä»“
        >>> hk_data = loader.fetch_hk_hold("20240115")
        >>> # è®¡ç®—æŒä»“å æ¯”å˜åŒ–
        >>> today = loader.fetch_hk_hold("20240115")
        >>> yesterday = loader.fetch_hk_hold("20240114")
        >>> merged = today.merge(yesterday, on='stock_code', suffixes=('', '_prev'))
        >>> merged['ratio_change'] = merged['ratio'] - merged['ratio_prev']
        """
        # æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼
        trade_date = trade_date.replace("-", "")
        
        logger.debug(f"è·å–åŒ—å‘æŒä»“: {trade_date}")
        
        # å°è¯•ç¼“å­˜
        cache_file = self.cache_dir / f"hk_hold_{trade_date}.parquet"
        if cache_file.exists():
            try:
                df = pd.read_parquet(cache_file)
                if not df.empty:
                    logger.debug(f"ä»ç¼“å­˜åŠ è½½åŒ—å‘æŒä»“: {trade_date}, {len(df)} æ¡")
                    if stock_code:
                        df = df[df["stock_code"] == stock_code[:6]]
                    return df
            except Exception:
                pass
        
        # API è·å–ï¼ˆè·å–å…¨å¸‚åœºæ•°æ®ï¼‰
        df = self._fetch_with_retry(
            self.pro.hk_hold,
            trade_date=trade_date
        )
        
        if df is None or df.empty:
            logger.debug(f"è·å–åŒ—å‘æŒä»“å¤±è´¥: {trade_date}")
            return None
        
        # æ·»åŠ  6 ä½è‚¡ç¥¨ä»£ç 
        df["stock_code"] = df["ts_code"].str[:6]
        
        # ä¿å­˜ç¼“å­˜ï¼ˆå…¨é‡æ•°æ®ï¼‰
        try:
            df.to_parquet(cache_file, index=False)
        except Exception:
            pass
        
        logger.debug(f"è·å–åŒ—å‘æŒä»“æˆåŠŸ: {trade_date}, {len(df)} æ¡")
        
        # å¦‚æœæŒ‡å®šäº†è‚¡ç¥¨ä»£ç ï¼Œè¿‡æ»¤
        if stock_code:
            df = df[df["stock_code"] == stock_code[:6]]
        
        return df
    
    def fetch_hk_hold_change(
        self,
        stock_list: List[str],
        current_date: str,
        days_back: int = 5
    ) -> pd.DataFrame:
        """
        è®¡ç®—åŒ—å‘èµ„é‡‘æŒä»“å æ¯”å˜åŒ–
        
        è·å–æŒ‡å®šè‚¡ç¥¨åˆ—è¡¨çš„åŒ—å‘èµ„é‡‘æŒä»“å˜åŒ–æ•°æ®ï¼Œ
        ç”¨äºæ„å»º SmartMoney å› å­ä¸­çš„åŒ—å‘ç©¿é€å› å­ã€‚
        
        Parameters
        ----------
        stock_list : List[str]
            è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼ˆ6ä½ä»£ç ï¼‰
        current_date : str
            å½“å‰æ—¥æœŸï¼Œæ ¼å¼ YYYYMMDD æˆ– YYYY-MM-DD
        days_back : int
            å›æº¯å¤©æ•°ï¼Œç”¨äºè®¡ç®—ç¯æ¯”å˜åŒ–ï¼Œé»˜è®¤ 5 å¤©
        
        Returns
        -------
        pd.DataFrame
            åŒ…å«ä»¥ä¸‹åˆ—ï¼š
            - stock_code: è‚¡ç¥¨ä»£ç 
            - hk_ratio: å½“å‰åŒ—å‘æŒä»“å æ¯”
            - hk_ratio_prev: å‰æœŸåŒ—å‘æŒä»“å æ¯”
            - hk_ratio_change: æŒä»“å æ¯”å˜åŒ–ï¼ˆç™¾åˆ†ç‚¹ï¼‰
            - hk_hold_score: æ ‡å‡†åŒ–åçš„åŒ—å‘ç©¿é€å¾—åˆ† (0-1)
        
        Notes
        -----
        - å¦‚æœæŸè‚¡ç¥¨æ— åŒ—å‘æŒä»“æ•°æ®ï¼Œå…¶å¾—åˆ†é»˜è®¤ä¸º 0.5ï¼ˆä¸­æ€§ï¼‰
        - hk_hold_score ä½¿ç”¨æ’ååˆ†ä½æ•°æ ‡å‡†åŒ–ï¼Œé¿å…æç«¯å€¼å½±å“
        
        Examples
        --------
        >>> loader = TushareDataLoader()
        >>> stocks = ["000001", "000002", "600000"]
        >>> hk_change = loader.fetch_hk_hold_change(stocks, "20240115")
        >>> # ç­›é€‰åŒ—å‘åŠ ä»“æ˜æ˜¾çš„è‚¡ç¥¨
        >>> bullish = hk_change[hk_change['hk_ratio_change'] > 0.1]
        """
        current_date = current_date.replace("-", "")
        
        # è·å–äº¤æ˜“æ—¥å†
        start_date = (
            datetime.strptime(current_date, "%Y%m%d") - timedelta(days=days_back + 10)
        ).strftime("%Y%m%d")
        
        calendar = self.fetch_trade_calendar(start_date, current_date)
        
        if len(calendar) < 2:
            logger.warning("äº¤æ˜“æ—¥ä¸è¶³ï¼Œæ— æ³•è®¡ç®—åŒ—å‘æŒä»“å˜åŒ–")
            return pd.DataFrame()
        
        # è·å–æœ€è¿‘ä¸¤ä¸ªäº¤æ˜“æ—¥çš„æ•°æ®
        current_trade_date = calendar[-1].strftime("%Y%m%d")
        prev_trade_date = calendar[max(0, len(calendar) - days_back - 1)].strftime("%Y%m%d")
        
        logger.info(f"è®¡ç®—åŒ—å‘æŒä»“å˜åŒ–: {prev_trade_date} -> {current_trade_date}")
        
        # è·å–ä¸¤æ—¥çš„åŒ—å‘æŒä»“æ•°æ®
        current_hold = self.fetch_hk_hold(current_trade_date)
        prev_hold = self.fetch_hk_hold(prev_trade_date)
        
        # åˆå§‹åŒ–ç»“æœ DataFrame
        result = pd.DataFrame({"stock_code": stock_list})
        result["hk_ratio"] = np.nan
        result["hk_ratio_prev"] = np.nan
        result["hk_ratio_change"] = 0.0
        
        # åˆå¹¶å½“å‰æŒä»“æ•°æ®
        if current_hold is not None and not current_hold.empty:
            current_hold_subset = current_hold[["stock_code", "ratio"]].rename(
                columns={"ratio": "hk_ratio"}
            )
            result = result.merge(
                current_hold_subset, on="stock_code", how="left", suffixes=("_drop", "")
            )
            if "hk_ratio_drop" in result.columns:
                result["hk_ratio"] = result["hk_ratio"].fillna(result["hk_ratio_drop"])
                result = result.drop(columns=["hk_ratio_drop"])
        
        # åˆå¹¶å‰æœŸæŒä»“æ•°æ®
        if prev_hold is not None and not prev_hold.empty:
            prev_hold_subset = prev_hold[["stock_code", "ratio"]].rename(
                columns={"ratio": "hk_ratio_prev"}
            )
            result = result.merge(
                prev_hold_subset, on="stock_code", how="left", suffixes=("_drop", "")
            )
            if "hk_ratio_prev_drop" in result.columns:
                result["hk_ratio_prev"] = result["hk_ratio_prev"].fillna(
                    result["hk_ratio_prev_drop"]
                )
                result = result.drop(columns=["hk_ratio_prev_drop"])
        
        # è®¡ç®—å˜åŒ–
        result["hk_ratio_change"] = (
            result["hk_ratio"].fillna(0) - result["hk_ratio_prev"].fillna(0)
        )
        
        # è®¡ç®—æ ‡å‡†åŒ–å¾—åˆ†ï¼ˆä½¿ç”¨æ’ååˆ†ä½æ•°ï¼‰
        valid_mask = result["hk_ratio_change"].notna()
        if valid_mask.sum() > 0:
            result.loc[valid_mask, "hk_hold_score"] = (
                result.loc[valid_mask, "hk_ratio_change"].rank(pct=True)
            )
        else:
            result["hk_hold_score"] = 0.5
        
        # å¡«å……ç¼ºå¤±å€¼
        result["hk_hold_score"] = result["hk_hold_score"].fillna(0.5)
        
        logger.info(f"åŒ—å‘æŒä»“å˜åŒ–è®¡ç®—å®Œæˆ: {len(result)} åªè‚¡ç¥¨")
        return result
    
    def calculate_smart_money_score(
        self,
        stock_list: List[str],
        start_date: str,
        end_date: str,
        north_weight: float = 0.6,
        large_order_weight: float = 0.4
    ) -> pd.DataFrame:
        """
        è®¡ç®—å…¨æ¯ä¸»åŠ›èµ„é‡‘å› å­ (Holographic Smart Money Score)
        
        ç»¼åˆåŒ—å‘èµ„é‡‘ç©¿é€å› å­å’Œå†…èµ„å¤§å•æµå‘å› å­ï¼Œæ„å»ºå¤åˆä¸»åŠ›èµ„é‡‘å¾—åˆ†ã€‚
        åœ¨ç«çƒ­è¡Œæƒ…ä¸­ï¼Œè¯¥å› å­å¯æœ‰æ•ˆè·Ÿè¸ª"èªæ˜é’±"çš„æµå‘ã€‚
        
        Parameters
        ----------
        stock_list : List[str]
            è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼ˆ6ä½ä»£ç ï¼‰
        start_date : str
            å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼ YYYYMMDD æˆ– YYYY-MM-DD
        end_date : str
            ç»“æŸæ—¥æœŸï¼ˆç”¨äºè®¡ç®—åŒ—å‘æŒä»“å˜åŒ–çš„å‚è€ƒæ—¥ï¼‰
        north_weight : float
            åŒ—å‘èµ„é‡‘å› å­æƒé‡ï¼Œé»˜è®¤ 0.6
        large_order_weight : float
            å¤§å•æµå‘å› å­æƒé‡ï¼Œé»˜è®¤ 0.4
        
        Returns
        -------
        pd.DataFrame
            åŒ…å«ä»¥ä¸‹åˆ—ï¼š
            - stock_code: è‚¡ç¥¨ä»£ç 
            - north_score: åŒ—å‘èµ„é‡‘å¾—åˆ† (0-1)
            - large_order_score: å¤§å•æµå‘å¾—åˆ† (0-1)
            - smart_money_score: ç»¼åˆä¸»åŠ›èµ„é‡‘å¾—åˆ† (0-1)
            - main_net_inflow: ä¸»åŠ›å‡€æµå…¥é¢ï¼ˆä¸‡å…ƒï¼‰
            - hk_ratio_change: åŒ—å‘æŒä»“å æ¯”å˜åŒ–
        
        Notes
        -----
        SmartMoney_Score = north_weight * North_Score + large_order_weight * Large_Order_Score
        
        å› å­é€»è¾‘ï¼š
        1. åŒ—å‘ç©¿é€ï¼ˆçœŸå¤–èµ„ï¼‰ï¼šçœ‹åŒ—å‘èµ„é‡‘æŒä»“å æ¯”çš„ç¯æ¯”å˜åŒ–ï¼ŒT+1 æ•°æ®ä½†æå…¶ç²¾å‡†
        2. å†…èµ„ä¸»åŠ›ï¼ˆå¤§å•ï¼‰ï¼šå¤§å•/è¶…å¤§å•çš„å‡€æµå…¥ç‡
        
        Examples
        --------
        >>> loader = TushareDataLoader()
        >>> stocks = loader.fetch_index_constituents("hs300")
        >>> smart_money = loader.calculate_smart_money_score(
        ...     stocks, "20240101", "20240115"
        ... )
        >>> # ç­›é€‰ä¸»åŠ›èµ„é‡‘æµå…¥å¼ºåŠ¿çš„è‚¡ç¥¨
        >>> bullish = smart_money[smart_money['smart_money_score'] > 0.7]
        """
        start_date = start_date.replace("-", "")
        end_date = end_date.replace("-", "")
        
        logger.info(f"ğŸ“Š è®¡ç®—å…¨æ¯ä¸»åŠ›èµ„é‡‘å› å­: {len(stock_list)} åªè‚¡ç¥¨")
        
        # 1. è·å–åŒ—å‘èµ„é‡‘å˜åŒ–å¾—åˆ†
        hk_change = self.fetch_hk_hold_change(
            stock_list=stock_list,
            current_date=end_date,
            days_back=5
        )
        
        # 2. è·å–èµ„é‡‘æµå‘æ•°æ®
        moneyflow_df = self.fetch_moneyflow_batch(
            stock_list=stock_list,
            start_date=start_date,
            end_date=end_date,
            show_progress=True
        )
        
        # 3. è®¡ç®—å¤§å•æµå‘å¾—åˆ†
        if not moneyflow_df.empty:
            # è®¡ç®—ä¸»åŠ›å‡€æµå…¥ = (ç‰¹å¤§å•ä¹°å…¥ - ç‰¹å¤§å•å–å‡º) + (å¤§å•ä¹°å…¥ - å¤§å•å–å‡º)
            flow_cols = ["buy_elg_amount", "sell_elg_amount", "buy_lg_amount", "sell_lg_amount"]
            if all(col in moneyflow_df.columns for col in flow_cols):
                moneyflow_df["main_net_inflow"] = (
                    moneyflow_df["buy_elg_amount"] - moneyflow_df["sell_elg_amount"] +
                    moneyflow_df["buy_lg_amount"] - moneyflow_df["sell_lg_amount"]
                )
            elif "net_mf_amount" in moneyflow_df.columns:
                # ä½¿ç”¨å‡€æµå…¥é¢å­—æ®µ
                moneyflow_df["main_net_inflow"] = moneyflow_df["net_mf_amount"]
            else:
                moneyflow_df["main_net_inflow"] = 0
            
            # æŒ‰è‚¡ç¥¨æ±‡æ€»æœ€è¿‘çš„ä¸»åŠ›å‡€æµå…¥
            flow_summary = moneyflow_df.groupby("stock_code").agg({
                "main_net_inflow": "sum"
            }).reset_index()
            
            # æ ‡å‡†åŒ–å¤§å•æµå‘å¾—åˆ†ï¼ˆæ’ååˆ†ä½æ•°ï¼‰
            if len(flow_summary) > 0:
                flow_summary["large_order_score"] = flow_summary["main_net_inflow"].rank(pct=True)
            else:
                flow_summary["large_order_score"] = 0.5
        else:
            flow_summary = pd.DataFrame({
                "stock_code": stock_list,
                "main_net_inflow": 0,
                "large_order_score": 0.5
            })
        
        # 4. åˆå¹¶ç»“æœ
        result = pd.DataFrame({"stock_code": stock_list})
        
        # åˆå¹¶åŒ—å‘å¾—åˆ†
        if not hk_change.empty:
            result = result.merge(
                hk_change[["stock_code", "hk_hold_score", "hk_ratio_change"]],
                on="stock_code",
                how="left"
            )
            result["north_score"] = result["hk_hold_score"].fillna(0.5)
        else:
            result["north_score"] = 0.5
            result["hk_ratio_change"] = 0.0
        
        # åˆå¹¶å¤§å•å¾—åˆ†
        result = result.merge(
            flow_summary[["stock_code", "large_order_score", "main_net_inflow"]],
            on="stock_code",
            how="left"
        )
        result["large_order_score"] = result["large_order_score"].fillna(0.5)
        result["main_net_inflow"] = result["main_net_inflow"].fillna(0)
        
        # 5. è®¡ç®—ç»¼åˆå¾—åˆ†
        result["smart_money_score"] = (
            north_weight * result["north_score"] +
            large_order_weight * result["large_order_score"]
        )
        
        # æ¸…ç†ä¸­é—´åˆ—
        if "hk_hold_score" in result.columns:
            result = result.drop(columns=["hk_hold_score"])
        
        logger.info(
            f"âœ… ä¸»åŠ›èµ„é‡‘å› å­è®¡ç®—å®Œæˆ: "
            f"å‡å€¼={result['smart_money_score'].mean():.3f}, "
            f"top10å‡å€¼={result.nlargest(10, 'smart_money_score')['smart_money_score'].mean():.3f}"
        )
        
        return result
    
    # ==================== æ¶¨åœæ¿ä¸é¾™å¤´å› å­ ====================
    
    def fetch_limit_list(
        self,
        trade_date: str,
        limit_type: str = "U"
    ) -> Optional[pd.DataFrame]:
        """
        è·å–æ¯æ—¥æ¶¨è·Œåœè‚¡ç¥¨åˆ—è¡¨
        
        ä½¿ç”¨ Tushare Pro limit_list æ¥å£è·å–æ¯æ—¥æ¶¨åœ/è·Œåœè‚¡ç¥¨æ˜ç»†ï¼Œ
        åŒ…å«å°å•é‡‘é¢ã€å°å•æ¯”ä¾‹ã€æŒ¯å¹…ã€ç‚¸æ¿æ¬¡æ•°ç­‰å¾®è§‚ç»“æ„æ•°æ®ã€‚
        
        Parameters
        ----------
        trade_date : str
            äº¤æ˜“æ—¥æœŸï¼Œæ ¼å¼ YYYYMMDD æˆ– YYYY-MM-DD
        limit_type : str
            æ¶¨è·Œåœç±»å‹ï¼š
            - "U": æ¶¨åœï¼ˆé»˜è®¤ï¼‰
            - "D": è·Œåœ
        
        Returns
        -------
        Optional[pd.DataFrame]
            æ¶¨è·Œåœæ˜ç»†æ•°æ®ï¼ŒåŒ…å«ï¼š
            - trade_date: äº¤æ˜“æ—¥æœŸ
            - ts_code: è‚¡ç¥¨ä»£ç 
            - stock_code: 6ä½è‚¡ç¥¨ä»£ç 
            - name: è‚¡ç¥¨åç§°
            - close: æ”¶ç›˜ä»·
            - pct_chg: æ¶¨è·Œå¹…
            - amp: æŒ¯å¹…(%)
            - fc_ratio: å°æˆæ¯”(%)ï¼ˆå°å•é‡‘é¢/æˆäº¤é¢ï¼‰
            - fl_ratio: å°æµæ¯”(%)ï¼ˆå°å•æ‰‹æ•°/æµé€šè‚¡æœ¬ï¼‰
            - fd_amount: å°å•é‡‘é¢ï¼ˆä¸‡å…ƒï¼‰
            - first_time: é¦–æ¬¡æ¶¨åœæ—¶é—´
            - last_time: æœ€åæ¶¨åœæ—¶é—´
            - open_times: æ‰“å¼€æ¬¡æ•°
            - strth: æ¶¨åœå¼ºåº¦ï¼ˆ0-100ï¼‰
            - limit: æ¶¨è·Œåœä»·æ ¼
            å¤±è´¥è¿”å› None
        
        Notes
        -----
        - Tushare Pro limit_list æ¥å£éœ€è¦è¾ƒé«˜ç§¯åˆ†æƒé™ï¼ˆçº¦ 2000 ç§¯åˆ†ï¼‰
        - æ¶¨åœå¼ºåº¦(strth)ï¼šç»¼åˆè€ƒè™‘å°å•ã€æŒ¯å¹…ã€ç‚¸æ¿æ¬¡æ•°çš„å¼ºåº¦è¯„åˆ†
        - fc_ratio å°æˆæ¯”è¶Šå¤§ï¼Œè¯´æ˜ä¹°ç›˜è¶Šåšå†³ï¼Œæ¬¡æ—¥é«˜å¼€æ¦‚ç‡å¤§
        
        Examples
        --------
        >>> loader = TushareDataLoader()
        >>> limits = loader.fetch_limit_list("20240115")
        >>> # ç­›é€‰å¼ºåŠ¿æ¶¨åœï¼ˆæœªç‚¸æ¿ã€å°æˆæ¯”>10%ï¼‰
        >>> strong = limits[(limits['open_times'] == 0) & (limits['fc_ratio'] > 10)]
        """
        # æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼
        trade_date = trade_date.replace("-", "")
        
        logger.debug(f"è·å–æ¶¨è·Œåœåˆ—è¡¨: {trade_date}, ç±»å‹={limit_type}")
        
        # å°è¯•ç¼“å­˜
        cache_file = self.cache_dir / f"limit_list_{trade_date}_{limit_type}.parquet"
        if cache_file.exists():
            try:
                df = pd.read_parquet(cache_file)
                if not df.empty:
                    logger.debug(f"ä»ç¼“å­˜åŠ è½½æ¶¨è·Œåœåˆ—è¡¨: {trade_date}, {len(df)} æ¡")
                    return df
            except Exception:
                pass
        
        # API è·å–
        df = self._fetch_with_retry(
            self.pro.limit_list,
            trade_date=trade_date,
            limit_type=limit_type
        )
        
        if df is None or df.empty:
            logger.debug(f"è·å–æ¶¨è·Œåœåˆ—è¡¨å¤±è´¥: {trade_date}")
            return None
        
        # æ·»åŠ  6 ä½è‚¡ç¥¨ä»£ç 
        df["stock_code"] = df["ts_code"].str[:6]
        
        # é‡å‘½åéƒ¨åˆ†å­—æ®µä»¥ç»Ÿä¸€
        rename_map = {
            "open_times": "open_num",  # ç‚¸æ¿æ¬¡æ•°
        }
        df = df.rename(columns=rename_map)
        
        # ç¡®ä¿å…³é”®å­—æ®µå­˜åœ¨
        if "open_num" not in df.columns:
            df["open_num"] = 0
        if "fc_ratio" not in df.columns and "fd_amount" in df.columns and "amount" in df.columns:
            # è®¡ç®—å°æˆæ¯”
            df["fc_ratio"] = df["fd_amount"] / df["amount"].replace(0, np.nan) * 100
        
        # ä¿å­˜ç¼“å­˜
        try:
            df.to_parquet(cache_file, index=False)
        except Exception:
            pass
        
        logger.debug(f"è·å–æ¶¨è·Œåœåˆ—è¡¨æˆåŠŸ: {trade_date}, {len(df)} æ¡")
        return df
    
    def fetch_limit_list_batch(
        self,
        start_date: str,
        end_date: str,
        limit_type: str = "U",
        show_progress: bool = True
    ) -> pd.DataFrame:
        """
        æ‰¹é‡è·å–å¤šæ—¥æ¶¨è·Œåœåˆ—è¡¨
        
        Parameters
        ----------
        start_date : str
            å¼€å§‹æ—¥æœŸ
        end_date : str
            ç»“æŸæ—¥æœŸ
        limit_type : str
            æ¶¨è·Œåœç±»å‹ï¼š"U"(æ¶¨åœ) æˆ– "D"(è·Œåœ)
        show_progress : bool
            æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡
        
        Returns
        -------
        pd.DataFrame
            åˆå¹¶åçš„æ¶¨è·Œåœæ•°æ®
        """
        start_date = start_date.replace("-", "")
        end_date = end_date.replace("-", "")
        
        # è·å–äº¤æ˜“æ—¥å†
        calendar = self.fetch_trade_calendar(start_date, end_date)
        
        all_data = []
        total = len(calendar)
        
        if show_progress:
            try:
                from tqdm import tqdm
                iterator = tqdm(
                    calendar,
                    desc="ğŸ”¥ è·å–æ¶¨åœæ•°æ®",
                    unit="å¤©",
                    ncols=80
                )
            except ImportError:
                iterator = calendar
                logger.info(f"å¼€å§‹è·å–æ¶¨åœæ•°æ®: {total} ä¸ªäº¤æ˜“æ—¥...")
        else:
            iterator = calendar
        
        for date in iterator:
            date_str = date.strftime("%Y%m%d")
            df = self.fetch_limit_list(date_str, limit_type)
            if df is not None and not df.empty:
                all_data.append(df)
        
        if not all_data:
            return pd.DataFrame()
        
        result = pd.concat(all_data, ignore_index=True)
        logger.info(f"æ‰¹é‡è·å–æ¶¨åœæ•°æ®å®Œæˆ: {len(calendar)} å¤©, {len(result)} æ¡è®°å½•")
        return result
    
    def calculate_consecutive_limits(
        self,
        stock_code: str,
        end_date: str,
        days_back: int = 30
    ) -> int:
        """
        è®¡ç®—è‚¡ç¥¨çš„è¿æ¿å¤©æ•°
        
        Parameters
        ----------
        stock_code : str
            è‚¡ç¥¨ä»£ç ï¼ˆ6ä½ï¼‰
        end_date : str
            æˆªæ­¢æ—¥æœŸ
        days_back : int
            å›æº¯å¤©æ•°ï¼Œé»˜è®¤ 30 å¤©
        
        Returns
        -------
        int
            è¿ç»­æ¶¨åœå¤©æ•°ï¼ˆ0 è¡¨ç¤ºéæ¶¨åœæˆ–å·²ä¸­æ–­ï¼‰
        
        Notes
        -----
        åªç»Ÿè®¡åˆ° end_date ä¸ºæ­¢çš„è¿ç»­æ¶¨åœå¤©æ•°ï¼Œ
        å¦‚æœ end_date å½“å¤©æœªæ¶¨åœï¼Œè¿”å› 0ã€‚
        """
        end_date = end_date.replace("-", "")
        start_date = (
            datetime.strptime(end_date, "%Y%m%d") - timedelta(days=days_back)
        ).strftime("%Y%m%d")
        
        # è·å–äº¤æ˜“æ—¥å†
        calendar = self.fetch_trade_calendar(start_date, end_date)
        if len(calendar) == 0:
            return 0
        
        # ä»æœ€è¿‘çš„äº¤æ˜“æ—¥å‘å‰å›æº¯
        consecutive_count = 0
        
        for date in reversed(calendar):
            date_str = date.strftime("%Y%m%d")
            limit_df = self.fetch_limit_list(date_str, limit_type="U")
            
            if limit_df is None or limit_df.empty:
                if consecutive_count == 0:
                    continue  # å¯èƒ½æ˜¯éäº¤æ˜“æ—¥æ•°æ®ç¼ºå¤±
                else:
                    break  # è¿æ¿ä¸­æ–­
            
            # æ£€æŸ¥è¯¥è‚¡ç¥¨æ˜¯å¦åœ¨æ¶¨åœåˆ—è¡¨ä¸­
            if stock_code in limit_df["stock_code"].values:
                consecutive_count += 1
            else:
                if consecutive_count > 0:
                    break  # è¿æ¿ä¸­æ–­
                # å¦‚æœä»æœªæ¶¨åœï¼Œç»§ç»­å‘å‰æ£€æŸ¥ï¼ˆå¯èƒ½end_dateå½“å¤©æœªæ¶¨åœï¼‰
                # ä½†å¦‚æœå·²ç»å›æº¯è¶…è¿‡5å¤©è¿˜æ²¡æ¶¨åœï¼Œç›´æ¥è¿”å›0
                if consecutive_count == 0:
                    date_diff = (datetime.strptime(end_date, "%Y%m%d") - date).days
                    if date_diff > 5:
                        return 0
        
        return consecutive_count
    
    def calculate_limit_strength(
        self,
        trade_date: str,
        min_fl_ratio: float = 1.0
    ) -> pd.DataFrame:
        """
        è®¡ç®—æ¶¨åœå°æ¿å¼ºåº¦å› å­ï¼ˆé¾™å¤´ä¿¡ä»°å› å­ï¼‰
        
        é’ˆå¯¹"è¿æ¿"å’Œ"æ¶¨åœ"è‚¡ç¥¨çš„å¾®è§‚ç»“æ„åˆ†æï¼Œ
        ç”¨äºæ•æ‰å¼ºåŠ¿è‚¡çš„æº¢ä»·é¢„æœŸã€‚
        
        Parameters
        ----------
        trade_date : str
            äº¤æ˜“æ—¥æœŸï¼Œæ ¼å¼ YYYYMMDD æˆ– YYYY-MM-DD
        min_fl_ratio : float
            æœ€ä½å°æµæ¯”é˜ˆå€¼ï¼ˆ%ï¼‰ï¼Œé»˜è®¤ 1.0
        
        Returns
        -------
        pd.DataFrame
            æ¶¨åœå¼ºåº¦å› å­æ•°æ®ï¼ŒåŒ…å«ï¼š
            - stock_code: è‚¡ç¥¨ä»£ç 
            - name: è‚¡ç¥¨åç§°
            - close: æ”¶ç›˜ä»·
            - pct_chg: æ¶¨è·Œå¹…
            - fd_amount: å°å•é‡‘é¢ï¼ˆä¸‡å…ƒï¼‰
            - bid_strength: å°æˆæ¯”ï¼ˆå°å•é‡‘é¢/æˆäº¤é¢ï¼‰
            - fl_ratio: å°æµæ¯”ï¼ˆå°å•/æµé€šè‚¡æœ¬ï¼‰
            - open_num: ç‚¸æ¿æ¬¡æ•°
            - is_strong_limit: æ˜¯å¦å¼ºåŠ¿æ¶¨åœ
            - dragon_score: é¾™å¤´ä¿¡ä»°å¾—åˆ† (0-1)
        
        Notes
        -----
        è®¡ç®—é€»è¾‘ï¼š
        1. ç­›é€‰æ¶¨åœï¼šåªåˆ†ææ”¶ç›˜æ¶¨åœçš„è‚¡ç¥¨
        2. å°æ¿å¼ºåº¦ï¼šå°å•é‡‘é¢ / æ—¥æˆäº¤é¢ï¼Œæ¯”å€¼è¶Šå¤§ä¹°ç›˜è¶Šåšå†³
        3. è´¨é‡è¿‡æ»¤ï¼šå°æµæ¯” > 1% ä¸”ç‚¸æ¿æ¬¡æ•° == 0 ä¸ºå¼ºåŠ¿æ¿
        4. å¼±åŠ¿é™æƒï¼šéå¼ºåŠ¿æ¿å¾—åˆ†å‡åŠ
        
        Examples
        --------
        >>> loader = TushareDataLoader()
        >>> dragon = loader.calculate_limit_strength("20240115")
        >>> # ç­›é€‰é«˜é¾™å¤´å¾—åˆ†çš„è‚¡ç¥¨
        >>> top_dragons = dragon[dragon['dragon_score'] > 0.8]
        """
        trade_date = trade_date.replace("-", "")
        
        logger.info(f"ğŸ‰ è®¡ç®—é¾™å¤´ä¿¡ä»°å› å­: {trade_date}")
        
        # è·å–æ¶¨åœåˆ—è¡¨
        limit_df = self.fetch_limit_list(trade_date, limit_type="U")
        
        if limit_df is None or limit_df.empty:
            logger.warning(f"æ— æ¶¨åœæ•°æ®: {trade_date}")
            return pd.DataFrame()
        
        result = limit_df.copy()
        
        # 1. è®¡ç®—å°æˆæ¯” (å°å•é‡‘é¢ / å½“æ—¥æˆäº¤é¢)
        # åæ˜ å¸‚åœºæƒœå”®ç¨‹åº¦å’ŒæŠ¢ç­¹æ„æ„¿
        if "fd_amount" in result.columns and "amount" in result.columns:
            result["bid_strength"] = (
                result["fd_amount"] / result["amount"].replace(0, np.nan)
            )
        elif "fc_ratio" in result.columns:
            # å¦‚æœå·²æœ‰å°æˆæ¯”å­—æ®µï¼Œè½¬æ¢ä¸ºå°æ•°
            result["bid_strength"] = result["fc_ratio"] / 100
        else:
            result["bid_strength"] = 0.5  # é»˜è®¤å€¼
        
        # 2. è¯†åˆ«å¼ºåŠ¿æ¶¨åœ
        # æ¡ä»¶ï¼šå°æµæ¯” > min_fl_ratio% ä¸” ç‚¸æ¿æ¬¡æ•° == 0
        if "fl_ratio" not in result.columns:
            result["fl_ratio"] = 0
        if "open_num" not in result.columns:
            result["open_num"] = 0
        
        result["is_strong_limit"] = (
            (result["fl_ratio"] >= min_fl_ratio) & 
            (result["open_num"] == 0)
        )
        
        # 3. è®¡ç®—é¾™å¤´å¾—åˆ†
        # åŸºç¡€å¾—åˆ† = å°æˆæ¯”çš„æ ‡å‡†åŒ–
        result["dragon_score"] = result["bid_strength"].rank(pct=True)
        
        # å¯¹å¼±åŠ¿æ¿ï¼ˆç‚¸æ¿æˆ–å°å•å¼±ï¼‰é™æƒ 50%
        result.loc[~result["is_strong_limit"], "dragon_score"] *= 0.5
        
        # ç¡®ä¿å¾—åˆ†åœ¨ [0, 1] èŒƒå›´å†…
        result["dragon_score"] = result["dragon_score"].clip(0, 1)
        
        # å¡«å……ç¼ºå¤±å€¼
        result["dragon_score"] = result["dragon_score"].fillna(0)
        
        # é€‰æ‹©è¾“å‡ºåˆ—
        output_cols = [
            "stock_code", "ts_code", "name", "close", "pct_chg",
            "fd_amount", "amount", "bid_strength", "fl_ratio", 
            "open_num", "is_strong_limit", "dragon_score"
        ]
        output_cols = [c for c in output_cols if c in result.columns]
        
        result = result[output_cols].copy()
        
        # æŒ‰é¾™å¤´å¾—åˆ†æ’åº
        result = result.sort_values("dragon_score", ascending=False)
        
        strong_count = result["is_strong_limit"].sum()
        logger.info(
            f"âœ… é¾™å¤´å› å­è®¡ç®—å®Œæˆ: {len(result)} åªæ¶¨åœ, "
            f"{strong_count} åªå¼ºåŠ¿æ¿, "
            f"top5å¾—åˆ†={result['dragon_score'].head(5).mean():.3f}"
        )
        
        return result
    
    def calculate_dragon_head_factor(
        self,
        trade_date: str,
        days_back: int = 5,
        consecutive_weight: float = 0.3,
        strength_weight: float = 0.7
    ) -> pd.DataFrame:
        """
        è®¡ç®—å®Œæ•´é¾™å¤´ä¿¡ä»°å› å­ï¼ˆå«è¿æ¿æº¢ä»·ï¼‰
        
        ç»¼åˆå°æ¿å¼ºåº¦å’Œè¿æ¿å¤©æ•°ï¼Œæ„å»ºå®Œæ•´çš„é¾™å¤´ä¿¡ä»°å› å­ã€‚
        
        Parameters
        ----------
        trade_date : str
            äº¤æ˜“æ—¥æœŸ
        days_back : int
            è®¡ç®—è¿æ¿å¤©æ•°æ—¶çš„å›æº¯å¤©æ•°ï¼Œé»˜è®¤ 5 å¤©
        consecutive_weight : float
            è¿æ¿æº¢ä»·æƒé‡ï¼Œé»˜è®¤ 0.3
        strength_weight : float
            å°æ¿å¼ºåº¦æƒé‡ï¼Œé»˜è®¤ 0.7
        
        Returns
        -------
        pd.DataFrame
            é¾™å¤´ä¿¡ä»°å› å­æ•°æ®ï¼ŒåŒ…å«ï¼š
            - stock_code: è‚¡ç¥¨ä»£ç 
            - dragon_score: å°æ¿å¼ºåº¦å¾—åˆ†
            - consecutive_days: è¿æ¿å¤©æ•°
            - consecutive_score: è¿æ¿æº¢ä»·å¾—åˆ†
            - dragon_head_factor: ç»¼åˆé¾™å¤´å› å­
        
        Notes
        -----
        ç»¼åˆå› å­è®¡ç®—ï¼š
        Dragon_Head_Factor = strength_weight * dragon_score + 
                             consecutive_weight * consecutive_score
        
        è¿æ¿æº¢ä»·é€»è¾‘ï¼š
        - 1è¿æ¿ï¼ˆé¦–æ¿ï¼‰: 1.0
        - 2è¿æ¿: 1.5
        - 3è¿æ¿: 2.0
        - 4è¿æ¿åŠä»¥ä¸Š: 2.5 + 0.2 * (n - 4)
        
        Examples
        --------
        >>> loader = TushareDataLoader()
        >>> dragon = loader.calculate_dragon_head_factor("20240115")
        >>> # è·å–é«˜ä½é¾™å¤´ï¼ˆè¿æ¿+å¼ºå°å•ï¼‰
        >>> high_dragons = dragon[
        ...     (dragon['consecutive_days'] >= 2) & 
        ...     (dragon['dragon_head_factor'] > 0.8)
        ... ]
        """
        trade_date = trade_date.replace("-", "")
        
        logger.info(f"ğŸ² è®¡ç®—å®Œæ•´é¾™å¤´ä¿¡ä»°å› å­: {trade_date}")
        
        # 1. è·å–å°æ¿å¼ºåº¦å› å­
        strength_df = self.calculate_limit_strength(trade_date)
        
        if strength_df.empty:
            return pd.DataFrame()
        
        result = strength_df.copy()
        
        # 2. è®¡ç®—è¿æ¿å¤©æ•°
        logger.info(f"è®¡ç®—è¿æ¿å¤©æ•°: {len(result)} åªè‚¡ç¥¨...")
        
        consecutive_days_list = []
        for stock in result["stock_code"]:
            cons_days = self.calculate_consecutive_limits(
                stock, trade_date, days_back=days_back
            )
            consecutive_days_list.append(cons_days)
        
        result["consecutive_days"] = consecutive_days_list
        
        # 3. è®¡ç®—è¿æ¿æº¢ä»·å¾—åˆ†
        # è¿æ¿è¶Šå¤šï¼Œæº¢ä»·é¢„æœŸè¶Šé«˜
        def calc_consecutive_score(days: int) -> float:
            """è¿æ¿æº¢ä»·å‡½æ•°"""
            if days <= 0:
                return 0.0
            elif days == 1:
                return 1.0
            elif days == 2:
                return 1.5
            elif days == 3:
                return 2.0
            else:
                return 2.5 + 0.2 * (days - 4)
        
        result["consecutive_premium"] = result["consecutive_days"].apply(calc_consecutive_score)
        
        # æ ‡å‡†åŒ–è¿æ¿å¾—åˆ†åˆ° [0, 1]
        max_premium = result["consecutive_premium"].max()
        if max_premium > 0:
            result["consecutive_score"] = result["consecutive_premium"] / max_premium
        else:
            result["consecutive_score"] = 0
        
        # 4. è®¡ç®—ç»¼åˆé¾™å¤´å› å­
        result["dragon_head_factor"] = (
            strength_weight * result["dragon_score"] +
            consecutive_weight * result["consecutive_score"]
        )
        
        # ç¡®ä¿å› å­åœ¨ [0, 1] èŒƒå›´
        result["dragon_head_factor"] = result["dragon_head_factor"].clip(0, 1)
        
        # æŒ‰ç»¼åˆå› å­æ’åº
        result = result.sort_values("dragon_head_factor", ascending=False)
        
        # ç»Ÿè®¡ä¿¡æ¯
        multi_limit = (result["consecutive_days"] >= 2).sum()
        logger.info(
            f"âœ… é¾™å¤´å› å­è®¡ç®—å®Œæˆ: "
            f"{len(result)} åªæ¶¨åœ, {multi_limit} åªè¿æ¿, "
            f"æœ€é«˜è¿æ¿={result['consecutive_days'].max()}å¤©"
        )
        
        return result
    
    def get_dragon_candidates(
        self,
        trade_date: str,
        min_consecutive: int = 1,
        min_factor: float = 0.6,
        top_n: int = 20
    ) -> pd.DataFrame:
        """
        è·å–é¾™å¤´å€™é€‰è‚¡
        
        ç­›é€‰æ»¡è¶³æ¡ä»¶çš„é«˜è´¨é‡é¾™å¤´è‚¡ï¼Œç”¨äºçŸ­çº¿äº¤æ˜“å‚è€ƒã€‚
        
        Parameters
        ----------
        trade_date : str
            äº¤æ˜“æ—¥æœŸ
        min_consecutive : int
            æœ€ä½è¿æ¿å¤©æ•°ï¼Œé»˜è®¤ 1ï¼ˆé¦–æ¿ï¼‰
        min_factor : float
            æœ€ä½é¾™å¤´å› å­å¾—åˆ†ï¼Œé»˜è®¤ 0.6
        top_n : int
            è¿”å›è‚¡ç¥¨æ•°é‡ä¸Šé™ï¼Œé»˜è®¤ 20
        
        Returns
        -------
        pd.DataFrame
            ç­›é€‰åçš„é¾™å¤´å€™é€‰è‚¡åˆ—è¡¨
        
        Examples
        --------
        >>> loader = TushareDataLoader()
        >>> candidates = loader.get_dragon_candidates(
        ...     "20240115",
        ...     min_consecutive=2,  # è‡³å°‘2è¿æ¿
        ...     min_factor=0.7
        ... )
        >>> print(candidates[['stock_code', 'name', 'consecutive_days', 'dragon_head_factor']])
        """
        dragon_df = self.calculate_dragon_head_factor(trade_date)
        
        if dragon_df.empty:
            return pd.DataFrame()
        
        # ç­›é€‰æ¡ä»¶
        mask = (
            (dragon_df["consecutive_days"] >= min_consecutive) &
            (dragon_df["dragon_head_factor"] >= min_factor) &
            (dragon_df["is_strong_limit"] == True)
        )
        
        candidates = dragon_df[mask].head(top_n).copy()
        
        logger.info(
            f"ğŸ¯ é¾™å¤´å€™é€‰è‚¡ç­›é€‰å®Œæˆ: "
            f"{len(candidates)} åª (æ¡ä»¶: è¿æ¿>={min_consecutive}, å› å­>={min_factor})"
        )
        
        return candidates
    
    # ==================== èèµ„èåˆ¸ä¸æ æ†å› å­ ====================
    
    def fetch_margin_detail(
        self,
        stock_code: str,
        start_date: str,
        end_date: str
    ) -> Optional[pd.DataFrame]:
        """
        è·å–ä¸ªè‚¡èèµ„èåˆ¸æ˜ç»†æ•°æ®
        
        ä½¿ç”¨ Tushare Pro margin_detail æ¥å£è·å–ä¸ªè‚¡èèµ„èåˆ¸äº¤æ˜“æ˜ç»†ï¼Œ
        åŒ…å«èèµ„ä¹°å…¥ã€èèµ„å¿è¿˜ã€èåˆ¸å–å‡ºç­‰æ•°æ®ã€‚
        
        Parameters
        ----------
        stock_code : str
            è‚¡ç¥¨ä»£ç ï¼ˆ6ä½ï¼‰ï¼Œå¦‚ "000001"
        start_date : str
            å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼ YYYYMMDD æˆ– YYYY-MM-DD
        end_date : str
            ç»“æŸæ—¥æœŸï¼Œæ ¼å¼ YYYYMMDD æˆ– YYYY-MM-DD
        
        Returns
        -------
        Optional[pd.DataFrame]
            èèµ„èåˆ¸æ˜ç»†æ•°æ®ï¼ŒåŒ…å«ï¼š
            - trade_date: äº¤æ˜“æ—¥æœŸ
            - ts_code: è‚¡ç¥¨ä»£ç 
            - stock_code: 6ä½è‚¡ç¥¨ä»£ç 
            - rzye: èèµ„ä½™é¢ï¼ˆå…ƒï¼‰
            - rzmre: èèµ„ä¹°å…¥é¢ï¼ˆå…ƒï¼‰
            - rzche: èèµ„å¿è¿˜é¢ï¼ˆå…ƒï¼‰
            - rqye: èåˆ¸ä½™é¢ï¼ˆå…ƒï¼‰
            - rqyl: èåˆ¸ä½™é‡ï¼ˆè‚¡ï¼‰
            - rqmcl: èåˆ¸å–å‡ºé‡ï¼ˆè‚¡ï¼‰
            - rqchl: èåˆ¸å¿è¿˜é‡ï¼ˆè‚¡ï¼‰
            å¤±è´¥è¿”å› None
        
        Notes
        -----
        - Tushare Pro margin_detail æ¥å£éœ€è¦è¾ƒé«˜ç§¯åˆ†æƒé™
        - èèµ„æ•°æ®æ˜¯åˆ¤æ–­æ•£æˆ·æ æ†æƒ…ç»ªçš„é‡è¦æŒ‡æ ‡
        - å½“èèµ„ä¹°å…¥å æ¯”è¿‡é«˜æ—¶ï¼Œå¾€å¾€æ˜¯é˜¶æ®µæ€§é¡¶éƒ¨ä¿¡å·
        
        Examples
        --------
        >>> loader = TushareDataLoader()
        >>> margin = loader.fetch_margin_detail("000001", "20240101", "20240115")
        >>> # è®¡ç®—èèµ„ä¹°å…¥å æ¯”è¶‹åŠ¿
        >>> print(margin[['trade_date', 'rzye', 'rzmre']].tail())
        """
        # æ ‡å‡†åŒ–è‚¡ç¥¨ä»£ç 
        ts_code = self._to_ts_code(stock_code)
        
        # æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼
        start_date = start_date.replace("-", "")
        end_date = end_date.replace("-", "")
        
        logger.debug(f"è·å–èèµ„èåˆ¸æ•°æ®: {stock_code}, {start_date} ~ {end_date}")
        
        # å°è¯•ç¼“å­˜
        cache_file = self.cache_dir / f"margin_{stock_code}_{start_date}_{end_date}.parquet"
        if cache_file.exists():
            try:
                df = pd.read_parquet(cache_file)
                if not df.empty:
                    logger.debug(f"ä»ç¼“å­˜åŠ è½½èèµ„èåˆ¸æ•°æ®: {stock_code}")
                    return df
            except Exception:
                pass
        
        # API è·å–
        df = self._fetch_with_retry(
            self.pro.margin_detail,
            ts_code=ts_code,
            start_date=start_date,
            end_date=end_date
        )
        
        if df is None or df.empty:
            logger.debug(f"è·å–èèµ„èåˆ¸æ•°æ®å¤±è´¥: {stock_code}")
            return None
        
        # æ·»åŠ  6 ä½è‚¡ç¥¨ä»£ç 
        df["stock_code"] = df["ts_code"].str[:6]
        
        # æ—¥æœŸæ ‡å‡†åŒ–
        if "trade_date" in df.columns:
            df["trade_date"] = pd.to_datetime(df["trade_date"])
            df = df.sort_values("trade_date")
        
        # ä¿å­˜ç¼“å­˜
        try:
            df.to_parquet(cache_file, index=False)
        except Exception:
            pass
        
        logger.debug(f"è·å–èèµ„èåˆ¸æ•°æ®æˆåŠŸ: {stock_code}, {len(df)} æ¡")
        return df
    
    def fetch_margin(
        self,
        trade_date: str
    ) -> Optional[pd.DataFrame]:
        """
        è·å–å…¨å¸‚åœºèèµ„èåˆ¸æ±‡æ€»æ•°æ®
        
        ä½¿ç”¨ Tushare Pro margin æ¥å£è·å–å…¨å¸‚åœºèèµ„èåˆ¸æ±‡æ€»ï¼Œ
        æ˜¯å¿«é€Ÿè·å–å¸‚åœºæ æ†æƒ…ç»ªçš„é«˜æ•ˆæ–¹å¼ã€‚
        
        Parameters
        ----------
        trade_date : str
            äº¤æ˜“æ—¥æœŸï¼Œæ ¼å¼ YYYYMMDD æˆ– YYYY-MM-DD
        
        Returns
        -------
        Optional[pd.DataFrame]
            å…¨å¸‚åœºèèµ„èåˆ¸æ±‡æ€»æ•°æ®ï¼ŒåŒ…å«ï¼š
            - trade_date: äº¤æ˜“æ—¥æœŸ
            - ts_code: è‚¡ç¥¨ä»£ç 
            - stock_code: 6ä½è‚¡ç¥¨ä»£ç 
            - rzye: èèµ„ä½™é¢ï¼ˆå…ƒï¼‰
            - rzmre: èèµ„ä¹°å…¥é¢ï¼ˆå…ƒï¼‰
            - rzche: èèµ„å¿è¿˜é¢ï¼ˆå…ƒï¼‰
            - rqye: èåˆ¸ä½™é¢ï¼ˆå…ƒï¼‰
            - rqmcl: èåˆ¸å–å‡ºé‡ï¼ˆè‚¡ï¼‰
            - rzrqye: èèµ„èåˆ¸ä½™é¢ï¼ˆå…ƒï¼‰
            å¤±è´¥è¿”å› None
        
        Examples
        --------
        >>> loader = TushareDataLoader()
        >>> margin = loader.fetch_margin("20240115")
        >>> # ç­›é€‰èèµ„ä½™é¢æœ€é«˜çš„è‚¡ç¥¨
        >>> top_margin = margin.nlargest(20, 'rzye')
        """
        trade_date = trade_date.replace("-", "")
        
        logger.debug(f"è·å–å…¨å¸‚åœºèèµ„èåˆ¸æ•°æ®: {trade_date}")
        
        # å°è¯•ç¼“å­˜
        cache_file = self.cache_dir / f"margin_{trade_date}.parquet"
        if cache_file.exists():
            try:
                df = pd.read_parquet(cache_file)
                if not df.empty:
                    logger.debug(f"ä»ç¼“å­˜åŠ è½½èèµ„èåˆ¸æ•°æ®: {trade_date}, {len(df)} æ¡")
                    return df
            except Exception:
                pass
        
        # API è·å–
        df = self._fetch_with_retry(
            self.pro.margin,
            trade_date=trade_date
        )
        
        if df is None or df.empty:
            logger.debug(f"è·å–èèµ„èåˆ¸æ•°æ®å¤±è´¥: {trade_date}")
            return None
        
        # æ·»åŠ  6 ä½è‚¡ç¥¨ä»£ç 
        df["stock_code"] = df["ts_code"].str[:6]
        
        # ä¿å­˜ç¼“å­˜
        try:
            df.to_parquet(cache_file, index=False)
        except Exception:
            pass
        
        logger.debug(f"è·å–èèµ„èåˆ¸æ•°æ®æˆåŠŸ: {trade_date}, {len(df)} æ¡")
        return df
    
    def fetch_margin_batch(
        self,
        stock_list: List[str],
        start_date: str,
        end_date: str,
        show_progress: bool = True,
        batch_size: int = 150,
        batch_sleep: float = 8.0
    ) -> pd.DataFrame:
        """
        æ‰¹é‡è·å–èèµ„èåˆ¸æ•°æ®
        
        Parameters
        ----------
        stock_list : List[str]
            è‚¡ç¥¨ä»£ç åˆ—è¡¨
        start_date : str
            å¼€å§‹æ—¥æœŸ
        end_date : str
            ç»“æŸæ—¥æœŸ
        show_progress : bool
            æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡
        batch_size : int
            æ¯æ‰¹æ¬¡å¤„ç†çš„è‚¡ç¥¨æ•°é‡
        batch_sleep : float
            æ¯æ‰¹æ¬¡ä¹‹é—´çš„ä¼‘æ¯æ—¶é—´ï¼ˆç§’ï¼‰
        
        Returns
        -------
        pd.DataFrame
            åˆå¹¶åçš„èèµ„èåˆ¸æ•°æ®
        """
        all_data = []
        total = len(stock_list)
        success_count = 0
        
        if show_progress:
            try:
                from tqdm import tqdm
                iterator = tqdm(
                    enumerate(stock_list),
                    total=total,
                    desc="ğŸ“Š è·å–èèµ„èåˆ¸",
                    unit="åª",
                    ncols=80
                )
            except ImportError:
                iterator = enumerate(stock_list)
                logger.info(f"å¼€å§‹è·å–èèµ„èåˆ¸æ•°æ®: {total} åªè‚¡ç¥¨...")
        else:
            iterator = enumerate(stock_list)
        
        for i, stock in iterator:
            df = self.fetch_margin_detail(stock, start_date, end_date)
            if df is not None and not df.empty:
                all_data.append(df)
                success_count += 1
            
            if show_progress and hasattr(iterator, 'set_postfix'):
                iterator.set_postfix({"æˆåŠŸ": success_count, "å½“å‰": stock})
            
            # æ‰¹æ¬¡ä¼‘æ¯
            if (i + 1) % batch_size == 0 and (i + 1) < total:
                if show_progress and hasattr(iterator, 'set_description'):
                    iterator.set_description(f"ğŸ“Š ä¼‘æ¯{batch_sleep}s")
                time.sleep(batch_sleep)
                if show_progress and hasattr(iterator, 'set_description'):
                    iterator.set_description("ğŸ“Š è·å–èèµ„èåˆ¸")
        
        if not all_data:
            return pd.DataFrame()
        
        result = pd.concat(all_data, ignore_index=True)
        logger.info(f"æ‰¹é‡è·å–èèµ„èåˆ¸å®Œæˆ: {success_count}/{total} åª, {len(result)} æ¡è®°å½•")
        return result
    
    def calculate_leverage_risk(
        self,
        stock_list: List[str],
        trade_date: str,
        lookback_days: int = 20
    ) -> pd.DataFrame:
        """
        è®¡ç®—æ æ†è¿‡çƒ­å› å­ (Leverage Overheat Factor)
        
        å…¸å‹çš„åå‘æŒ‡æ ‡ï¼šå½“æ•£æˆ·ç–¯ç‹‚èèµ„ä¹°å…¥æ—¶ï¼Œå¾€å¾€æ˜¯é˜¶æ®µæ€§é¡¶éƒ¨ã€‚
        
        Parameters
        ----------
        stock_list : List[str]
            è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼ˆ6ä½ä»£ç ï¼‰
        trade_date : str
            äº¤æ˜“æ—¥æœŸï¼Œæ ¼å¼ YYYYMMDD æˆ– YYYY-MM-DD
        lookback_days : int
            è®¡ç®—å†å²å‡å€¼å’Œæ ‡å‡†å·®çš„å›æº¯å¤©æ•°ï¼Œé»˜è®¤ 20 å¤©
        
        Returns
        -------
        pd.DataFrame
            æ æ†è¿‡çƒ­å› å­æ•°æ®ï¼ŒåŒ…å«ï¼š
            - stock_code: è‚¡ç¥¨ä»£ç 
            - rzye: èèµ„ä½™é¢ï¼ˆå…ƒï¼‰
            - rzmre: èèµ„ä¹°å…¥é¢ï¼ˆå…ƒï¼‰
            - margin_buy_ratio: èèµ„ä¹°å…¥å æ¯”ï¼ˆèèµ„ä¹°å…¥é¢/æˆäº¤é¢ï¼‰
            - margin_balance_ratio: èèµ„ä½™é¢å¸‚å€¼æ¯”ï¼ˆèèµ„ä½™é¢/æ€»å¸‚å€¼ï¼‰
            - leverage_heat: æ æ†è¿‡çƒ­å› å­ï¼ˆæ ‡å‡†åŒ–åï¼Œå€¼è¶Šå¤§é£é™©è¶Šé«˜ï¼‰
            - leverage_risk_score: é£é™©å¾—åˆ† (0-1ï¼Œè¶Šé«˜è¶Šå±é™©)
        
        Notes
        -----
        è®¡ç®—é€»è¾‘ï¼š
        1. èèµ„ä¹°å…¥å æ¯” = èèµ„ä¹°å…¥é¢ / å½“æ—¥æˆäº¤é¢ï¼ˆäº¤æ˜“æ‹¥æŒ¤åº¦ï¼‰
        2. èèµ„ä½™é¢å¸‚å€¼æ¯” = èèµ„ä½™é¢ / æ€»å¸‚å€¼ï¼ˆå­˜é‡æ æ†å‹åŠ›ï¼‰
        3. è¿‡çƒ­å› å­ = (å½“æ—¥èèµ„ä¹°å…¥å æ¯” - 20æ—¥å‡å€¼) / 20æ—¥æ ‡å‡†å·®
        
        é£é™©è­¦ç¤ºï¼š
        - leverage_heat > 2: æåº¦è¿‡çƒ­ï¼Œé«˜é£é™©
        - leverage_heat > 1: åçƒ­ï¼Œéœ€è­¦æƒ•
        - leverage_heat < -1: åå†·ï¼Œå¯èƒ½æ˜¯ä¹°å…¥æœºä¼š
        
        Examples
        --------
        >>> loader = TushareDataLoader()
        >>> stocks = ["000001", "000002", "600000"]
        >>> risk = loader.calculate_leverage_risk(stocks, "20240115")
        >>> # ç­›é€‰è¿‡çƒ­è‚¡ç¥¨ï¼ˆå¯èƒ½è§é¡¶ï¼‰
        >>> overheated = risk[risk['leverage_heat'] > 2]
        """
        trade_date = trade_date.replace("-", "")
        start_date = (
            datetime.strptime(trade_date, "%Y%m%d") - timedelta(days=lookback_days + 10)
        ).strftime("%Y%m%d")
        
        logger.info(f"ğŸ”¥ è®¡ç®—æ æ†è¿‡çƒ­å› å­: {len(stock_list)} åªè‚¡ç¥¨, {trade_date}")
        
        # 1. è·å–èèµ„èåˆ¸æ•°æ®
        margin_df = self.fetch_margin_batch(
            stock_list=stock_list,
            start_date=start_date,
            end_date=trade_date,
            show_progress=True
        )
        
        # 2. è·å–æˆäº¤é¢å’Œå¸‚å€¼æ•°æ®
        daily_basic = self.fetch_daily_basic(trade_date, stock_list)
        
        # 3. è·å–æ—¥çº¿æˆäº¤é¢
        daily_data = self.fetch_daily_data_batch(
            stock_list=stock_list,
            start_date=start_date,
            end_date=trade_date,
            show_progress=False
        )
        
        if margin_df.empty:
            logger.warning("æ— èèµ„èåˆ¸æ•°æ®ï¼Œè¿”å›ç©ºç»“æœ")
            return pd.DataFrame({"stock_code": stock_list, "leverage_heat": 0, "leverage_risk_score": 0.5})
        
        # 4. è®¡ç®—èèµ„ä¹°å…¥å æ¯”ï¼ˆéœ€è¦åˆå¹¶æˆäº¤é¢ï¼‰
        if not daily_data.empty and "amount" in daily_data.columns:
            # æŒ‰è‚¡ç¥¨å’Œæ—¥æœŸåˆå¹¶
            if "trade_date" in margin_df.columns:
                margin_df["trade_date"] = pd.to_datetime(margin_df["trade_date"])
            if "date" in daily_data.columns:
                daily_data = daily_data.rename(columns={"date": "trade_date"})
            
            margin_df = margin_df.merge(
                daily_data[["stock_code", "trade_date", "amount"]],
                on=["stock_code", "trade_date"],
                how="left"
            )
        
        # 5. è®¡ç®—èèµ„ä¹°å…¥å æ¯”
        if "rzmre" in margin_df.columns and "amount" in margin_df.columns:
            margin_df["margin_buy_ratio"] = (
                margin_df["rzmre"] / margin_df["amount"].replace(0, np.nan)
            )
        else:
            margin_df["margin_buy_ratio"] = np.nan
        
        # 6. è®¡ç®—å†å²å‡å€¼å’Œæ ‡å‡†å·®
        result_list = []
        for stock in stock_list:
            stock_margin = margin_df[margin_df["stock_code"] == stock].copy()
            
            if stock_margin.empty:
                result_list.append({
                    "stock_code": stock,
                    "rzye": np.nan,
                    "rzmre": np.nan,
                    "margin_buy_ratio": np.nan,
                    "margin_balance_ratio": np.nan,
                    "leverage_heat": 0,
                    "leverage_risk_score": 0.5
                })
                continue
            
            # æŒ‰æ—¥æœŸæ’åº
            stock_margin = stock_margin.sort_values("trade_date")
            
            # è®¡ç®—æ»šåŠ¨ç»Ÿè®¡
            stock_margin["ratio_mean"] = stock_margin["margin_buy_ratio"].rolling(
                window=lookback_days, min_periods=5
            ).mean()
            stock_margin["ratio_std"] = stock_margin["margin_buy_ratio"].rolling(
                window=lookback_days, min_periods=5
            ).std()
            
            # å–æœ€æ–°ä¸€å¤©çš„æ•°æ®
            latest = stock_margin.iloc[-1]
            
            # è®¡ç®—è¿‡çƒ­å› å­ï¼ˆZ-scoreï¼‰
            if pd.notna(latest.get("ratio_std")) and latest["ratio_std"] > 0:
                leverage_heat = (
                    (latest["margin_buy_ratio"] - latest["ratio_mean"]) / latest["ratio_std"]
                )
            else:
                leverage_heat = 0
            
            # è®¡ç®—èèµ„ä½™é¢å¸‚å€¼æ¯”
            margin_balance_ratio = np.nan
            if daily_basic is not None and not daily_basic.empty:
                stock_basic = daily_basic[daily_basic["stock_code"] == stock]
                if not stock_basic.empty and "rzye" in latest:
                    total_mv = stock_basic["total_mv"].iloc[0]
                    if pd.notna(total_mv) and total_mv > 0:
                        margin_balance_ratio = latest.get("rzye", 0) / total_mv
            
            result_list.append({
                "stock_code": stock,
                "rzye": latest.get("rzye", np.nan),
                "rzmre": latest.get("rzmre", np.nan),
                "margin_buy_ratio": latest.get("margin_buy_ratio", np.nan),
                "margin_balance_ratio": margin_balance_ratio,
                "leverage_heat": leverage_heat,
            })
        
        result = pd.DataFrame(result_list)
        
        # 7. è®¡ç®—é£é™©å¾—åˆ†ï¼ˆä½¿ç”¨æ’ååˆ†ä½æ•°ï¼‰
        valid_mask = result["leverage_heat"].notna() & (result["leverage_heat"] != 0)
        if valid_mask.sum() > 0:
            result.loc[valid_mask, "leverage_risk_score"] = (
                result.loc[valid_mask, "leverage_heat"].rank(pct=True)
            )
        else:
            result["leverage_risk_score"] = 0.5
        
        result["leverage_risk_score"] = result["leverage_risk_score"].fillna(0.5)
        
        # ç»Ÿè®¡ä¿¡æ¯
        overheated_count = (result["leverage_heat"] > 2).sum()
        cold_count = (result["leverage_heat"] < -1).sum()
        
        logger.info(
            f"âœ… æ æ†è¿‡çƒ­å› å­è®¡ç®—å®Œæˆ: "
            f"{len(result)} åª, è¿‡çƒ­={overheated_count}, åå†·={cold_count}"
        )
        
        return result
    
    def calculate_market_leverage_sentiment(
        self,
        trade_date: str,
        index_code: str = "hs300"
    ) -> Dict[str, Any]:
        """
        è®¡ç®—å¸‚åœºæ•´ä½“æ æ†æƒ…ç»ª
        
        è·å–æŒ‡å®šæŒ‡æ•°æˆåˆ†è‚¡çš„æ æ†æƒ…ç»ªæ±‡æ€»ï¼Œç”¨äºåˆ¤æ–­å¸‚åœºæ•´ä½“é£é™©æ°´å¹³ã€‚
        
        Parameters
        ----------
        trade_date : str
            äº¤æ˜“æ—¥æœŸ
        index_code : str
            æŒ‡æ•°ä»£ç ï¼Œå¦‚ "hs300", "zz500"
        
        Returns
        -------
        Dict[str, Any]
            å¸‚åœºæ æ†æƒ…ç»ªæŒ‡æ ‡ï¼š
            - avg_leverage_heat: å¹³å‡è¿‡çƒ­å› å­
            - overheated_ratio: è¿‡çƒ­è‚¡ç¥¨å æ¯”
            - cold_ratio: åå†·è‚¡ç¥¨å æ¯”
            - market_risk_level: å¸‚åœºé£é™©ç­‰çº§ï¼ˆlow/medium/high/extremeï¼‰
            - signal: ä¿¡å·å»ºè®®ï¼ˆbuy/hold/sellï¼‰
        
        Examples
        --------
        >>> loader = TushareDataLoader()
        >>> sentiment = loader.calculate_market_leverage_sentiment("20240115")
        >>> print(f"å¸‚åœºé£é™©ç­‰çº§: {sentiment['market_risk_level']}")
        >>> print(f"å»ºè®®æ“ä½œ: {sentiment['signal']}")
        """
        trade_date = trade_date.replace("-", "")
        
        logger.info(f"ğŸ“ˆ è®¡ç®—å¸‚åœºæ•´ä½“æ æ†æƒ…ç»ª: {index_code}, {trade_date}")
        
        # è·å–æˆåˆ†è‚¡
        stock_list = self.fetch_index_constituents(index_code)
        
        if not stock_list:
            return {
                "avg_leverage_heat": 0,
                "overheated_ratio": 0,
                "cold_ratio": 0,
                "market_risk_level": "unknown",
                "signal": "hold"
            }
        
        # é‡‡æ ·ï¼ˆé¿å…è¯·æ±‚è¿‡å¤šï¼‰
        sample_size = min(100, len(stock_list))
        sampled_stocks = stock_list[:sample_size]
        
        # è®¡ç®—æ æ†å› å­
        leverage_df = self.calculate_leverage_risk(
            stock_list=sampled_stocks,
            trade_date=trade_date
        )
        
        if leverage_df.empty:
            return {
                "avg_leverage_heat": 0,
                "overheated_ratio": 0,
                "cold_ratio": 0,
                "market_risk_level": "unknown",
                "signal": "hold"
            }
        
        # ç»Ÿè®¡æŒ‡æ ‡
        valid_heat = leverage_df["leverage_heat"].dropna()
        avg_heat = valid_heat.mean() if len(valid_heat) > 0 else 0
        overheated_ratio = (valid_heat > 2).sum() / len(valid_heat) if len(valid_heat) > 0 else 0
        cold_ratio = (valid_heat < -1).sum() / len(valid_heat) if len(valid_heat) > 0 else 0
        
        # åˆ¤æ–­é£é™©ç­‰çº§
        if avg_heat > 2 or overheated_ratio > 0.3:
            risk_level = "extreme"
            signal = "sell"
        elif avg_heat > 1 or overheated_ratio > 0.2:
            risk_level = "high"
            signal = "reduce"
        elif avg_heat > 0.5:
            risk_level = "medium"
            signal = "hold"
        elif avg_heat < -1 and cold_ratio > 0.3:
            risk_level = "low"
            signal = "buy"
        else:
            risk_level = "normal"
            signal = "hold"
        
        result = {
            "trade_date": trade_date,
            "index_code": index_code,
            "sample_size": len(sampled_stocks),
            "avg_leverage_heat": round(avg_heat, 3),
            "overheated_ratio": round(overheated_ratio, 3),
            "cold_ratio": round(cold_ratio, 3),
            "market_risk_level": risk_level,
            "signal": signal
        }
        
        logger.info(
            f"âœ… å¸‚åœºæ æ†æƒ…ç»ª: é£é™©={risk_level}, "
            f"è¿‡çƒ­={avg_heat:.2f}, è¿‡çƒ­æ¯”={overheated_ratio:.1%}"
        )
        
        return result
    
    def get_leverage_warning_stocks(
        self,
        stock_list: List[str],
        trade_date: str,
        heat_threshold: float = 2.0
    ) -> pd.DataFrame:
        """
        è·å–æ æ†è¿‡çƒ­é¢„è­¦è‚¡ç¥¨
        
        ç­›é€‰å‡ºèèµ„è¿‡çƒ­çš„è‚¡ç¥¨ï¼Œæç¤ºæ½œåœ¨çš„å›è°ƒé£é™©ã€‚
        
        Parameters
        ----------
        stock_list : List[str]
            è‚¡ç¥¨ä»£ç åˆ—è¡¨
        trade_date : str
            äº¤æ˜“æ—¥æœŸ
        heat_threshold : float
            è¿‡çƒ­é˜ˆå€¼ï¼Œé»˜è®¤ 2.0ï¼ˆ2å€æ ‡å‡†å·®ï¼‰
        
        Returns
        -------
        pd.DataFrame
            è¿‡çƒ­é¢„è­¦è‚¡ç¥¨åˆ—è¡¨
        
        Examples
        --------
        >>> loader = TushareDataLoader()
        >>> stocks = loader.fetch_index_constituents("hs300")
        >>> warnings = loader.get_leverage_warning_stocks(stocks, "20240115")
        >>> print(f"å‘ç° {len(warnings)} åªè¿‡çƒ­è‚¡ç¥¨")
        """
        leverage_df = self.calculate_leverage_risk(stock_list, trade_date)
        
        if leverage_df.empty:
            return pd.DataFrame()
        
        # ç­›é€‰è¿‡çƒ­è‚¡ç¥¨
        warnings = leverage_df[leverage_df["leverage_heat"] >= heat_threshold].copy()
        warnings = warnings.sort_values("leverage_heat", ascending=False)
        
        if len(warnings) > 0:
            logger.warning(
                f"âš ï¸ å‘ç° {len(warnings)} åªæ æ†è¿‡çƒ­è‚¡ç¥¨ "
                f"(heat >= {heat_threshold})"
            )
        
        return warnings


# ==================== ä¾¿æ·å‡½æ•° ====================

def create_tushare_loader(config: Optional[Dict] = None) -> TushareDataLoader:
    """
    åˆ›å»º Tushare æ•°æ®åŠ è½½å™¨
    
    Parameters
    ----------
    config : Optional[Dict]
        é…ç½®å­—å…¸ï¼ŒåŒ…å« tushare.api_token
    
    Returns
    -------
    TushareDataLoader
        æ•°æ®åŠ è½½å™¨å®ä¾‹
    """
    api_token = None
    
    if config:
        api_token = config.get("tushare", {}).get("api_token")
    
    if not api_token:
        api_token = os.environ.get("TUSHARE_TOKEN")
    
    return TushareDataLoader(api_token=api_token)

