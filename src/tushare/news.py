"""
Tushare æ–°é—»èµ„è®¯æ¨¡å—

è¯¥æ¨¡å—æä¾›æ–°é—»æ•°æ®è·å–åŠŸèƒ½ï¼Œä½œä¸º Mixin ç±»æ··å…¥ä¸»ç±»ã€‚
æ”¯æŒ Tushare å’Œ AKShare åŒæ•°æ®æºã€‚

Features
--------
- Tushare Pro æ–°é—»æ¥å£
- AKShare è´¢è”ç¤¾ç”µæŠ¥ï¼ˆå…è´¹æ— é™åˆ¶ï¼‰
- å¤šæ•°æ®æºè‡ªåŠ¨åˆ‡æ¢
- æ–°é—»ç¼“å­˜æœºåˆ¶
"""

from typing import Optional, Dict, List
from datetime import datetime, timedelta
import logging
import time

import pandas as pd

logger = logging.getLogger(__name__)

# å…¨å±€å˜é‡ï¼šè¿½è¸ªæ–°é—» API æœ€åè°ƒç”¨æ—¶é—´ï¼ˆè·¨å®ä¾‹å…±äº«ï¼‰
_GLOBAL_NEWS_API_LAST_CALL = 0.0
_GLOBAL_NEWS_RATE_LIMIT_COUNT = 0
# å…¨å±€æ–°é—»ç¼“å­˜ï¼ˆè·¨å®ä¾‹å…±äº«ï¼Œé¿å…é‡å¤è°ƒç”¨ APIï¼‰
_GLOBAL_NEWS_CACHE: Dict[str, pd.DataFrame] = {}


class TushareNewsMixin:
    """
    Tushare æ–°é—»èµ„è®¯ Mixin
    
    æä¾›æ–°é—»æ•°æ®è·å–åŠŸèƒ½ï¼Œéœ€è¦ä¸ TushareDataLoaderBase ç»„åˆä½¿ç”¨ã€‚
    
    Methods
    -------
    fetch_news(stock_code, start_date, end_date, src)
        è·å–æ–°é—»èµ„è®¯æ•°æ®
    fetch_news_multi_source(start_date, end_date, prefer_akshare)
        å¤šæ•°æ®æºæ–°é—»è·å–
    fetch_stock_news(stock_code, days_back)
        è·å–å•åªè‚¡ç¥¨ç›¸å…³æ–°é—»
    """
    
    def fetch_news(
        self,
        stock_code: Optional[str] = None,
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
        src: str = "sina"
    ) -> Optional[pd.DataFrame]:
        """
        è·å–æ–°é—»èµ„è®¯æ•°æ®
        
        ä½¿ç”¨ Tushare Pro news æ¥å£è·å–è´¢ç»æ–°é—»ã€‚
        
        Parameters
        ----------
        stock_code : Optional[str]
            è‚¡ç¥¨ä»£ç ï¼ˆ6ä½ï¼‰ï¼Œå¦‚æœæä¾›åˆ™è¿‡æ»¤ç›¸å…³æ–°é—»
        start_date : Optional[str]
            å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼ YYYYMMDD
        end_date : Optional[str]
            ç»“æŸæ—¥æœŸï¼Œæ ¼å¼ YYYYMMDD
        src : str
            æ–°é—»æ¥æºï¼Œå¯é€‰ï¼šsina(æ–°æµª), wallstreetcn(åå°”è¡—è§é—»), 
            10jqka(åŒèŠ±é¡º), eastmoney(ä¸œæ–¹è´¢å¯Œ), yuncaijing(äº‘è´¢ç»)
            é»˜è®¤ sina
        
        Returns
        -------
        Optional[pd.DataFrame]
            æ–°é—»æ•°æ®ï¼ŒåŒ…å« datetime, title, content, channels ç­‰å­—æ®µ
            å¤±è´¥è¿”å› None
        
        Notes
        -----
        - Tushare Pro æ–°é—»æ¥å£éœ€è¦è¾ƒé«˜ç§¯åˆ†æƒé™
        - å¦‚æœæ¥å£ä¸å¯ç”¨ï¼Œä¼šè¿”å›ç©º DataFrame
        
        Examples
        --------
        >>> loader = TushareDataLoader()
        >>> news = loader.fetch_news(start_date="20240101", end_date="20240115")
        >>> print(news[['datetime', 'title']].head())
        """
        # æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼
        if start_date:
            start_date = start_date.replace("-", "")
        if end_date:
            end_date = end_date.replace("-", "")
        
        global _GLOBAL_NEWS_API_LAST_CALL, _GLOBAL_NEWS_RATE_LIMIT_COUNT
        
        # æ£€æŸ¥æ˜¯å¦åœ¨é…ç½®ä¸­è·³è¿‡æ–°é—»è·å–
        if getattr(self, '_skip_news', False):
            logger.debug("æ–°é—»è·å–å·²åœ¨é…ç½®ä¸­ç¦ç”¨ (tushare.skip_news=true)")
            return pd.DataFrame()
        
        # æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡æ–°é—»è·å–ï¼ˆé¢‘ç‡é™åˆ¶ä¿æŠ¤ï¼‰
        if _GLOBAL_NEWS_RATE_LIMIT_COUNT >= 3:
            if _GLOBAL_NEWS_RATE_LIMIT_COUNT >= 100:
                logger.debug("æ–°é—»æ¥å£ä»Šæ—¥é…é¢å·²ç”¨å®Œï¼Œè·³è¿‡")
            elif _GLOBAL_NEWS_RATE_LIMIT_COUNT >= 10:
                logger.debug("æ–°é—»æ¥å£æœ¬å°æ—¶é…é¢å·²ç”¨å®Œï¼Œè·³è¿‡")
            else:
                logger.warning("æ–°é—»æ¥å£é¢‘ç¹è§¦å‘é™åˆ¶ï¼Œæœ¬æ¬¡è·³è¿‡ï¼ˆéœ€è¦æ›´é«˜ç§¯åˆ†æƒé™ï¼‰")
            return pd.DataFrame()
        
        # å°è¯•ç¼“å­˜ï¼ˆæ–°é—»æŒ‰æ—¥æœŸå’Œæ¥æºç¼“å­˜ï¼‰
        cache_key = f"news_{src}_{start_date}_{end_date}"
        if stock_code:
            cache_key += f"_{stock_code.replace('.', '')[:6]}"
        cache_file = self.cache_dir / f"{cache_key}.parquet"
        
        if cache_file.exists():
            try:
                # æ£€æŸ¥ç¼“å­˜æ˜¯å¦åœ¨24å°æ—¶å†…
                cache_mtime = datetime.fromtimestamp(cache_file.stat().st_mtime)
                if (datetime.now() - cache_mtime).total_seconds() < 86400:  # 24å°æ—¶
                    df = pd.read_parquet(cache_file)
                    if not df.empty:
                        logger.info(f"ä»ç¼“å­˜åŠ è½½æ–°é—»: {len(df)} æ¡")
                        return df
            except Exception:
                pass
        
        # æ–°é—»æ¥å£ç‰¹æ®Šé™æµï¼šæ¯åˆ†é’Ÿæœ€å¤š 1 æ¬¡ï¼ˆä½¿ç”¨å…¨å±€å˜é‡è·¨å®ä¾‹å…±äº«ï¼‰
        elapsed = time.time() - _GLOBAL_NEWS_API_LAST_CALL
        if elapsed < self.NEWS_API_INTERVAL:
            wait_time = self.NEWS_API_INTERVAL - elapsed
            logger.info(f"â³ æ–°é—»æ¥å£é™æµï¼ˆæ¯åˆ†é’Ÿ1æ¬¡ï¼‰ï¼Œç­‰å¾… {wait_time:.0f} ç§’...")
            time.sleep(wait_time)
        
        logger.info(f"è·å–æ–°é—»èµ„è®¯: src={src}, {start_date} ~ {end_date}")
        
        try:
            # æ›´æ–°å…¨å±€æœ€åè°ƒç”¨æ—¶é—´
            _GLOBAL_NEWS_API_LAST_CALL = time.time()
            
            df = self._fetch_with_retry(
                self.pro.news,
                src=src,
                start_date=start_date,
                end_date=end_date
            )
            
            if df is None:
                # _fetch_with_retry è¿”å› None è¯´æ˜å¯èƒ½é‡åˆ°é™æµ
                # å¢åŠ è®¡æ•°å™¨é¿å…åç»­é‡å¤å°è¯•
                _GLOBAL_NEWS_RATE_LIMIT_COUNT += 1
                logger.warning(
                    f"æ–°é—»æ¥å£è¯·æ±‚å¤±è´¥ (ç´¯è®¡ {_GLOBAL_NEWS_RATE_LIMIT_COUNT} æ¬¡)ï¼Œ"
                    "å¯èƒ½è§¦å‘é…é¢é™åˆ¶ï¼Œä½¿ç”¨ç¼“å­˜æˆ–è·³è¿‡"
                )
                
                # å°è¯•è¿”å›è¿‡æœŸç¼“å­˜ä½œä¸ºå›é€€
                if cache_file.exists():
                    try:
                        df = pd.read_parquet(cache_file)
                        if not df.empty:
                            logger.info(f"ä½¿ç”¨è¿‡æœŸç¼“å­˜å›é€€: {len(df)} æ¡æ–°é—»")
                            return df
                    except Exception:
                        pass
                
                return pd.DataFrame()
            
            if df.empty:
                logger.debug("æ— æ–°é—»æ•°æ®")
                return pd.DataFrame()
            
            # æˆåŠŸåˆ™é‡ç½®å…¨å±€è®¡æ•°å™¨
            _GLOBAL_NEWS_RATE_LIMIT_COUNT = 0
            
            # å¦‚æœæŒ‡å®šäº†è‚¡ç¥¨ä»£ç ï¼Œå°è¯•è¿‡æ»¤ç›¸å…³æ–°é—»
            if stock_code:
                # åœ¨æ ‡é¢˜æˆ–å†…å®¹ä¸­æœç´¢è‚¡ç¥¨ä»£ç æˆ–åç§°
                stock_code_clean = stock_code.replace(".", "")[:6]
                mask = (
                    df["title"].str.contains(stock_code_clean, na=False) |
                    df["content"].str.contains(stock_code_clean, na=False)
                )
                df = df[mask]
            
            # ä¿å­˜ç¼“å­˜
            if not df.empty:
                try:
                    df.to_parquet(cache_file, index=False)
                    logger.debug(f"æ–°é—»å·²ç¼“å­˜: {cache_file.name}")
                except Exception:
                    pass
            
            logger.info(f"è·å–æ–°é—»æˆåŠŸ: {len(df)} æ¡")
            return df
            
        except Exception as e:
            error_msg = str(e)
            # è®°å½•é¢‘ç‡é™åˆ¶ï¼ˆä½¿ç”¨å…¨å±€å˜é‡ï¼Œå·²åœ¨å‡½æ•°å¼€å¤´å£°æ˜ï¼‰
            if "æ¯å¤©" in error_msg:
                # æ¯å¤©é™åˆ¶ - ä»Šå¤©ä¸å†å°è¯•
                _GLOBAL_NEWS_RATE_LIMIT_COUNT = 100
                logger.warning(f"âš ï¸ æ–°é—»æ¥å£æ¯å¤©é…é¢å·²ç”¨å®Œï¼Œä»Šæ—¥è·³è¿‡æ–°é—»è·å–")
                logger.warning(f"   æç¤ºï¼šTushare æ–°é—»æ¥å£éœ€è¦è¾ƒé«˜ç§¯åˆ†ï¼ˆ2000+ï¼‰æ‰èƒ½è§£é™¤é™åˆ¶")
                logger.warning(f"   æç¤ºï¼šå¯åœ¨é…ç½®ä¸­è®¾ç½® llm.enable_sentiment_filter: false æš‚æ—¶ç¦ç”¨æƒ…ç»ªåˆ†æ")
            elif "æ¯å°æ—¶" in error_msg:
                # æ¯å°æ—¶é™åˆ¶ - æœ¬æ¬¡ä¼šè¯å†…ä¸å†å°è¯•
                _GLOBAL_NEWS_RATE_LIMIT_COUNT = 10
                logger.warning(f"âš ï¸ æ–°é—»æ¥å£æ¯å°æ—¶é™åˆ¶å·²è¾¾ä¸Šé™ï¼Œæœ¬æ¬¡è·³è¿‡æ–°é—»è·å–")
                logger.warning(f"   æç¤ºï¼šå¯åœ¨é…ç½®ä¸­è®¾ç½® llm.enable_sentiment_filter: false æš‚æ—¶ç¦ç”¨æƒ…ç»ªåˆ†æ")
            elif "æ¯åˆ†é’Ÿ" in error_msg or "é¢‘ç‡" in error_msg.lower() or "æŠ±æ­‰" in error_msg:
                _GLOBAL_NEWS_RATE_LIMIT_COUNT += 1
                logger.warning(f"æ–°é—»æ¥å£é¢‘ç‡é™åˆ¶ ({_GLOBAL_NEWS_RATE_LIMIT_COUNT}/3): {e}")
            else:
                logger.warning(f"è·å–æ–°é—»å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def _fetch_news_akshare(
        self,
        start_date: Optional[str] = None,
        end_date: Optional[str] = None
    ) -> pd.DataFrame:
        """
        ä½¿ç”¨ AKShare è·å–è´¢ç»æ–°é—»ï¼ˆå‡çº§ç‰ˆï¼šè´¢è”ç¤¾ç”µæŠ¥ä¸ºæ ¸å¿ƒæºï¼‰
        
        AKShare æ˜¯å…è´¹å¼€æºçš„æ•°æ®æ¥å£ï¼Œæ— é…é¢é™åˆ¶ã€‚
        ä¼˜å…ˆä½¿ç”¨è´¢è”ç¤¾ç”µæŠ¥ä½œä¸º A è‚¡æœ€å¿«æ¶ˆæ¯æºã€‚
        
        Parameters
        ----------
        start_date : Optional[str]
            å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼ YYYYMMDD
        end_date : Optional[str]
            ç»“æŸæ—¥æœŸï¼Œæ ¼å¼ YYYYMMDD
        
        Returns
        -------
        pd.DataFrame
            æ–°é—»æ•°æ®ï¼ŒåŒ…å« datetime, title, content, source åˆ—
        
        Notes
        -----
        æ•°æ®æºä¼˜å…ˆçº§:
        1. stock_telegraph_cls(): è´¢è”ç¤¾ç”µæŠ¥ï¼ˆAè‚¡æœ€å¿«ï¼Œæµå¼æ¥å£ï¼Œçº¦300æ¡ï¼‰
        2. stock_zh_a_alerts_cls(): è´¢è”ç¤¾å¿«è®¯ï¼ˆè¡¥å……ï¼‰
        3. stock_news_em(): ä¸œæ–¹è´¢å¯Œè‚¡ç¥¨æ–°é—»ï¼ˆæ·±åº¦æŠ¥é“è¡¥å……ï¼‰
        """
        try:
            import akshare as ak
        except ImportError:
            logger.warning("AKShare æœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨å¤‡é€‰æ–°é—»æºã€‚å®‰è£…: pip install akshare")
            return pd.DataFrame()
        
        all_news = []
        today_str = datetime.now().strftime("%Y-%m-%d")
        
        # =====================================================================
        # æ ¸å¿ƒæºï¼šè´¢è”ç¤¾ç”µæŠ¥ (Aè‚¡æœ€å¿«æ¶ˆæ¯æº)
        # =====================================================================
        try:
            logger.info("âš¡ è·å–è´¢è”ç¤¾ç”µæŠ¥ (CLS Telegraph)...")
            df_telegraph = ak.stock_telegraph_cls(symbol="å…¨éƒ¨")
            if df_telegraph is not None and not df_telegraph.empty:
                # æ ‡å‡†åŒ–åˆ—å
                col_mapping = {
                    'å‘å¸ƒæ—¶é—´': 'time_str',
                    'å‘å¸ƒæ—¥æœŸ': 'date_str', 
                    'æ ‡é¢˜': 'title',
                    'å†…å®¹': 'content'
                }
                df_telegraph = df_telegraph.rename(columns={
                    k: v for k, v in col_mapping.items() if k in df_telegraph.columns
                })
                
                # å¤„ç†æ—¥æœŸæ—¶é—´
                if 'time_str' in df_telegraph.columns:
                    if 'date_str' in df_telegraph.columns:
                        df_telegraph['datetime'] = pd.to_datetime(
                            df_telegraph['date_str'].astype(str) + ' ' + 
                            df_telegraph['time_str'].astype(str),
                            errors='coerce'
                        )
                    else:
                        now = datetime.now()
                        current_time = now.strftime("%H:%M:%S")
                        
                        def infer_date(time_val: str) -> str:
                            try:
                                time_str = str(time_val).strip()
                                if time_str > current_time:
                                    yesterday = (now - timedelta(days=1)).strftime("%Y-%m-%d")
                                    return yesterday + ' ' + time_str
                                else:
                                    return today_str + ' ' + time_str
                            except Exception:
                                return today_str + ' ' + str(time_val)
                        
                        df_telegraph['datetime'] = df_telegraph['time_str'].apply(infer_date)
                        df_telegraph['datetime'] = pd.to_datetime(
                            df_telegraph['datetime'], errors='coerce'
                        )
                elif 'datetime' not in df_telegraph.columns:
                    for col in df_telegraph.columns:
                        if 'æ—¶é—´' in col or 'æ—¥æœŸ' in col:
                            df_telegraph['datetime'] = pd.to_datetime(
                                df_telegraph[col], errors='coerce'
                            )
                            break
                    else:
                        df_telegraph['datetime'] = pd.Timestamp.now()
                
                if 'title' not in df_telegraph.columns:
                    if 'content' in df_telegraph.columns:
                        df_telegraph['title'] = df_telegraph['content'].str[:50] + '...'
                    else:
                        df_telegraph['title'] = ''
                
                if 'content' not in df_telegraph.columns:
                    if 'title' in df_telegraph.columns:
                        df_telegraph['content'] = df_telegraph['title']
                    else:
                        df_telegraph['content'] = ''
                
                df_telegraph['source'] = 'è´¢è”ç¤¾ç”µæŠ¥'
                all_news.append(df_telegraph[['datetime', 'title', 'content', 'source']])
                logger.info(f"âœ… è´¢è”ç¤¾ç”µæŠ¥è·å–æˆåŠŸ: {len(df_telegraph)} æ¡")
        except Exception as e:
            logger.debug(f"è´¢è”ç¤¾ç”µæŠ¥è·å–å¤±è´¥: {e}")
        
        # =====================================================================
        # è¾…åŠ©æº1ï¼šè´¢è”ç¤¾å¿«è®¯
        # =====================================================================
        try:
            logger.info("ğŸ“° è·å–è´¢è”ç¤¾å¿«è®¯ (CLS Alerts)...")
            df_cls = ak.stock_zh_a_alerts_cls()
            if df_cls is not None and not df_cls.empty:
                df_cls = df_cls.rename(columns={
                    'æ—¶é—´': 'datetime',
                    'æ ‡é¢˜': 'title', 
                    'å†…å®¹': 'content'
                })
                
                if 'title' not in df_cls.columns and 'content' in df_cls.columns:
                    df_cls['title'] = df_cls['content'].str[:50] + '...'
                if 'content' not in df_cls.columns and 'title' in df_cls.columns:
                    df_cls['content'] = df_cls['title']
                
                if 'datetime' in df_cls.columns:
                    df_cls['datetime'] = pd.to_datetime(df_cls['datetime'], errors='coerce')
                
                df_cls['source'] = 'è´¢è”ç¤¾å¿«è®¯'
                all_news.append(df_cls[['datetime', 'title', 'content', 'source']])
                logger.info(f"âœ… è´¢è”ç¤¾å¿«è®¯è·å–æˆåŠŸ: {len(df_cls)} æ¡")
        except Exception as e:
            logger.debug(f"è´¢è”ç¤¾å¿«è®¯è·å–å¤±è´¥: {e}")
        
        # =====================================================================
        # è¾…åŠ©æº2ï¼šä¸œæ–¹è´¢å¯Œè‚¡ç¥¨æ–°é—»
        # =====================================================================
        try:
            logger.info("ğŸ“° è·å–ä¸œæ–¹è´¢å¯Œæ–°é—» (EM)...")
            df_em = ak.stock_news_em(symbol="å…¨éƒ¨")
            if df_em is not None and not df_em.empty:
                df_em = df_em.rename(columns={
                    'å‘å¸ƒæ—¶é—´': 'datetime',
                    'æ–°é—»æ ‡é¢˜': 'title',
                    'æ–°é—»å†…å®¹': 'content',
                    'æ–‡ç« æ¥æº': 'source'
                })
                
                if 'source' not in df_em.columns:
                    df_em['source'] = 'ä¸œæ–¹è´¢å¯Œ'
                
                if 'datetime' in df_em.columns:
                    df_em['datetime'] = pd.to_datetime(df_em['datetime'], errors='coerce')
                    
                all_news.append(df_em[['datetime', 'title', 'content', 'source']])
                logger.info(f"âœ… ä¸œæ–¹è´¢å¯Œæ–°é—»è·å–æˆåŠŸ: {len(df_em)} æ¡")
        except Exception as e:
            logger.debug(f"ä¸œæ–¹è´¢å¯Œæ–°é—»è·å–å¤±è´¥: {e}")
        
        # =====================================================================
        # åˆå¹¶ã€å»é‡ã€è¿‡æ»¤
        # =====================================================================
        if not all_news:
            logger.warning("âš ï¸ AKShare æ‰€æœ‰æ–°é—»æºå‡è·å–å¤±è´¥")
            return pd.DataFrame()
        
        result = pd.concat(all_news, ignore_index=True)
        
        # åŸºäº content å»é‡ï¼ˆä¼˜å…ˆä¿ç•™è´¢è”ç¤¾ç”µæŠ¥çš„æ•°æ®ï¼‰
        source_priority = {'è´¢è”ç¤¾ç”µæŠ¥': 0, 'è´¢è”ç¤¾å¿«è®¯': 1, 'ä¸œæ–¹è´¢å¯Œ': 2}
        result['_priority'] = result['source'].map(source_priority).fillna(99)
        result = result.sort_values('_priority')
        result = result.drop_duplicates(subset=['content'], keep='first')
        result = result.drop(columns=['_priority'])
        
        # æ—¥æœŸè¿‡æ»¤
        if 'datetime' in result.columns:
            try:
                result = result.dropna(subset=['datetime'])
                
                if start_date:
                    start_dt = pd.to_datetime(start_date)
                    result = result[result['datetime'] >= start_dt]
                if end_date:
                    end_dt = pd.to_datetime(end_date) + timedelta(days=1)
                    result = result[result['datetime'] < end_dt]
                
                result = result.sort_values('datetime', ascending=False)
            except Exception as e:
                logger.debug(f"æ—¥æœŸè¿‡æ»¤å¼‚å¸¸: {e}")
        
        logger.info(
            f"ğŸ“Š AKShare æ–°é—»æ±‡æ€»: {len(result)} æ¡ | "
            f"ç”µæŠ¥: {len(result[result['source'] == 'è´¢è”ç¤¾ç”µæŠ¥']) if 'è´¢è”ç¤¾ç”µæŠ¥' in result['source'].values else 0}, "
            f"å¿«è®¯: {len(result[result['source'] == 'è´¢è”ç¤¾å¿«è®¯']) if 'è´¢è”ç¤¾å¿«è®¯' in result['source'].values else 0}, "
            f"ä¸œè´¢: {len(result[result['source'] == 'ä¸œæ–¹è´¢å¯Œ']) if 'ä¸œæ–¹è´¢å¯Œ' in result['source'].values else 0}"
        )
        return result
    
    def fetch_news_multi_source(
        self,
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
        prefer_akshare: bool = False
    ) -> pd.DataFrame:
        """
        å¤šæ•°æ®æºæ–°é—»è·å–ï¼ˆè‡ªåŠ¨åˆ‡æ¢ï¼‰
        
        æ ¹æ®é…ç½®å’Œå¯ç”¨æ€§é€‰æ‹©æ•°æ®æºï¼š
        - tushare: ä»…ä½¿ç”¨ Tushareï¼ˆéœ€è¦ç§¯åˆ†ï¼‰
        - akshare: ä»…ä½¿ç”¨ AKShareï¼ˆå…è´¹æ— é™åˆ¶ï¼‰
        - auto: ä¼˜å…ˆ Tushareï¼Œé™æµæ—¶è‡ªåŠ¨åˆ‡æ¢åˆ° AKShare
        
        Parameters
        ----------
        start_date : Optional[str]
            å¼€å§‹æ—¥æœŸ
        end_date : Optional[str]
            ç»“æŸæ—¥æœŸ
        prefer_akshare : bool
            æ˜¯å¦ä¼˜å…ˆä½¿ç”¨ AKShareï¼Œé»˜è®¤ False
        
        Returns
        -------
        pd.DataFrame
            æ–°é—»æ•°æ®
        """
        global _GLOBAL_NEWS_RATE_LIMIT_COUNT
        
        # è¯»å–é…ç½®çš„æ–°é—»æ•°æ®æº
        news_source = getattr(self, '_news_source', 'auto')
        
        # å¦‚æœé…ç½®ä¸ºä»…ä½¿ç”¨ AKShare
        if news_source == 'akshare':
            logger.info("é…ç½®æŒ‡å®šä½¿ç”¨ AKShare ä½œä¸ºæ–°é—»æ•°æ®æº")
            return self._fetch_news_akshare(start_date, end_date)
        
        # å¦‚æœé…ç½®ä¸ºä»…ä½¿ç”¨ Tushare
        if news_source == 'tushare':
            return self.fetch_news(start_date=start_date, end_date=end_date)
        
        # è‡ªåŠ¨æ¨¡å¼ (auto)
        if _GLOBAL_NEWS_RATE_LIMIT_COUNT > 0 or prefer_akshare:
            logger.debug(f"Tushare æ–°é—»æ¥å£å·²é™æµ (è®¡æ•°={_GLOBAL_NEWS_RATE_LIMIT_COUNT})ï¼Œä½¿ç”¨ AKShare")
            return self._fetch_news_akshare(start_date, end_date)
        
        # å°è¯• Tushare
        df = self.fetch_news(start_date=start_date, end_date=end_date)
        
        # å¦‚æœ Tushare å¤±è´¥ï¼Œå›é€€åˆ° AKShare
        if df.empty:
            logger.info("Tushare æ–°é—»è·å–å¤±è´¥ï¼Œåˆ‡æ¢åˆ° AKShare")
            return self._fetch_news_akshare(start_date, end_date)
        
        return df
    
    def fetch_all_news_once(
        self,
        days_back: int = 7,
        src: str = "sina"
    ) -> pd.DataFrame:
        """
        ä¸€æ¬¡æ€§è·å–æ‰€æœ‰æ–°é—»ï¼ˆä¼˜åŒ–ï¼šé¿å…å¤šæ¬¡ API è°ƒç”¨ï¼‰
        
        è·å–æœ€è¿‘å‡ å¤©çš„æ‰€æœ‰æ–°é—»ï¼Œç¼“å­˜åä¾›å¤šåªè‚¡ç¥¨ä½¿ç”¨ã€‚
        æ–°é—»æ¥å£æ¯åˆ†é’Ÿåªèƒ½è°ƒç”¨1æ¬¡ï¼Œå› æ­¤ä¸€æ¬¡è·å–å…¨éƒ¨æ•°æ®æ›´é«˜æ•ˆã€‚
        
        Parameters
        ----------
        days_back : int
            å›æº¯å¤©æ•°ï¼Œé»˜è®¤ 7 å¤©
        src : str
            æ–°é—»æºï¼Œé»˜è®¤ sina
        
        Returns
        -------
        pd.DataFrame
            æ‰€æœ‰æ–°é—»æ•°æ®
        """
        global _GLOBAL_NEWS_CACHE
        
        end_date = datetime.now().strftime("%Y%m%d")
        start_date = (datetime.now() - timedelta(days=days_back)).strftime("%Y%m%d")
        
        # ä½¿ç”¨å…¨å±€ç¼“å­˜ï¼ˆè·¨å®ä¾‹å…±äº«ï¼‰
        cache_key = f"all_news_{start_date}_{end_date}"
        if cache_key in _GLOBAL_NEWS_CACHE:
            cached = _GLOBAL_NEWS_CACHE[cache_key]
            if cached is not None and not cached.empty:
                logger.debug(f"ä½¿ç”¨å…¨å±€ç¼“å­˜çš„æ–°é—»æ•°æ®: {len(cached)} æ¡")
                return cached
        
        # è·å–æ‰€æœ‰æ–°é—»
        df = self.fetch_news_multi_source(
            start_date=start_date,
            end_date=end_date
        )
        
        # ç¼“å­˜åˆ°å…¨å±€å˜é‡
        _GLOBAL_NEWS_CACHE[cache_key] = df if df is not None else pd.DataFrame()
        
        if df is not None and not df.empty:
            logger.info(f"ğŸ“° ä¸€æ¬¡æ€§è·å–æ–°é—»å®Œæˆ: {len(df)} æ¡ï¼Œå¯ä¾›æ‰€æœ‰è‚¡ç¥¨ä½¿ç”¨")
        
        return df if df is not None else pd.DataFrame()
    
    def _load_all_stock_names(self) -> None:
        """
        ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰è‚¡ç¥¨åç§°åˆ°å…¨å±€ç¼“å­˜
        
        é¿å…ä¸ºæ¯åªè‚¡ç¥¨å•ç‹¬æŸ¥è¯¢ APIã€‚
        """
        global _GLOBAL_NEWS_CACHE
        
        # æ£€æŸ¥æ˜¯å¦å·²åŠ è½½
        if '_stock_names_loaded' in _GLOBAL_NEWS_CACHE:
            return
        
        logger.info("ğŸ“‹ æ‰¹é‡åŠ è½½è‚¡ç¥¨åç§°...")
        
        try:
            df = self._fetch_with_retry(
                self.pro.stock_basic,
                exchange='',
                list_status='L',
                fields='ts_code,name'
            )
            
            if df is not None and not df.empty:
                for _, row in df.iterrows():
                    code = row['ts_code'][:6]
                    name = row['name']
                    _GLOBAL_NEWS_CACHE[f"stock_name_{code}"] = name
                
                logger.info(f"ğŸ“‹ è‚¡ç¥¨åç§°åŠ è½½å®Œæˆ: {len(df)} åª")
            
            _GLOBAL_NEWS_CACHE['_stock_names_loaded'] = True
            
        except Exception as e:
            logger.warning(f"æ‰¹é‡åŠ è½½è‚¡ç¥¨åç§°å¤±è´¥: {e}")
            _GLOBAL_NEWS_CACHE['_stock_names_loaded'] = True
    
    def _get_stock_name(self, stock_code: str) -> Optional[str]:
        """
        è·å–è‚¡ç¥¨åç§°ï¼ˆç”¨äºæ–°é—»åŒ¹é…ï¼‰
        
        Parameters
        ----------
        stock_code : str
            è‚¡ç¥¨ä»£ç ï¼ˆ6ä½ï¼‰
        
        Returns
        -------
        Optional[str]
            è‚¡ç¥¨åç§°ï¼Œå¦‚"è´µå·èŒ…å°"ï¼Œè·å–å¤±è´¥è¿”å› None
        """
        global _GLOBAL_NEWS_CACHE
        
        # ç¡®ä¿è‚¡ç¥¨åç§°å·²æ‰¹é‡åŠ è½½
        if '_stock_names_loaded' not in _GLOBAL_NEWS_CACHE:
            self._load_all_stock_names()
        
        cache_key = f"stock_name_{stock_code}"
        return _GLOBAL_NEWS_CACHE.get(cache_key)
    
    def fetch_stock_news(
        self,
        stock_code: str,
        days_back: int = 7
    ) -> str:
        """
        è·å–å•åªè‚¡ç¥¨ç›¸å…³æ–°é—»ï¼ˆç”¨äºæƒ…æ„Ÿåˆ†æï¼‰
        
        ä»ç¼“å­˜çš„å…¨é‡æ–°é—»ä¸­ç­›é€‰ä¸æŒ‡å®šè‚¡ç¥¨ç›¸å…³çš„æ–°é—»ã€‚
        ä¼˜åŒ–ï¼šåªè°ƒç”¨ä¸€æ¬¡ API è·å–å…¨é‡æ–°é—»ï¼Œç„¶åæœ¬åœ°ç­›é€‰ã€‚
        
        Parameters
        ----------
        stock_code : str
            è‚¡ç¥¨ä»£ç ï¼ˆ6ä½ï¼‰
        days_back : int
            å›æº¯å¤©æ•°ï¼Œé»˜è®¤ 7 å¤©
        
        Returns
        -------
        str
            åˆå¹¶çš„æ–°é—»æ–‡æœ¬ï¼Œç”¨äºæƒ…æ„Ÿåˆ†æ
            æ— æ–°é—»æ—¶è¿”å›ç©ºå­—ç¬¦ä¸²
        """
        # å…ˆè·å–å…¨é‡æ–°é—»ï¼ˆä¼šè‡ªåŠ¨ç¼“å­˜ï¼‰
        all_news_df = self.fetch_all_news_once(days_back=days_back)
        
        if all_news_df.empty:
            logger.debug(f"æ— æ–°é—»æ•°æ®å¯ç”¨")
            return ""
        
        # ä»å…¨é‡æ–°é—»ä¸­ç­›é€‰
        stock_code_clean = stock_code.replace(".", "")[:6]
        
        # è·å–è‚¡ç¥¨åç§°ç”¨äºåŒ¹é…
        stock_name = self._get_stock_name(stock_code_clean)
        
        # åˆ›å»º mask
        mask = pd.Series(False, index=all_news_df.index)
        
        # åŒ¹é…æ¡ä»¶
        search_terms = [stock_code_clean]
        if stock_name:
            search_terms.append(stock_name)
            if len(stock_name) > 2:
                search_terms.append(stock_name[-2:])
        
        for term in search_terms:
            if "title" in all_news_df.columns:
                mask = mask | all_news_df["title"].str.contains(term, na=False, regex=False)
            if "content" in all_news_df.columns:
                mask = mask | all_news_df["content"].str.contains(term, na=False, regex=False)
        
        filtered_df = all_news_df.loc[mask]
        
        if filtered_df.empty:
            logger.debug(f"è‚¡ç¥¨ {stock_code} ({stock_name or 'æœªçŸ¥'}) æ— ç›¸å…³æ–°é—»")
            return ""
        
        # æå–æ ‡é¢˜å’Œå†…å®¹
        all_news = []
        for _, row in filtered_df.head(5).iterrows():
            title = row.get("title", "")
            content = row.get("content", "")
            if title:
                all_news.append(str(title))
            if content and len(str(content)) < 500:
                all_news.append(str(content)[:200])
        
        if not all_news:
            return ""
        
        # åˆå¹¶æ–°é—»æ–‡æœ¬
        combined = " | ".join(all_news)
        
        # æˆªæ–­
        if len(combined) > 1500:
            combined = combined[:1500] + "..."
        
        logger.debug(f"è·å–è‚¡ç¥¨æ–°é—»æˆåŠŸ: {stock_code}, {len(all_news)} æ¡")
        return combined

