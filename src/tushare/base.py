"""
Tushare æ•°æ®åŠ è½½å™¨åŸºç¡€æ¨¡å—

è¯¥æ¨¡å—æä¾› TushareDataLoaderBase åŸºç±»ï¼ŒåŒ…å«ï¼š
- API åˆå§‹åŒ–å’Œé™æµæœºåˆ¶
- æ—¥çº¿è¡Œæƒ…æ•°æ®è·å–
- è´¢åŠ¡æŒ‡æ ‡æ•°æ®è·å–
- æŒ‡æ•°æˆåˆ†è‚¡å’Œå…¨å¸‚åœºè‚¡ç¥¨
- äº¤æ˜“æ—¥å†å’Œè¡Œä¸šåˆ†ç±»

Notes
-----
ä½¿ç”¨å‰éœ€è¦é…ç½® Tushare API Tokenï¼š
1. åœ¨ config/strategy_config.yaml ä¸­è®¾ç½® tushare.api_token
2. æˆ–é€šè¿‡ç¯å¢ƒå˜é‡ TUSHARE_TOKEN è®¾ç½®
"""

from typing import Optional, List, Dict, Any
from datetime import datetime, timedelta
from pathlib import Path
import logging
import time
import os

import pandas as pd
import numpy as np

logger = logging.getLogger(__name__)


class TushareDataLoaderBase:
    """
    Tushare Pro æ•°æ®åŠ è½½å™¨åŸºç±»
    
    æä¾›ç¨³å®šå¯é çš„ A è‚¡æ•°æ®è·å–æœåŠ¡ï¼ŒåŒ…æ‹¬ï¼š
    - æ—¥çº¿è¡Œæƒ…æ•°æ® (OHLCV + åŸºç¡€æŒ‡æ ‡)
    - è´¢åŠ¡æŒ‡æ ‡æ•°æ® (PE, PB, ROE ç­‰)
    - æŒ‡æ•°æˆåˆ†è‚¡æƒé‡
    - è‚¡ç¥¨åŸºç¡€ä¿¡æ¯
    
    Parameters
    ----------
    api_token : Optional[str]
        Tushare API Tokenï¼Œå¦‚æœä¸æä¾›åˆ™ä»ç¯å¢ƒå˜é‡ TUSHARE_TOKEN è¯»å–
    cache_dir : str
        æœ¬åœ°ç¼“å­˜ç›®å½•ï¼Œé»˜è®¤ "data/tushare_cache"
    
    Attributes
    ----------
    pro : tushare.pro_api
        Tushare Pro API å®ä¾‹
    cache_dir : Path
        ç¼“å­˜ç›®å½•è·¯å¾„
    
    Examples
    --------
    >>> loader = TushareDataLoaderBase(api_token="your_token")
    >>> df = loader.fetch_daily_data("000001.SZ", "20240101", "20241231")
    >>> financial = loader.fetch_financial_indicator("000001.SZ")
    """
    
    # API è¯·æ±‚é™æµå‚æ•°
    # æ™®é€šç”¨æˆ·é™åˆ¶: 200 æ¬¡/åˆ†é’Ÿ = 3.33 æ¬¡/ç§’
    # ä»˜è´¹ç”¨æˆ·é™åˆ¶æ›´é«˜ï¼Œå¯é€‚å½“é™ä½é—´éš”
    REQUEST_INTERVAL = 0.12  # æ¯æ¬¡è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰- æ¿€è¿›æ¨¡å¼
    MAX_RETRIES = 3
    RETRY_DELAY = 2.0  # é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰
    RATE_LIMIT_DELAY = 30.0  # è§¦å‘é¢‘ç‡é™åˆ¶åç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
    HTTP_TIMEOUT = 60  # HTTP è¯·æ±‚è¶…æ—¶ï¼ˆç§’ï¼‰
    
    # æ–°é—»æ¥å£ç‰¹æ®Šé™åˆ¶ï¼šæ¯åˆ†é’Ÿæœ€å¤š 1 æ¬¡
    NEWS_API_INTERVAL = 61.0  # æ–°é—»æ¥å£è°ƒç”¨é—´éš”ï¼ˆç§’ï¼‰
    
    # è‚¡ç¥¨æ± ä»£ç æ˜ å°„
    INDEX_CODE_MAPPING = {
        "hs300": "000300.SH",
        "zz500": "000905.SH",
        "zz1000": "000852.SH",
        "sz50": "000016.SH",
        "cyb": "399006.SZ",  # åˆ›ä¸šæ¿æŒ‡
    }
    
    def __init__(
        self,
        api_token: Optional[str] = None,
        cache_dir: str = "data/tushare_cache"
    ) -> None:
        """
        åˆå§‹åŒ– Tushare æ•°æ®åŠ è½½å™¨
        
        Parameters
        ----------
        api_token : Optional[str]
            Tushare API Token
        cache_dir : str
            ç¼“å­˜ç›®å½•
        """
        # è·å– API Token (ä¼˜å…ˆçº§: å‚æ•° > ç¯å¢ƒå˜é‡ > é…ç½®æ–‡ä»¶)
        self.api_token = api_token or os.environ.get("TUSHARE_TOKEN", "")
        
        # å°è¯•ä»é…ç½®æ–‡ä»¶è¯»å–
        self._skip_news = False  # é»˜è®¤ä¸è·³è¿‡æ–°é—»
        self._news_source = "auto"
        try:
            import yaml
            config_path = Path("config/strategy_config.yaml")
            if config_path.exists():
                with open(config_path, "r", encoding="utf-8") as f:
                    config = yaml.safe_load(f)
                tushare_config = config.get("tushare", {})
                
                # è¯»å– Tokenï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
                if not self.api_token:
                    self.api_token = tushare_config.get("api_token", "")
                    if self.api_token:
                        logger.info("ä»é…ç½®æ–‡ä»¶åŠ è½½ Tushare Token")
                
                # è¯»å– skip_news é…ç½®
                self._skip_news = tushare_config.get("skip_news", False)
                if self._skip_news:
                    logger.info("ğŸ“° æ–°é—»è·å–å·²ç¦ç”¨ (tushare.skip_news=true)")
                
                # è¯»å– news_source é…ç½®
                self._news_source = tushare_config.get("news_source", "auto")
                if self._news_source == "akshare":
                    logger.info("ğŸ“° æ–°é—»æ•°æ®æº: AKShare (å…è´¹æ— é™åˆ¶)")
                elif self._news_source == "tushare":
                    logger.info("ğŸ“° æ–°é—»æ•°æ®æº: Tushare")
                else:
                    logger.debug("ğŸ“° æ–°é—»æ•°æ®æº: è‡ªåŠ¨åˆ‡æ¢ (Tushare -> AKShare)")
        except Exception as e:
            logger.debug(f"ä»é…ç½®æ–‡ä»¶è¯»å–é…ç½®å¤±è´¥: {e}")
        
        if not self.api_token:
            raise ValueError(
                "Tushare API Token æœªé…ç½®ï¼\n"
                "è¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼ä¹‹ä¸€é…ç½®ï¼š\n"
                "1. æ„é€ å‡½æ•°å‚æ•° api_token\n"
                "2. ç¯å¢ƒå˜é‡ TUSHARE_TOKEN\n"
                "3. config/strategy_config.yaml ä¸­çš„ tushare.api_token\n"
                "è·å– Token: https://tushare.pro/register"
            )
        
        # åˆå§‹åŒ– Tushare Pro API
        try:
            import tushare as ts
            
            # è®¾ç½® Token å¹¶åˆå§‹åŒ– API
            ts.set_token(self.api_token)
            self.pro = ts.pro_api()
            
            # é…ç½®æ›´é•¿çš„ HTTP è¶…æ—¶ï¼ˆé€šè¿‡ä¿®æ”¹åº•å±‚ DataApiï¼‰
            try:
                if hasattr(self.pro, '_DataApi__http'):
                    # æ–°ç‰ˆ Tushare ä½¿ç”¨ __http å±æ€§
                    self.pro._DataApi__http.timeout = self.HTTP_TIMEOUT
                elif hasattr(self.pro, 'timeout'):
                    self.pro.timeout = self.HTTP_TIMEOUT
                logger.info(f"Tushare Pro API åˆå§‹åŒ–æˆåŠŸ (timeout={self.HTTP_TIMEOUT}s)")
            except Exception:
                logger.info("Tushare Pro API åˆå§‹åŒ–æˆåŠŸ")
                
        except ImportError:
            raise ImportError("è¯·å®‰è£… tushare: pip install tushare")
        except Exception as e:
            raise RuntimeError(f"Tushare API åˆå§‹åŒ–å¤±è´¥: {e}")
        
        # è®¾ç½®ç¼“å­˜ç›®å½•
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # è¯·æ±‚è®¡æ•°å™¨ï¼ˆç”¨äºé™æµï¼‰
        self._last_request_time = 0.0
    
    def _rate_limit(self) -> None:
        """API è¯·æ±‚é™æµ"""
        elapsed = time.time() - self._last_request_time
        if elapsed < self.REQUEST_INTERVAL:
            time.sleep(self.REQUEST_INTERVAL - elapsed)
        self._last_request_time = time.time()
    
    def _fetch_with_retry(
        self,
        func,
        *args,
        **kwargs
    ) -> Optional[pd.DataFrame]:
        """
        å¸¦é‡è¯•çš„ API è¯·æ±‚
        
        Parameters
        ----------
        func : callable
            Tushare API å‡½æ•°
        *args, **kwargs
            å‡½æ•°å‚æ•°
        
        Returns
        -------
        Optional[pd.DataFrame]
            è¿”å›æ•°æ®ï¼Œå¤±è´¥è¿”å› None
        """
        rate_limit_count = 0  # é¢‘ç‡é™åˆ¶é‡è¯•è®¡æ•°
        MAX_RATE_LIMIT_RETRIES = 2  # é¢‘ç‡é™åˆ¶æœ€å¤§é‡è¯•æ¬¡æ•°
        
        for attempt in range(self.MAX_RETRIES):
            try:
                self._rate_limit()
                result = func(*args, **kwargs)
                if result is not None and not result.empty:
                    return result
                # ç©ºç»“æœä¹Ÿç®—æˆåŠŸï¼Œä¸éœ€è¦é‡è¯•
                if result is not None:
                    return result
            except Exception as e:
                error_msg = str(e)
                error_msg_lower = error_msg.lower()
                
                # æ£€æŸ¥æ˜¯å¦è§¦å‘é¢‘ç‡é™åˆ¶ï¼ˆå¤šç§é”™è¯¯æ ¼å¼ï¼‰
                rate_limit_keywords = ["æ¯åˆ†é’Ÿæœ€å¤šè®¿é—®", "æ¯å¤©æœ€å¤šè®¿é—®", "æ¯å°æ—¶æœ€å¤šè®¿é—®", 
                                       "æŠ±æ­‰", "é¢‘ç‡", "rate limit", "too many", "é™åˆ¶"]
                is_rate_limit = any(kw in error_msg or kw in error_msg_lower for kw in rate_limit_keywords)
                
                if is_rate_limit:
                    rate_limit_count += 1
                    if rate_limit_count > MAX_RATE_LIMIT_RETRIES:
                        logger.warning(
                            f"API é¢‘ç‡é™åˆ¶é‡è¯•æ¬¡æ•°è¶…é™ ({rate_limit_count} æ¬¡)ï¼Œè·³è¿‡è¯¥è¯·æ±‚ã€‚"
                            f"é”™è¯¯: {error_msg[:80]}"
                        )
                        return None
                    
                    # æ£€æŸ¥æ˜¯å¦ä¸ºæ¯å¤©/æ¯å°æ—¶é™åˆ¶ï¼ˆæ— æ³•é€šè¿‡ç­‰å¾…è§£å†³ï¼‰
                    if "æ¯å¤©æœ€å¤šè®¿é—®" in error_msg or "æ¯å°æ—¶æœ€å¤šè®¿é—®" in error_msg:
                        logger.warning(
                            f"è§¦å‘ Tushare æ¯æ—¥/æ¯å°æ—¶é…é¢é™åˆ¶ï¼Œæ— æ³•ç»§ç»­è¯·æ±‚ã€‚"
                            f"é”™è¯¯: {error_msg[:80]}"
                        )
                        return None
                    
                    logger.warning(
                        f"è§¦å‘ API é¢‘ç‡é™åˆ¶ (é‡è¯• {rate_limit_count}/{MAX_RATE_LIMIT_RETRIES})ï¼Œ"
                        f"ç­‰å¾… {self.RATE_LIMIT_DELAY} ç§’... é”™è¯¯: {error_msg[:80]}"
                    )
                    time.sleep(self.RATE_LIMIT_DELAY)
                    continue  # é¢‘ç‡é™åˆ¶é‡è¯•ä¸æ¶ˆè€— attempt æ¬¡æ•°
                
                # ç½‘ç»œè¶…æ—¶ï¼šä½¿ç”¨æŒ‡æ•°é€€é¿
                elif "timeout" in error_msg_lower or "timed out" in error_msg_lower:
                    wait_time = self.RETRY_DELAY * (2 ** attempt)  # æŒ‡æ•°é€€é¿: 2, 4, 8 ç§’
                    logger.warning(
                        f"ç½‘ç»œè¶…æ—¶ (å°è¯• {attempt + 1}/{self.MAX_RETRIES}), "
                        f"ç­‰å¾… {wait_time:.1f}s åé‡è¯•..."
                    )
                    if attempt < self.MAX_RETRIES - 1:
                        time.sleep(wait_time)
                # è¿æ¥é”™è¯¯ï¼šå¯èƒ½æ˜¯ç½‘ç»œé—®é¢˜
                elif "connection" in error_msg_lower or "connect" in error_msg_lower:
                    wait_time = self.RETRY_DELAY * (2 ** attempt)
                    logger.warning(
                        f"è¿æ¥å¤±è´¥ (å°è¯• {attempt + 1}/{self.MAX_RETRIES}): {e}, "
                        f"ç­‰å¾… {wait_time:.1f}s åé‡è¯•..."
                    )
                    if attempt < self.MAX_RETRIES - 1:
                        time.sleep(wait_time)
                else:
                    logger.warning(f"API è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{self.MAX_RETRIES}): {e}")
                    if attempt < self.MAX_RETRIES - 1:
                        time.sleep(self.RETRY_DELAY * (attempt + 1))
        return None
    
    # ==================== æŒ‡æ•°æˆåˆ†è‚¡ ====================
    
    def fetch_index_constituents(
        self,
        index_code: str = "hs300",
        trade_date: Optional[str] = None
    ) -> List[str]:
        """
        è·å–æŒ‡æ•°æˆåˆ†è‚¡åˆ—è¡¨
        
        Parameters
        ----------
        index_code : str
            æŒ‡æ•°ä»£ç ï¼Œæ”¯æŒ: hs300, zz500, zz1000, sz50, cyb
            æˆ–ç›´æ¥ä½¿ç”¨ Tushare ä»£ç å¦‚ "000300.SH"
        trade_date : Optional[str]
            äº¤æ˜“æ—¥æœŸï¼Œæ ¼å¼ YYYYMMDDï¼Œé»˜è®¤æœ€è¿‘äº¤æ˜“æ—¥
        
        Returns
        -------
        List[str]
            æˆåˆ†è‚¡ä»£ç åˆ—è¡¨ï¼ˆ6ä½ä»£ç ï¼Œå¦‚ "000001"ï¼‰
        
        Examples
        --------
        >>> loader = TushareDataLoaderBase()
        >>> stocks = loader.fetch_index_constituents("hs300")
        >>> print(len(stocks))  # çº¦ 300 åª
        """
        # è½¬æ¢æŒ‡æ•°ä»£ç 
        ts_index_code = self.INDEX_CODE_MAPPING.get(index_code.lower(), index_code)
        
        # é»˜è®¤ä½¿ç”¨æœ€è¿‘äº¤æ˜“æ—¥
        if trade_date is None:
            trade_date = (datetime.now() - timedelta(days=7)).strftime("%Y%m%d")
        
        logger.info(f"è·å–æŒ‡æ•°æˆåˆ†è‚¡: {ts_index_code}, æ—¥æœŸ: {trade_date}")
        
        # å°è¯•ç¼“å­˜
        cache_file = self.cache_dir / f"index_{index_code}_{trade_date[:6]}.parquet"
        if cache_file.exists():
            try:
                df = pd.read_parquet(cache_file)
                if not df.empty:
                    logger.info(f"ä»ç¼“å­˜åŠ è½½æŒ‡æ•°æˆåˆ†è‚¡: {len(df)} åª")
                    # è¿”å› 6 ä½ä»£ç 
                    return df["con_code"].str[:6].tolist()
            except Exception as e:
                logger.warning(f"ç¼“å­˜è¯»å–å¤±è´¥: {e}")
        
        # API è·å–
        df = self._fetch_with_retry(
            self.pro.index_weight,
            index_code=ts_index_code,
            start_date=trade_date,
            end_date=trade_date
        )
        
        if df is None or df.empty:
            # å°è¯•æœ€è¿‘ä¸€ä¸ªæœˆçš„æ•°æ®
            end_date = trade_date
            start_date = (datetime.strptime(trade_date, "%Y%m%d") - timedelta(days=30)).strftime("%Y%m%d")
            df = self._fetch_with_retry(
                self.pro.index_weight,
                index_code=ts_index_code,
                start_date=start_date,
                end_date=end_date
            )
        
        if df is None or df.empty:
            logger.warning(f"æ— æ³•è·å–æŒ‡æ•°æˆåˆ†è‚¡: {ts_index_code}")
            return []
        
        # å–æœ€æ–°æ—¥æœŸçš„æˆåˆ†è‚¡
        df = df.sort_values("trade_date", ascending=False)
        latest_date = df["trade_date"].iloc[0]
        df = df[df["trade_date"] == latest_date]
        
        # ä¿å­˜ç¼“å­˜
        try:
            df.to_parquet(cache_file, index=False)
            logger.info(f"æŒ‡æ•°æˆåˆ†è‚¡å·²ç¼“å­˜: {cache_file}")
        except Exception as e:
            logger.warning(f"ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
        
        # è¿”å› 6 ä½ä»£ç 
        stock_list = df["con_code"].str[:6].tolist()
        logger.info(f"è·å–åˆ° {len(stock_list)} åªæˆåˆ†è‚¡")
        return stock_list
    
    def fetch_all_stocks(
        self,
        exchange: Optional[str] = None,
        list_status: str = "L"
    ) -> List[str]:
        """
        è·å–å…¨å¸‚åœºè‚¡ç¥¨åˆ—è¡¨
        
        ä½¿ç”¨ Tushare stock_basic æ¥å£è·å–æ‰€æœ‰ä¸Šå¸‚è‚¡ç¥¨ã€‚
        
        Parameters
        ----------
        exchange : Optional[str]
            äº¤æ˜“æ‰€ç­›é€‰ï¼š
            - None: å…¨éƒ¨ï¼ˆé»˜è®¤ï¼‰
            - "SSE": ä¸Šäº¤æ‰€
            - "SZSE": æ·±äº¤æ‰€
        list_status : str
            ä¸Šå¸‚çŠ¶æ€ï¼š
            - "L": ä¸Šå¸‚ä¸­ï¼ˆé»˜è®¤ï¼‰
            - "D": é€€å¸‚
            - "P": æš‚åœä¸Šå¸‚
        
        Returns
        -------
        List[str]
            è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼ˆ6ä½ä»£ç ï¼‰
        
        Notes
        -----
        - é»˜è®¤åªè·å–ä¸Šå¸‚ä¸­çš„è‚¡ç¥¨
        - ä¼šè‡ªåŠ¨è¿‡æ»¤ STã€é€€å¸‚é£é™©è­¦ç¤ºè‚¡ç¥¨
        - ç»“æœä¼šç¼“å­˜åˆ°æœ¬åœ°ï¼ˆå½“æ—¥æœ‰æ•ˆï¼‰
        
        Examples
        --------
        >>> loader = TushareDataLoaderBase()
        >>> all_stocks = loader.fetch_all_stocks()
        >>> print(f"å…¨å¸‚åœºå…± {len(all_stocks)} åªè‚¡ç¥¨")
        """
        logger.info(f"ğŸ” è·å–å…¨å¸‚åœºè‚¡ç¥¨åˆ—è¡¨: exchange={exchange}, list_status={list_status}")
        
        # å°è¯•ä»Šæ—¥ç¼“å­˜
        today = datetime.now().strftime("%Y%m%d")
        cache_file = self.cache_dir / f"stock_basic_{today}.parquet"
        
        if cache_file.exists():
            try:
                df = pd.read_parquet(cache_file)
                # åº”ç”¨ç­›é€‰æ¡ä»¶
                if exchange:
                    df = df[df["exchange"] == exchange]
                stock_list = df["ts_code"].str[:6].tolist()
                logger.info(f"ä»ç¼“å­˜åŠ è½½å…¨å¸‚åœºè‚¡ç¥¨åˆ—è¡¨: {len(stock_list)} åª")
                return stock_list
            except Exception as e:
                logger.warning(f"è¯»å–ç¼“å­˜å¤±è´¥: {e}")
        
        # è°ƒç”¨ API è·å–è‚¡ç¥¨åŸºç¡€ä¿¡æ¯
        df = self._fetch_with_retry(
            self.pro.stock_basic,
            exchange=exchange or "",
            list_status=list_status,
            fields="ts_code,symbol,name,area,industry,market,list_date,exchange"
        )
        
        if df is None or df.empty:
            # ç½‘ç»œå¤±è´¥æ—¶ï¼Œå°è¯•ä½¿ç”¨æœ€è¿‘çš„ç¼“å­˜æ–‡ä»¶
            logger.warning("API è¯·æ±‚å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨å†å²ç¼“å­˜...")
            cache_files = sorted(
                self.cache_dir.glob("stock_basic_*.parquet"),
                reverse=True
            )
            for old_cache in cache_files[:5]:  # æœ€å¤šæ£€æŸ¥æœ€è¿‘5ä¸ªç¼“å­˜
                try:
                    df = pd.read_parquet(old_cache)
                    if not df.empty:
                        if exchange:
                            df = df[df["exchange"] == exchange]
                        stock_list = df["ts_code"].str[:6].tolist()
                        logger.info(
                            f"ä½¿ç”¨å†å²ç¼“å­˜ {old_cache.name}: {len(stock_list)} åªè‚¡ç¥¨"
                        )
                        return stock_list
                except Exception:
                    continue
            logger.warning("æ— å¯ç”¨ç¼“å­˜ï¼Œæ— æ³•è·å–å…¨å¸‚åœºè‚¡ç¥¨åˆ—è¡¨")
            return []
        
        # è¿‡æ»¤ ST å’Œé€€å¸‚é£é™©è‚¡ç¥¨
        if "name" in df.columns:
            st_mask = df["name"].str.contains(r"ST|\*ST|é€€|S\s|PT", na=False, regex=True)
            before_count = len(df)
            df = df[~st_mask]
            filtered_count = before_count - len(df)
            if filtered_count > 0:
                logger.info(f"è¿‡æ»¤ ST/é€€å¸‚é£é™©è‚¡ç¥¨: {filtered_count} åª")
        
        # ä¿å­˜ç¼“å­˜
        try:
            df.to_parquet(cache_file, index=False)
            logger.info(f"å…¨å¸‚åœºè‚¡ç¥¨åˆ—è¡¨å·²ç¼“å­˜: {cache_file}")
        except Exception as e:
            logger.warning(f"ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
        
        # è¿”å› 6 ä½ä»£ç 
        stock_list = df["ts_code"].str[:6].tolist()
        logger.info(f"è·å–å…¨å¸‚åœºè‚¡ç¥¨åˆ—è¡¨å®Œæˆ: {len(stock_list)} åª")
        return stock_list
    
    # ==================== æ—¥çº¿æ•°æ® ====================
    
    def fetch_daily_data(
        self,
        stock_code: str,
        start_date: str,
        end_date: str,
        adj: str = "qfq"
    ) -> Optional[pd.DataFrame]:
        """
        è·å–å•åªè‚¡ç¥¨æ—¥çº¿æ•°æ®
        
        Parameters
        ----------
        stock_code : str
            è‚¡ç¥¨ä»£ç ï¼ˆ6ä½ï¼Œå¦‚ "000001"ï¼‰
        start_date : str
            å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼ YYYYMMDD æˆ– YYYY-MM-DD
        end_date : str
            ç»“æŸæ—¥æœŸï¼Œæ ¼å¼ YYYYMMDD æˆ– YYYY-MM-DD
        adj : str
            å¤æƒæ–¹å¼: qfq(å‰å¤æƒ), hfq(åå¤æƒ), None(ä¸å¤æƒ)
        
        Returns
        -------
        Optional[pd.DataFrame]
            æ—¥çº¿æ•°æ®ï¼ŒåŒ…å« date, open, high, low, close, volume, amount ç­‰
        """
        # æ ‡å‡†åŒ–è‚¡ç¥¨ä»£ç 
        ts_code = self._to_ts_code(stock_code)
        
        # æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼
        start_date = start_date.replace("-", "")
        end_date = end_date.replace("-", "")
        
        # å°è¯•ç¼“å­˜
        cache_file = self.cache_dir / f"daily_{stock_code}_{start_date}_{end_date}.parquet"
        if cache_file.exists():
            try:
                df = pd.read_parquet(cache_file)
                if not df.empty:
                    logger.debug(f"ä»ç¼“å­˜åŠ è½½æ—¥çº¿æ•°æ®: {stock_code}")
                    return self._standardize_daily_columns(df)
            except Exception:
                pass
        
        # API è·å–
        df = self._fetch_with_retry(
            self.pro.daily,
            ts_code=ts_code,
            start_date=start_date,
            end_date=end_date
        )
        
        if df is None or df.empty:
            logger.debug(f"è·å–æ—¥çº¿æ•°æ®å¤±è´¥: {stock_code}")
            return None
        
        # å‰å¤æƒå¤„ç†
        if adj == "qfq":
            adj_factor = self._fetch_with_retry(
                self.pro.adj_factor,
                ts_code=ts_code,
                start_date=start_date,
                end_date=end_date
            )
            if adj_factor is not None and not adj_factor.empty:
                df = df.merge(adj_factor[["trade_date", "adj_factor"]], on="trade_date", how="left")
                df["adj_factor"] = df["adj_factor"].fillna(1.0)
                latest_factor = df["adj_factor"].iloc[0]
                factor = df["adj_factor"] / latest_factor
                for col in ["open", "high", "low", "close"]:
                    if col in df.columns:
                        df[col] = df[col] * factor
        
        # ä¿å­˜ç¼“å­˜
        try:
            df.to_parquet(cache_file, index=False)
        except Exception:
            pass
        
        return self._standardize_daily_columns(df)
    
    def fetch_daily_data_batch(
        self,
        stock_list: List[str],
        start_date: str,
        end_date: str,
        adj: str = "qfq",
        show_progress: bool = True,
        batch_size: int = 200,
        batch_sleep: float = 5.0
    ) -> pd.DataFrame:
        """
        æ‰¹é‡è·å–æ—¥çº¿æ•°æ®ï¼ˆå¸¦é™æµä¿æŠ¤ï¼‰
        
        Parameters
        ----------
        stock_list : List[str]
            è‚¡ç¥¨ä»£ç åˆ—è¡¨
        start_date : str
            å¼€å§‹æ—¥æœŸ
        end_date : str
            ç»“æŸæ—¥æœŸ
        adj : str
            å¤æƒæ–¹å¼
        show_progress : bool
            æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
        batch_size : int
            æ¯æ‰¹æ¬¡å¤„ç†çš„è‚¡ç¥¨æ•°é‡ï¼ˆé»˜è®¤ 200ï¼‰
        batch_sleep : float
            æ¯æ‰¹æ¬¡ä¹‹é—´çš„ä¼‘æ¯æ—¶é—´ï¼ˆç§’ï¼‰
        
        Returns
        -------
        pd.DataFrame
            åˆå¹¶åçš„æ—¥çº¿æ•°æ®
        """
        all_data = []
        total = len(stock_list)
        success_count = 0
        
        # ä½¿ç”¨ tqdm è¿›åº¦æ¡
        if show_progress:
            try:
                from tqdm import tqdm
                iterator = tqdm(
                    enumerate(stock_list), 
                    total=total, 
                    desc="ğŸ“Š è·å–æ—¥çº¿æ•°æ®",
                    unit="åª",
                    ncols=80
                )
            except ImportError:
                iterator = enumerate(stock_list)
                logger.info(f"å¼€å§‹è·å–æ—¥çº¿æ•°æ®: {total} åªè‚¡ç¥¨...")
        else:
            iterator = enumerate(stock_list)
        
        for i, stock in iterator:
            df = self.fetch_daily_data(stock, start_date, end_date, adj)
            if df is not None and not df.empty:
                df["stock_code"] = stock
                all_data.append(df)
                success_count += 1
            
            # æ›´æ–°è¿›åº¦æ¡åç¼€
            if show_progress and hasattr(iterator, 'set_postfix'):
                iterator.set_postfix({"æˆåŠŸ": success_count, "å½“å‰": stock})
            
            # æ‰¹æ¬¡ä¼‘æ¯ï¼ˆé¿å…è§¦å‘é¢‘ç‡é™åˆ¶ï¼‰
            if (i + 1) % batch_size == 0 and (i + 1) < total:
                if show_progress and hasattr(iterator, 'set_description'):
                    iterator.set_description(f"ğŸ“Š ä¼‘æ¯{batch_sleep}s")
                time.sleep(batch_sleep)
                if show_progress and hasattr(iterator, 'set_description'):
                    iterator.set_description("ğŸ“Š è·å–æ—¥çº¿æ•°æ®")
        
        if not all_data:
            return pd.DataFrame()
        
        result = pd.concat(all_data, ignore_index=True)
        logger.info(f"æ‰¹é‡è·å–æ—¥çº¿æ•°æ®å®Œæˆ: {success_count}/{total} åªè‚¡ç¥¨æˆåŠŸ, {len(result)} æ¡è®°å½•")
        return result
    
    # ==================== è´¢åŠ¡æŒ‡æ ‡ ====================
    
    def fetch_financial_indicator(
        self,
        stock_code: str,
        period: Optional[str] = None
    ) -> Optional[pd.DataFrame]:
        """
        è·å–å•åªè‚¡ç¥¨è´¢åŠ¡æŒ‡æ ‡
        
        Parameters
        ----------
        stock_code : str
            è‚¡ç¥¨ä»£ç ï¼ˆ6ä½ï¼‰
        period : Optional[str]
            æŠ¥å‘ŠæœŸï¼Œæ ¼å¼ YYYYMMDDï¼Œå¦‚ "20231231"
            å¦‚æœä¸æä¾›ï¼Œè¿”å›æœ€è¿‘ 8 ä¸ªå­£åº¦çš„æ•°æ®
        
        Returns
        -------
        Optional[pd.DataFrame]
            è´¢åŠ¡æŒ‡æ ‡æ•°æ®ï¼ŒåŒ…å«ï¼š
            - roe: å‡€èµ„äº§æ”¶ç›Šç‡
            - roe_dt: æ‰£éå‡€èµ„äº§æ”¶ç›Šç‡
            - roa: æ€»èµ„äº§æ”¶ç›Šç‡
            - gross_margin: æ¯›åˆ©ç‡
            - profit_to_gr: å‡€åˆ©ç‡
            - eps: æ¯è‚¡æ”¶ç›Š
            - bps: æ¯è‚¡å‡€èµ„äº§
        """
        ts_code = self._to_ts_code(stock_code)
        
        # å°è¯•ç¼“å­˜
        cache_file = self.cache_dir / f"fina_{stock_code}.parquet"
        cache_valid = False
        
        if cache_file.exists():
            try:
                cache_mtime = datetime.fromtimestamp(cache_file.stat().st_mtime)
                # ç¼“å­˜æœ‰æ•ˆæœŸ 7 å¤©
                if (datetime.now() - cache_mtime).days < 7:
                    df = pd.read_parquet(cache_file)
                    if not df.empty:
                        logger.debug(f"ä»ç¼“å­˜åŠ è½½è´¢åŠ¡æŒ‡æ ‡: {stock_code}")
                        cache_valid = True
                        return self._standardize_financial_columns(df)
            except Exception:
                pass
        
        # API è·å–
        df = self._fetch_with_retry(
            self.pro.fina_indicator,
            ts_code=ts_code,
            period=period
        )
        
        if df is None or df.empty:
            logger.debug(f"è·å–è´¢åŠ¡æŒ‡æ ‡å¤±è´¥: {stock_code}")
            return None
        
        # ä¿å­˜ç¼“å­˜
        try:
            df.to_parquet(cache_file, index=False)
        except Exception:
            pass
        
        return self._standardize_financial_columns(df)
    
    def fetch_daily_basic(
        self,
        trade_date: Optional[str] = None,
        stock_list: Optional[List[str]] = None
    ) -> Optional[pd.DataFrame]:
        """
        è·å–æ¯æ—¥åŸºç¡€æŒ‡æ ‡ï¼ˆPE, PB, å¸‚å€¼ç­‰ï¼‰
        
        è¿™æ˜¯è·å–ä¼°å€¼æ•°æ®æœ€é«˜æ•ˆçš„æ–¹å¼ï¼Œä¸€æ¬¡è¯·æ±‚è·å–å…¨å¸‚åœºæ•°æ®ã€‚
        
        Parameters
        ----------
        trade_date : Optional[str]
            äº¤æ˜“æ—¥æœŸï¼Œæ ¼å¼ YYYYMMDDï¼Œé»˜è®¤æœ€è¿‘äº¤æ˜“æ—¥
        stock_list : Optional[List[str]]
            è‚¡ç¥¨åˆ—è¡¨ï¼Œç”¨äºè¿‡æ»¤ç»“æœ
        
        Returns
        -------
        Optional[pd.DataFrame]
            åŸºç¡€æŒ‡æ ‡æ•°æ®ï¼ŒåŒ…å«ï¼š
            - pe_ttm: å¸‚ç›ˆç‡ TTM
            - pb: å¸‚å‡€ç‡
            - ps_ttm: å¸‚é”€ç‡ TTM
            - dv_ttm: è‚¡æ¯ç‡ TTM
            - total_mv: æ€»å¸‚å€¼ï¼ˆä¸‡å…ƒï¼‰
            - circ_mv: æµé€šå¸‚å€¼ï¼ˆä¸‡å…ƒï¼‰
            - turnover_rate: æ¢æ‰‹ç‡
        """
        if trade_date is None:
            trade_date = (datetime.now() - timedelta(days=1)).strftime("%Y%m%d")
        
        # å°è¯•ç¼“å­˜
        cache_file = self.cache_dir / f"daily_basic_{trade_date}.parquet"
        
        if cache_file.exists():
            try:
                df = pd.read_parquet(cache_file)
                if not df.empty:
                    logger.info(f"ä»ç¼“å­˜åŠ è½½æ¯æ—¥åŸºç¡€æŒ‡æ ‡: {trade_date}, {len(df)} æ¡")
                    if stock_list:
                        df = df[df["ts_code"].str[:6].isin(stock_list)]
                    return self._standardize_basic_columns(df)
            except Exception as e:
                logger.warning(f"ç¼“å­˜è¯»å–å¤±è´¥: {e}")
        
        # API è·å–
        df = self._fetch_with_retry(
            self.pro.daily_basic,
            trade_date=trade_date
        )
        
        if df is None or df.empty:
            # å°è¯•å‰å‡ å¤©
            for days_ago in range(1, 8):
                alt_date = (datetime.strptime(trade_date, "%Y%m%d") - timedelta(days=days_ago)).strftime("%Y%m%d")
                df = self._fetch_with_retry(
                    self.pro.daily_basic,
                    trade_date=alt_date
                )
                if df is not None and not df.empty:
                    logger.info(f"ä½¿ç”¨ {alt_date} çš„åŸºç¡€æŒ‡æ ‡æ•°æ®")
                    break
        
        if df is None or df.empty:
            logger.warning(f"æ— æ³•è·å–æ¯æ—¥åŸºç¡€æŒ‡æ ‡: {trade_date}")
            return None
        
        # ä¿å­˜ç¼“å­˜
        try:
            df.to_parquet(cache_file, index=False)
            logger.info(f"æ¯æ—¥åŸºç¡€æŒ‡æ ‡å·²ç¼“å­˜: {cache_file}")
        except Exception as e:
            logger.warning(f"ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
        
        if stock_list:
            df = df[df["ts_code"].str[:6].isin(stock_list)]
        
        return self._standardize_basic_columns(df)
    
    def fetch_financial_batch(
        self,
        stock_list: List[str],
        show_progress: bool = True,
        batch_size: int = 150,
        batch_sleep: float = 8.0
    ) -> pd.DataFrame:
        """
        æ‰¹é‡è·å–è´¢åŠ¡æŒ‡æ ‡ï¼ˆå¸¦é™æµä¿æŠ¤ï¼‰
        
        Parameters
        ----------
        stock_list : List[str]
            è‚¡ç¥¨ä»£ç åˆ—è¡¨
        show_progress : bool
            æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
        batch_size : int
            æ¯æ‰¹æ¬¡å¤„ç†çš„è‚¡ç¥¨æ•°é‡ï¼ˆé»˜è®¤ 150ï¼‰
        batch_sleep : float
            æ¯æ‰¹æ¬¡ä¹‹é—´çš„ä¼‘æ¯æ—¶é—´ï¼ˆç§’ï¼‰
        
        Returns
        -------
        pd.DataFrame
            åˆå¹¶åçš„è´¢åŠ¡æŒ‡æ ‡æ•°æ®
        """
        all_data = []
        total = len(stock_list)
        success_count = 0
        
        # ä½¿ç”¨ tqdm è¿›åº¦æ¡
        if show_progress:
            try:
                from tqdm import tqdm
                iterator = tqdm(
                    enumerate(stock_list), 
                    total=total, 
                    desc="ğŸ“ˆ è·å–è´¢åŠ¡æŒ‡æ ‡",
                    unit="åª",
                    ncols=80
                )
            except ImportError:
                iterator = enumerate(stock_list)
                logger.info(f"å¼€å§‹è·å–è´¢åŠ¡æŒ‡æ ‡: {total} åªè‚¡ç¥¨...")
        else:
            iterator = enumerate(stock_list)
        
        for i, stock in iterator:
            df = self.fetch_financial_indicator(stock)
            if df is not None and not df.empty:
                # åªå–æœ€æ–°ä¸€æœŸ
                df = df.sort_values("end_date", ascending=False).head(1)
                df["stock_code"] = stock
                all_data.append(df)
                success_count += 1
            
            # æ›´æ–°è¿›åº¦æ¡åç¼€
            if show_progress and hasattr(iterator, 'set_postfix'):
                iterator.set_postfix({"æˆåŠŸ": success_count, "å½“å‰": stock})
            
            # æ‰¹æ¬¡ä¼‘æ¯ï¼ˆé¿å…è§¦å‘é¢‘ç‡é™åˆ¶ï¼‰
            if (i + 1) % batch_size == 0 and (i + 1) < total:
                if show_progress and hasattr(iterator, 'set_description'):
                    iterator.set_description(f"ğŸ“ˆ ä¼‘æ¯{batch_sleep}s")
                time.sleep(batch_sleep)
                if show_progress and hasattr(iterator, 'set_description'):
                    iterator.set_description("ğŸ“ˆ è·å–è´¢åŠ¡æŒ‡æ ‡")
        
        if not all_data:
            return pd.DataFrame()
        
        # è¿‡æ»¤æ‰ç©ºçš„ DataFrameï¼Œé¿å… FutureWarning
        valid_data = []
        for df in all_data:
            if df is None or df.empty:
                continue
            # åˆ é™¤å…¨ä¸º NaN çš„åˆ—
            df_cleaned = df.dropna(axis=1, how='all')
            if not df_cleaned.empty and not df_cleaned.isna().all().all():
                valid_data.append(df_cleaned)
        
        if not valid_data:
            return pd.DataFrame()
        
        result = pd.concat(valid_data, ignore_index=True)
        logger.info(f"æ‰¹é‡è·å–è´¢åŠ¡æŒ‡æ ‡å®Œæˆ: {success_count}/{total} åªè‚¡ç¥¨æˆåŠŸ, {len(result)} æ¡è®°å½•")
        return result
    
    # ==================== æŒ‡æ•°æ—¥çº¿ ====================
    
    def fetch_index_daily(
        self,
        index_code: str,
        start_date: str,
        end_date: str
    ) -> Optional[pd.DataFrame]:
        """
        è·å–æŒ‡æ•°æ—¥çº¿æ•°æ®
        
        Parameters
        ----------
        index_code : str
            æŒ‡æ•°ä»£ç ï¼Œå¦‚ "000300" æˆ– "hs300"
        start_date : str
            å¼€å§‹æ—¥æœŸ
        end_date : str
            ç»“æŸæ—¥æœŸ
        
        Returns
        -------
        Optional[pd.DataFrame]
            æŒ‡æ•°æ—¥çº¿æ•°æ®
        """
        # è½¬æ¢æŒ‡æ•°ä»£ç 
        if index_code.lower() in self.INDEX_CODE_MAPPING:
            ts_code = self.INDEX_CODE_MAPPING[index_code.lower()]
        elif "." in index_code:
            ts_code = index_code
        else:
            # å‡è®¾æ˜¯ä¸Šè¯æŒ‡æ•°
            ts_code = f"{index_code}.SH"
        
        start_date = start_date.replace("-", "")
        end_date = end_date.replace("-", "")
        
        df = self._fetch_with_retry(
            self.pro.index_daily,
            ts_code=ts_code,
            start_date=start_date,
            end_date=end_date
        )
        
        if df is None or df.empty:
            return None
        
        return self._standardize_daily_columns(df)
    
    # ==================== äº¤æ˜“æ—¥å† ====================
    
    def fetch_trade_calendar(
        self,
        start_date: str,
        end_date: str,
        exchange: str = "SSE"
    ) -> pd.DatetimeIndex:
        """
        è·å–äº¤æ˜“æ—¥å†
        
        Parameters
        ----------
        start_date : str
            å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼ YYYY-MM-DD æˆ– YYYYMMDD
        end_date : str
            ç»“æŸæ—¥æœŸï¼Œæ ¼å¼ YYYY-MM-DD æˆ– YYYYMMDD
        exchange : str
            äº¤æ˜“æ‰€ï¼ŒSSE(ä¸Šäº¤æ‰€ï¼Œé»˜è®¤) æˆ– SZSE(æ·±äº¤æ‰€)
        
        Returns
        -------
        pd.DatetimeIndex
            äº¤æ˜“æ—¥æœŸç´¢å¼•
        
        Examples
        --------
        >>> loader = TushareDataLoaderBase()
        >>> calendar = loader.fetch_trade_calendar("2024-01-01", "2024-12-31")
        >>> print(f"2024å¹´å…± {len(calendar)} ä¸ªäº¤æ˜“æ—¥")
        """
        # æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼
        start_date = start_date.replace("-", "")
        end_date = end_date.replace("-", "")
        
        logger.info(f"è·å–äº¤æ˜“æ—¥å†: {start_date} ~ {end_date}")
        
        # å°è¯•ç¼“å­˜
        cache_file = self.cache_dir / f"trade_cal_{start_date[:4]}.parquet"
        
        if cache_file.exists():
            try:
                df = pd.read_parquet(cache_file)
                # è¿‡æ»¤æ—¥æœŸèŒƒå›´
                df = df[
                    (df["cal_date"] >= start_date) & 
                    (df["cal_date"] <= end_date) &
                    (df["is_open"] == 1)
                ]
                if not df.empty:
                    calendar = pd.to_datetime(df["cal_date"])
                    logger.debug(f"ä»ç¼“å­˜åŠ è½½äº¤æ˜“æ—¥å†: {len(calendar)} å¤©")
                    return pd.DatetimeIndex(sorted(calendar))
            except Exception:
                pass
        
        # API è·å–
        df = self._fetch_with_retry(
            self.pro.trade_cal,
            exchange=exchange,
            start_date=start_date,
            end_date=end_date
        )
        
        if df is None or df.empty:
            logger.warning("æ— æ³•è·å–äº¤æ˜“æ—¥å†ï¼Œä½¿ç”¨å·¥ä½œæ—¥è¿‘ä¼¼")
            return pd.bdate_range(start=start_date, end=end_date)
        
        # ä¿å­˜ç¼“å­˜ï¼ˆæ•´å¹´æ•°æ®ï¼‰
        try:
            full_year_df = self._fetch_with_retry(
                self.pro.trade_cal,
                exchange=exchange,
                start_date=f"{start_date[:4]}0101",
                end_date=f"{start_date[:4]}1231"
            )
            if full_year_df is not None and not full_year_df.empty:
                full_year_df.to_parquet(cache_file, index=False)
        except Exception:
            pass
        
        # è¿‡æ»¤äº¤æ˜“æ—¥
        trade_days = df[df["is_open"] == 1]["cal_date"]
        calendar = pd.to_datetime(trade_days)
        calendar = pd.DatetimeIndex(sorted(calendar))
        calendar.name = "date"
        
        logger.info(f"è·å–äº¤æ˜“æ—¥å†æˆåŠŸ: {len(calendar)} ä¸ªäº¤æ˜“æ—¥")
        return calendar
    
    def is_trade_day(self, date: Optional[str] = None) -> bool:
        """
        åˆ¤æ–­æŒ‡å®šæ—¥æœŸæ˜¯å¦ä¸ºäº¤æ˜“æ—¥
        
        Parameters
        ----------
        date : Optional[str]
            æ—¥æœŸï¼Œæ ¼å¼ YYYY-MM-DD æˆ– YYYYMMDDï¼Œé»˜è®¤ä»Šå¤©
        
        Returns
        -------
        bool
            æ˜¯å¦ä¸ºäº¤æ˜“æ—¥
        """
        if date is None:
            date = datetime.now().strftime("%Y%m%d")
        else:
            date = date.replace("-", "")
        
        calendar = self.fetch_trade_calendar(date, date)
        return len(calendar) > 0
    
    # ==================== è¡Œä¸šåˆ†ç±» ====================
    
    def fetch_industry_mapping(
        self,
        use_cache: bool = True
    ) -> Dict[str, str]:
        """
        è·å–è‚¡ç¥¨è¡Œä¸šåˆ†ç±»æ˜ å°„
        
        è¿”å›è‚¡ç¥¨ä»£ç åˆ°è¡Œä¸šåç§°çš„æ˜ å°„å­—å…¸ã€‚
        
        Parameters
        ----------
        use_cache : bool
            æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼Œé»˜è®¤ True
        
        Returns
        -------
        Dict[str, str]
            è‚¡ç¥¨ä»£ç ï¼ˆ6ä½ï¼‰åˆ°è¡Œä¸šåç§°çš„æ˜ å°„
        
        Examples
        --------
        >>> loader = TushareDataLoaderBase()
        >>> industry_map = loader.fetch_industry_mapping()
        >>> print(industry_map.get("000001"))  # é“¶è¡Œ
        """
        logger.info("è·å–è‚¡ç¥¨è¡Œä¸šåˆ†ç±»æ˜ å°„")
        
        # å°è¯•ç¼“å­˜
        today = datetime.now().strftime("%Y%m%d")
        cache_file = self.cache_dir / f"industry_mapping_{today[:6]}.parquet"
        
        if use_cache and cache_file.exists():
            try:
                df = pd.read_parquet(cache_file)
                mapping = dict(zip(df["stock_code"], df["industry"]))
                logger.info(f"ä»ç¼“å­˜åŠ è½½è¡Œä¸šæ˜ å°„: {len(mapping)} åªè‚¡ç¥¨")
                return mapping
            except Exception as e:
                logger.warning(f"ç¼“å­˜è¯»å–å¤±è´¥: {e}")
        
        # API è·å–
        df = self._fetch_with_retry(
            self.pro.stock_basic,
            list_status="L",
            fields="ts_code,symbol,name,industry,market,list_date"
        )
        
        if df is None or df.empty:
            logger.warning("æ— æ³•è·å–è¡Œä¸šåˆ†ç±»æ•°æ®")
            return {}
        
        # æå– 6 ä½è‚¡ç¥¨ä»£ç 
        df["stock_code"] = df["ts_code"].str[:6]
        
        # ä¿å­˜ç¼“å­˜
        try:
            df[["stock_code", "industry"]].to_parquet(cache_file, index=False)
            logger.info(f"è¡Œä¸šæ˜ å°„å·²ç¼“å­˜: {cache_file}")
        except Exception as e:
            logger.warning(f"ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
        
        # æ„å»ºæ˜ å°„
        mapping = dict(zip(df["stock_code"], df["industry"]))
        logger.info(f"è·å–è¡Œä¸šæ˜ å°„æˆåŠŸ: {len(mapping)} åªè‚¡ç¥¨")
        return mapping
    
    def fetch_sw_industry_mapping(
        self,
        level: int = 1
    ) -> Dict[str, str]:
        """
        è·å–ç”³ä¸‡è¡Œä¸šåˆ†ç±»æ˜ å°„
        
        Parameters
        ----------
        level : int
            è¡Œä¸šåˆ†ç±»çº§åˆ«ï¼š1(ä¸€çº§), 2(äºŒçº§), 3(ä¸‰çº§)
            é»˜è®¤ 1ï¼ˆä¸€çº§è¡Œä¸šï¼‰
        
        Returns
        -------
        Dict[str, str]
            è‚¡ç¥¨ä»£ç ï¼ˆ6ä½ï¼‰åˆ°ç”³ä¸‡è¡Œä¸šåç§°çš„æ˜ å°„
        """
        logger.info(f"è·å–ç”³ä¸‡{level}çº§è¡Œä¸šåˆ†ç±»")
        
        # å°è¯•ç¼“å­˜
        today = datetime.now().strftime("%Y%m%d")
        cache_file = self.cache_dir / f"sw_industry_L{level}_{today[:6]}.parquet"
        
        if cache_file.exists():
            try:
                df = pd.read_parquet(cache_file)
                mapping = dict(zip(df["stock_code"], df["industry"]))
                logger.info(f"ä»ç¼“å­˜åŠ è½½ç”³ä¸‡åˆ†ç±»: {len(mapping)} åªè‚¡ç¥¨")
                return mapping
            except Exception as e:
                logger.warning(f"ç¼“å­˜è¯»å–å¤±è´¥: {e}")
        
        # Tushare çš„ç”³ä¸‡åˆ†ç±»æ¥å£
        try:
            # æ–¹å¼1: index_classify è·å–ç”³ä¸‡æŒ‡æ•°æˆåˆ†
            # è·å–ç”³ä¸‡è¡Œä¸šæŒ‡æ•°åˆ—è¡¨
            index_list = self._fetch_with_retry(
                self.pro.index_classify,
                index_code="",
                level=f"L{level}",
                src="SW2021"
            )
            
            if index_list is not None and not index_list.empty:
                # éå†æ¯ä¸ªè¡Œä¸šï¼Œè·å–æˆåˆ†è‚¡
                all_mappings = []
                
                for _, row in index_list.iterrows():
                    industry_code = row.get("index_code", "")
                    industry_name = row.get("industry_name", row.get("name", ""))
                    
                    if not industry_code:
                        continue
                    
                    # è·å–è¯¥è¡Œä¸šæˆåˆ†è‚¡
                    members = self._fetch_with_retry(
                        self.pro.index_member,
                        index_code=industry_code
                    )
                    
                    if members is not None and not members.empty:
                        for _, m in members.iterrows():
                            stock_code = m.get("con_code", "")[:6]
                            all_mappings.append({
                                "stock_code": stock_code,
                                "industry": industry_name
                            })
                
                if all_mappings:
                    df = pd.DataFrame(all_mappings)
                    # å»é‡ï¼Œä¿ç•™ç¬¬ä¸€ä¸ª
                    df = df.drop_duplicates(subset=["stock_code"], keep="first")
                    
                    # ä¿å­˜ç¼“å­˜
                    try:
                        df.to_parquet(cache_file, index=False)
                        logger.info(f"ç”³ä¸‡åˆ†ç±»å·²ç¼“å­˜: {cache_file}")
                    except Exception:
                        pass
                    
                    result = dict(zip(df["stock_code"], df["industry"]))
                    logger.info(f"è·å–ç”³ä¸‡åˆ†ç±»æˆåŠŸ: {len(result)} åªè‚¡ç¥¨")
                    return result
                    
        except Exception as e:
            logger.debug(f"ç”³ä¸‡åˆ†ç±»æ¥å£ä¸å¯ç”¨: {e}")
        
        # é™çº§åˆ°æ™®é€šè¡Œä¸šåˆ†ç±»
        logger.info("ä½¿ç”¨æ™®é€šè¡Œä¸šåˆ†ç±»æ›¿ä»£ç”³ä¸‡åˆ†ç±»")
        return self.fetch_industry_mapping()
    
    # ==================== è¾…åŠ©æ–¹æ³• ====================
    
    def _to_ts_code(self, stock_code: str) -> str:
        """
        è½¬æ¢è‚¡ç¥¨ä»£ç ä¸º Tushare æ ¼å¼
        
        Parameters
        ----------
        stock_code : str
            6ä½è‚¡ç¥¨ä»£ç 
        
        Returns
        -------
        str
            Tushare æ ¼å¼ä»£ç ï¼Œå¦‚ "000001.SZ"
        """
        if "." in stock_code:
            return stock_code
        
        code = stock_code.strip()
        
        # æ ¹æ®é¦–ä½åˆ¤æ–­äº¤æ˜“æ‰€
        if code.startswith(("6", "5")):
            return f"{code}.SH"
        elif code.startswith(("0", "3", "2")):
            return f"{code}.SZ"
        elif code.startswith("8") or code.startswith("4"):
            return f"{code}.BJ"  # åŒ—äº¤æ‰€
        else:
            return f"{code}.SZ"
    
    def _standardize_daily_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ ‡å‡†åŒ–æ—¥çº¿æ•°æ®åˆ—å"""
        column_mapping = {
            "trade_date": "date",
            "ts_code": "ts_code",
            "open": "open",
            "high": "high",
            "low": "low",
            "close": "close",
            "vol": "volume",
            "amount": "amount",
            "pct_chg": "pct_change",
            "change": "change",
        }
        
        df = df.rename(columns=column_mapping)
        
        if "date" in df.columns:
            df["date"] = pd.to_datetime(df["date"])
            df = df.sort_values("date")
        
        # æˆäº¤é‡å•ä½è½¬æ¢ï¼ˆTushare å•ä½æ˜¯æ‰‹ï¼Œè½¬ä¸ºè‚¡ï¼‰
        if "volume" in df.columns:
            df["volume"] = df["volume"] * 100
        
        # æˆäº¤é¢å•ä½è½¬æ¢ï¼ˆTushare å•ä½æ˜¯åƒå…ƒï¼Œè½¬ä¸ºå…ƒï¼‰
        if "amount" in df.columns:
            df["amount"] = df["amount"] * 1000
        
        return df
    
    def _standardize_basic_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ ‡å‡†åŒ–æ¯æ—¥åŸºç¡€æŒ‡æ ‡åˆ—å"""
        column_mapping = {
            "trade_date": "date",
            "ts_code": "ts_code",
            "pe_ttm": "pe_ttm",
            "pe": "pe",
            "pb": "pb",
            "ps_ttm": "ps_ttm",
            "dv_ttm": "dividend_yield",
            "dv_ratio": "dividend_yield",
            "total_mv": "total_mv",
            "circ_mv": "circ_mv",
            "turnover_rate": "turn",
            "turnover_rate_f": "turn_free",
        }
        
        df = df.rename(columns=column_mapping)
        
        # æå– 6 ä½è‚¡ç¥¨ä»£ç 
        if "ts_code" in df.columns:
            df["stock_code"] = df["ts_code"].str[:6]
        
        if "date" in df.columns:
            df["date"] = pd.to_datetime(df["date"])
        
        # å¸‚å€¼å•ä½è½¬æ¢ï¼ˆä¸‡å…ƒ -> å…ƒï¼‰
        for col in ["total_mv", "circ_mv"]:
            if col in df.columns:
                df[col] = df[col] * 10000
        
        return df
    
    def _standardize_financial_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ ‡å‡†åŒ–è´¢åŠ¡æŒ‡æ ‡åˆ—å"""
        column_mapping = {
            "ts_code": "ts_code",
            "ann_date": "ann_date",
            "end_date": "end_date",
            "roe": "roe",
            "roe_dt": "roe_dt",
            "roe_yearly": "roe_ttm",
            "roa": "roa",
            "grossprofit_margin": "gross_margin",
            "profit_to_gr": "net_margin",
            "eps": "eps",
            "bps": "bps",
            "netprofit_margin": "net_margin",
            "current_ratio": "current_ratio",
            "quick_ratio": "quick_ratio",
        }
        
        df = df.rename(columns=column_mapping)
        
        # æå– 6 ä½è‚¡ç¥¨ä»£ç 
        if "ts_code" in df.columns:
            df["stock_code"] = df["ts_code"].str[:6]
        
        return df
    
    # ==================== æ•°æ®å•ä½éªŒè¯å·¥å…· ====================
    
    @staticmethod
    def validate_data_units(
        df: pd.DataFrame,
        check_columns: Optional[List[str]] = None,
        expected_units: Optional[Dict[str, str]] = None
    ) -> Dict[str, Dict[str, Any]]:
        """
        éªŒè¯æ•°æ®å•ä½ä¸€è‡´æ€§
        
        æ£€æŸ¥å…³é”®é‡‘é¢å­—æ®µçš„æ•°é‡çº§æ˜¯å¦åˆç†ï¼Œå¸®åŠ©å‘ç°å•ä½ä¸ä¸€è‡´çš„é—®é¢˜ã€‚
        
        Parameters
        ----------
        df : pd.DataFrame
            å¾…éªŒè¯çš„æ•°æ®
        check_columns : Optional[List[str]]
            è¦æ£€æŸ¥çš„åˆ—ååˆ—è¡¨ï¼Œé»˜è®¤æ£€æŸ¥å¸¸è§é‡‘é¢åˆ—
        expected_units : Optional[Dict[str, str]]
            åˆ—ååˆ°é¢„æœŸå•ä½çš„æ˜ å°„
        
        Returns
        -------
        Dict[str, Dict[str, Any]]
            æ¯åˆ—çš„éªŒè¯ç»“æœ
        """
        if check_columns is None:
            # é»˜è®¤æ£€æŸ¥çš„å¸¸è§é‡‘é¢åˆ—
            check_columns = [
                'amount', 'volume', 'total_mv', 'circ_mv',
                'buy_elg_amount', 'sell_elg_amount', 'buy_lg_amount', 'sell_lg_amount',
                'net_mf_amount', 'fd_amount', 'rzye', 'rzmre', 'rqye'
            ]
        
        # Tushare æ ‡å‡†å•ä½å‚è€ƒ
        default_units = {
            'amount': 'åƒå…ƒ(daily)/ä¸‡å…ƒ(moneyflow)',
            'volume': 'æ‰‹',
            'total_mv': 'ä¸‡å…ƒ',
            'circ_mv': 'ä¸‡å…ƒ',
            'buy_elg_amount': 'ä¸‡å…ƒ',
            'sell_elg_amount': 'ä¸‡å…ƒ',
            'buy_lg_amount': 'ä¸‡å…ƒ',
            'sell_lg_amount': 'ä¸‡å…ƒ',
            'net_mf_amount': 'ä¸‡å…ƒ',
            'fd_amount': 'ä¸‡å…ƒ',
            'rzye': 'å…ƒ',
            'rzmre': 'å…ƒ',
            'rqye': 'å…ƒ',
        }
        
        if expected_units:
            default_units.update(expected_units)
        
        results = {}
        
        for col in check_columns:
            if col not in df.columns:
                continue
            
            series = df[col].dropna()
            if len(series) == 0:
                results[col] = {"warning": "åˆ—ä¸ºç©º"}
                continue
            
            min_val = series.min()
            max_val = series.max()
            mean_val = series.mean()
            
            # è®¡ç®—æ•°é‡çº§
            if mean_val > 0:
                magnitude = int(np.log10(abs(mean_val)))
            else:
                magnitude = 0
            
            # æ¨æµ‹å•ä½
            likely_unit = "æœªçŸ¥"
            warning = None
            
            if col in ['amount']:
                if magnitude >= 9:
                    likely_unit = "å…ƒ"
                elif magnitude >= 6:
                    likely_unit = "åƒå…ƒ"
                elif magnitude >= 3:
                    likely_unit = "ä¸‡å…ƒ"
                else:
                    likely_unit = "å¯èƒ½æœ‰é—®é¢˜"
                    warning = f"æˆäº¤é¢æ•°é‡çº§å¼‚å¸¸: 10^{magnitude}"
                    
            elif col in ['total_mv', 'circ_mv']:
                if magnitude >= 10:
                    likely_unit = "å…ƒ"
                    warning = "å¸‚å€¼å•ä½å¯èƒ½æ˜¯å…ƒï¼Œé¢„æœŸä¸ºä¸‡å…ƒ"
                elif magnitude >= 6:
                    likely_unit = "ä¸‡å…ƒï¼ˆæ­£å¸¸ï¼‰"
                else:
                    likely_unit = "å¯èƒ½æœ‰é—®é¢˜"
                    warning = f"å¸‚å€¼æ•°é‡çº§å¼‚å¸¸: 10^{magnitude}"
                    
            elif col in ['rzye', 'rzmre']:
                if magnitude >= 8:
                    likely_unit = "å…ƒï¼ˆæ­£å¸¸ï¼‰"
                elif magnitude >= 5:
                    likely_unit = "ä¸‡å…ƒ"
                    warning = "èèµ„æ•°æ®å•ä½å¯èƒ½æ˜¯ä¸‡å…ƒï¼Œé¢„æœŸä¸ºå…ƒ"
                else:
                    likely_unit = "å¯èƒ½æœ‰é—®é¢˜"
                    
            elif 'amount' in col.lower():  # èµ„é‡‘æµå‘é‡‘é¢
                if magnitude >= 6:
                    likely_unit = "å…ƒ"
                    warning = "èµ„é‡‘æµå‘å•ä½å¯èƒ½æ˜¯å…ƒï¼Œé¢„æœŸä¸ºä¸‡å…ƒ"
                elif magnitude >= 2:
                    likely_unit = "ä¸‡å…ƒï¼ˆæ­£å¸¸ï¼‰"
                else:
                    likely_unit = "å¯èƒ½æœ‰é—®é¢˜"
            
            results[col] = {
                "min": min_val,
                "max": max_val,
                "mean": mean_val,
                "magnitude": magnitude,
                "likely_unit": likely_unit,
                "expected_unit": default_units.get(col, "æœªçŸ¥"),
                "warning": warning
            }
        
        # è¾“å‡ºè­¦å‘Š
        for col, info in results.items():
            if info.get("warning"):
                logger.warning(f"âš ï¸ æ•°æ®å•ä½è­¦å‘Š - {col}: {info['warning']}")
        
        return results
    
    def print_data_summary(
        self,
        df: pd.DataFrame,
        title: str = "æ•°æ®æ‘˜è¦"
    ) -> None:
        """
        æ‰“å°æ•°æ®æ‘˜è¦ï¼ˆç”¨äºè°ƒè¯•æ•°æ®å•ä½é—®é¢˜ï¼‰
        
        Parameters
        ----------
        df : pd.DataFrame
            è¦æ£€æŸ¥çš„æ•°æ®
        title : str
            æ ‡é¢˜
        """
        print(f"\n{'='*60}")
        print(f"ğŸ“Š {title}")
        print(f"{'='*60}")
        print(f"è¡Œæ•°: {len(df)}, åˆ—æ•°: {len(df.columns)}")
        print(f"\nå‰5è¡Œæ ·æœ¬:")
        print(df.head())
        
        # æ•°å€¼åˆ—ç»Ÿè®¡
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        if numeric_cols:
            print(f"\næ•°å€¼åˆ—ç»Ÿè®¡:")
            for col in numeric_cols[:10]:  # æœ€å¤šæ˜¾ç¤º10åˆ—
                series = df[col].dropna()
                if len(series) > 0:
                    print(f"  {col:25s}: min={series.min():>15,.2f}, "
                          f"max={series.max():>15,.2f}, "
                          f"mean={series.mean():>15,.2f}")
        
        # å•ä½éªŒè¯
        validation = self.validate_data_units(df)
        if any(v.get("warning") for v in validation.values()):
            print(f"\nâš ï¸ å•ä½è­¦å‘Š:")
            for col, info in validation.items():
                if info.get("warning"):
                    print(f"  {col}: {info['warning']}")
        
        print(f"{'='*60}\n")


# ==================== ä¾¿æ·å‡½æ•° ====================

def create_tushare_loader(config: Optional[Dict] = None) -> "TushareDataLoaderBase":
    """
    åˆ›å»º Tushare æ•°æ®åŠ è½½å™¨
    
    Parameters
    ----------
    config : Optional[Dict]
        é…ç½®å­—å…¸ï¼ŒåŒ…å« tushare.api_token
    
    Returns
    -------
    TushareDataLoaderBase
        æ•°æ®åŠ è½½å™¨å®ä¾‹
    """
    api_token = None
    
    if config:
        api_token = config.get("tushare", {}).get("api_token")
    
    if not api_token:
        api_token = os.environ.get("TUSHARE_TOKEN")
    
    return TushareDataLoaderBase(api_token=api_token)

